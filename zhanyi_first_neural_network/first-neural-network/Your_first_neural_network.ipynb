{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 你的第一个神经网络\n",
    "\n",
    "在此项目中，你将构建你的第一个神经网络，并用该网络预测每日自行车租客人数。我们提供了一些代码，但是需要你来实现神经网络（大部分内容）。提交此项目后，欢迎进一步探索该数据和模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载和准备数据\n",
    "\n",
    "构建神经网络的关键一步是正确地准备数据。不同尺度级别的变量使网络难以高效地掌握正确的权重。我们在下方已经提供了加载和准备数据的代码。你很快将进一步学习这些代码！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Bike-Sharing-Dataset/hour.csv'\n",
    "\n",
    "rides = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据简介\n",
    "\n",
    "此数据集包含的是从 2011 年 1 月 1 日到 2012 年 12 月 31 日期间每天每小时的骑车人数。骑车用户分成临时用户和注册用户，cnt 列是骑车用户数汇总列。你可以在上方看到前几行数据。\n",
    "\n",
    "下图展示的是数据集中前 10 天左右的骑车人数（某些天不一定是 24 个条目，所以不是精确的 10 天）。你可以在这里看到每小时租金。这些数据很复杂！周末的骑行人数少些，工作日上下班期间是骑行高峰期。我们还可以从上方的数据中看到温度、湿度和风速信息，所有这些信息都会影响骑行人数。你需要用你的模型展示所有这些数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x6c44c30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvgAAAIPCAYAAAAGtapCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzMzM6uQ/wNsebXPxKcHXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6c44270>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 263,
       "width": 380
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rides[:24*10].plot(x='dteday', y='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 虚拟变量（哑变量）\n",
    "\n",
    "下面是一些分类变量，例如季节、天气、月份。要在我们的模型中包含这些数据，我们需要创建二进制虚拟变量。用 Pandas 库中的 `get_dummies()` 就可以轻松实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "      <th>season_1</th>\n",
       "      <th>season_2</th>\n",
       "      <th>...</th>\n",
       "      <th>hr_21</th>\n",
       "      <th>hr_22</th>\n",
       "      <th>hr_23</th>\n",
       "      <th>weekday_0</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   yr  holiday  temp   hum  windspeed  casual  registered  cnt  season_1  \\\n",
       "0   0        0  0.24  0.81        0.0       3          13   16         1   \n",
       "1   0        0  0.22  0.80        0.0       8          32   40         1   \n",
       "2   0        0  0.22  0.80        0.0       5          27   32         1   \n",
       "3   0        0  0.24  0.75        0.0       3          10   13         1   \n",
       "4   0        0  0.24  0.75        0.0       0           1    1         1   \n",
       "\n",
       "   season_2    ...      hr_21  hr_22  hr_23  weekday_0  weekday_1  weekday_2  \\\n",
       "0         0    ...          0      0      0          0          0          0   \n",
       "1         0    ...          0      0      0          0          0          0   \n",
       "2         0    ...          0      0      0          0          0          0   \n",
       "3         0    ...          0      0      0          0          0          0   \n",
       "4         0    ...          0      0      0          0          0          0   \n",
       "\n",
       "   weekday_3  weekday_4  weekday_5  weekday_6  \n",
       "0          0          0          0          1  \n",
       "1          0          0          0          1  \n",
       "2          0          0          0          1  \n",
       "3          0          0          0          1  \n",
       "4          0          0          0          1  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
    "for each in dummy_fields:\n",
    "    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "    rides = pd.concat([rides, dummies], axis=1)\n",
    "\n",
    "fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n",
    "                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\n",
    "data = rides.drop(fields_to_drop, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调整目标变量\n",
    "\n",
    "为了更轻松地训练网络，我们将对每个连续变量标准化，即转换和调整变量，使它们的均值为 0，标准差为 1。\n",
    "\n",
    "我们会保存换算因子，以便当我们使用网络进行预测时可以还原数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
    "# Store scalings in a dictionary so we can convert back later\n",
    "scaled_features = {}\n",
    "for each in quant_features:\n",
    "    mean, std = data[each].mean(), data[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    data.loc[:, each] = (data[each] - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据拆分为训练、测试和验证数据集\n",
    "\n",
    "我们将大约最后 21 天的数据保存为测试数据集，这些数据集会在训练完网络后使用。我们将使用该数据集进行预测，并与实际的骑行人数进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for approximately the last 21 days \n",
    "test_data = data[-21*24:]\n",
    "\n",
    "# Now remove the test data from the data set \n",
    "data = data[:-21*24]\n",
    "\n",
    "# Separate the data into features and targets\n",
    "target_fields = ['cnt', 'casual', 'registered']\n",
    "features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
    "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将数据拆分为两个数据集，一个用作训练，一个在网络训练完后用来验证网络。因为数据是有时间序列特性的，所以我们用历史数据进行训练，然后尝试预测未来数据（验证数据集）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold out the last 60 days or so of the remaining data as a validation set\n",
    "train_features, train_targets = features[:-60*24], targets[:-60*24]\n",
    "val_features, val_targets = features[-60*24:], targets[-60*24:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始构建网络\n",
    "\n",
    "下面你将构建自己的网络。我们已经构建好结构和反向传递部分。你将实现网络的前向传递部分。还需要设置超参数：学习速率、隐藏单元的数量，以及训练传递数量。\n",
    "\n",
    "<img src=\"assets/neural_network.png\" width=300px>\n",
    "\n",
    "该网络有两个层级，一个隐藏层和一个输出层。隐藏层级将使用 S 型函数作为激活函数。输出层只有一个节点，用于递归，节点的输出和节点的输入相同。即激活函数是 $f(x)=x$。这种函数获得输入信号，并生成输出信号，但是会考虑阈值，称为激活函数。我们完成网络的每个层级，并计算每个神经元的输出。一个层级的所有输出变成下一层级神经元的输入。这一流程叫做前向传播（forward propagation）。\n",
    "\n",
    "我们在神经网络中使用权重将信号从输入层传播到输出层。我们还使用权重将错误从输出层传播回网络，以便更新权重。这叫做反向传播（backpropagation）。\n",
    "\n",
    "> **提示**：你需要为反向传播实现计算输出激活函数 ($f(x) = x$) 的导数。如果你不熟悉微积分，其实该函数就等同于等式 $y = x$。该等式的斜率是多少？也就是导数 $f(x)$。\n",
    "\n",
    "\n",
    "你需要完成以下任务：\n",
    "\n",
    "1. 实现 S 型激活函数。将 `__init__` 中的 `self.activation_function`  设为你的 S 型函数。\n",
    "2. 在 `train` 方法中实现前向传递。\n",
    "3. 在 `train` 方法中实现反向传播算法，包括计算输出错误。\n",
    "4. 在 `run` 方法中实现前向传递。\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, \n",
    "                                       (self.input_nodes, self.hidden_nodes))\n",
    "\n",
    "        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
    "                                       (self.hidden_nodes, self.output_nodes))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        #### TODO: Set self.activation_function to your implemented sigmoid function ####\n",
    "        #\n",
    "        # Note: in Python, you can define a function with a lambda expression,\n",
    "        # as shown below.\n",
    "        self.activation_function = lambda x : 1 / (1 + np.exp(-x))  # Replace 0 with your sigmoid calculation.\n",
    "        \n",
    "        ### If the lambda code above is not something you're familiar with,\n",
    "        # You can uncomment out the following three lines and put your \n",
    "        # implementation there instead.\n",
    "        #\n",
    "        #def sigmoid(x):\n",
    "        #    return 0  # Replace 0 with your sigmoid calculation here\n",
    "        #self.activation_function = sigmoid\n",
    "                    \n",
    "    \n",
    "    def train(self, features, targets):\n",
    "        ''' Train the network on batch of features and targets. \n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            \n",
    "            features: 2D array, each row is one data record, each column is a feature\n",
    "            targets: 1D array of target values\n",
    "        \n",
    "        '''\n",
    "        n_records = features.shape[0]\n",
    "        delta_weights_i_h = np.zeros(self.weights_input_to_hidden.shape)\n",
    "        delta_weights_h_o = np.zeros(self.weights_hidden_to_output.shape)\n",
    "        for X, y in zip(features, targets):\n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "            # TODO: Hidden layer - Replace these values with your calculations.\n",
    "            hidden_inputs = np.dot(X, self.weights_input_to_hidden) # signals into hidden layer\n",
    "            hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
    "\n",
    "            # TODO: Output layer - Replace these values with your calculations.\n",
    "            final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output) # signals into final output layer\n",
    "            final_outputs = final_inputs # signals from final output layer\n",
    "            \n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # TODO: Output error - Replace this value with your calculations.\n",
    "            error = y - final_outputs # Output layer error is the difference between desired target and actual output.\n",
    "            \n",
    "           \n",
    "            \n",
    "            # TODO: Calculate the hidden layer's contribution to the error\n",
    "            hidden_error = np.dot(error , (self.weights_hidden_to_output).T)\n",
    "           \n",
    "             \n",
    "            # TODO: Backpropagated error terms - Replace these values with your calculations.\n",
    "            output_error_term = error\n",
    "            hidden_error_term = hidden_error * hidden_outputs * (1 - hidden_outputs)\n",
    "\n",
    "            # Weight step (input to hidden)\n",
    "            delta_weights_i_h += hidden_error_term * (X[:,None])\n",
    "            # Weight step (hidden to output)\n",
    "            delta_weights_h_o += output_error_term * hidden_outputs[:, None]\n",
    "\n",
    "        # TODO: Update the weights - Replace these values with your calculations.\n",
    "        self.weights_hidden_to_output += self.lr * delta_weights_h_o / n_records # update hidden-to-output weights with gradient descent step\n",
    "        self.weights_input_to_hidden += self.lr * delta_weights_i_h / n_records # update input-to-hidden weights with gradient descent step\n",
    " \n",
    "    def run(self, features):\n",
    "        ''' Run a forward pass through the network with input features \n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            features: 1D array of feature values\n",
    "        '''\n",
    "        \n",
    "        #### Implement the forward pass here ####\n",
    "        # TODO: Hidden layer - replace these values with the appropriate calculations.\n",
    "        hidden_inputs = np.dot(features, self.weights_input_to_hidden) # signals into hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
    "        \n",
    "        # TODO: Output layer - Replace these values with the appropriate calculations.\n",
    "        final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output) # signals into final output layer\n",
    "        final_outputs = final_inputs # signals from final output layer \n",
    "        \n",
    "        print(final_outputs.shape)\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, Y):\n",
    "    return np.mean((y-Y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单元测试\n",
    "\n",
    "运行这些单元测试，检查你的网络实现是否正确。这样可以帮助你确保网络已正确实现，然后再开始训练网络。这些测试必须成功才能通过此项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.018s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=5 errors=0 failures=0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "inputs = np.array([[0.5, -0.2, 0.1]])\n",
    "targets = np.array([[0.4]])\n",
    "test_w_i_h = np.array([[0.1, -0.2],\n",
    "                       [0.4, 0.5],\n",
    "                       [-0.3, 0.2]])\n",
    "test_w_h_o = np.array([[0.3],\n",
    "                       [-0.1]])\n",
    "\n",
    "class TestMethods(unittest.TestCase):\n",
    "    \n",
    "    ##########\n",
    "    # Unit tests for data loading\n",
    "    ##########\n",
    "    \n",
    "    def test_data_path(self):\n",
    "        # Test that file path to dataset has been unaltered\n",
    "        self.assertTrue(data_path.lower() == 'bike-sharing-dataset/hour.csv')\n",
    "        \n",
    "    def test_data_loaded(self):\n",
    "        # Test that data frame loaded\n",
    "        self.assertTrue(isinstance(rides, pd.DataFrame))\n",
    "    \n",
    "    ##########\n",
    "    # Unit tests for network functionality\n",
    "    ##########\n",
    "\n",
    "    def test_activation(self):\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        # Test that the activation function is a sigmoid\n",
    "        self.assertTrue(np.all(network.activation_function(0.5) == 1/(1+np.exp(-0.5))))\n",
    "\n",
    "    def test_train(self):\n",
    "        # Test that weights are updated correctly on training\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        network.weights_input_to_hidden = test_w_i_h.copy()\n",
    "        network.weights_hidden_to_output = test_w_h_o.copy()\n",
    "        \n",
    "        network.train(inputs, targets)\n",
    "        self.assertTrue(np.allclose(network.weights_hidden_to_output, \n",
    "                                    np.array([[ 0.37275328], \n",
    "                                              [-0.03172939]])))\n",
    "        self.assertTrue(np.allclose(network.weights_input_to_hidden,\n",
    "                                    np.array([[ 0.10562014, -0.20185996], \n",
    "                                              [0.39775194, 0.50074398], \n",
    "                                              [-0.29887597, 0.19962801]])))\n",
    "\n",
    "    def test_run(self):\n",
    "        # Test correctness of run method\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        network.weights_input_to_hidden = test_w_i_h.copy()\n",
    "        network.weights_hidden_to_output = test_w_h_o.copy()\n",
    "\n",
    "        self.assertTrue(np.allclose(network.run(inputs), 0.09998924))\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestMethods())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练网络\n",
    "\n",
    "现在你将设置网络的超参数。策略是设置的超参数使训练集上的错误很小但是数据不会过拟合。如果网络训练时间太长，或者有太多的隐藏节点，可能就会过于针对特定训练集，无法泛化到验证数据集。即当训练集的损失降低时，验证集的损失将开始增大。\n",
    "\n",
    "你还将采用随机梯度下降 (SGD) 方法训练网络。对于每次训练，都获取随机样本数据，而不是整个数据集。与普通梯度下降相比，训练次数要更多，但是每次时间更短。这样的话，网络训练效率更高。稍后你将详细了解 SGD。\n",
    "\n",
    "\n",
    "### 选择迭代次数\n",
    "\n",
    "也就是训练网络时从训练数据中抽样的批次数量。迭代次数越多，模型就与数据越拟合。但是，如果迭代次数太多，模型就无法很好地泛化到其他数据，这叫做过拟合。你需要选择一个使训练损失很低并且验证损失保持中等水平的数字。当你开始过拟合时，你会发现训练损失继续下降，但是验证损失开始上升。\n",
    "\n",
    "### 选择学习速率\n",
    "\n",
    "速率可以调整权重更新幅度。如果速率太大，权重就会太大，导致网络无法与数据相拟合。建议从 0.1 开始。如果网络在与数据拟合时遇到问题，尝试降低学习速率。注意，学习速率越低，权重更新的步长就越小，神经网络收敛的时间就越长。\n",
    "\n",
    "\n",
    "### 选择隐藏节点数量\n",
    "\n",
    "隐藏节点越多，模型的预测结果就越准确。尝试不同的隐藏节点的数量，看看对性能有何影响。你可以查看损失字典，寻找网络性能指标。如果隐藏单元的数量太少，那么模型就没有足够的空间进行学习，如果太多，则学习方向就有太多的选择。选择隐藏单元数量的技巧在于找到合适的平衡点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.0% ... Training loss: 0.964 ... Validation loss: 1.293(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.0% ... Training loss: 0.943 ... Validation loss: 1.332(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.0% ... Training loss: 0.929 ... Validation loss: 1.278(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.1% ... Training loss: 0.912 ... Validation loss: 1.322(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.1% ... Training loss: 0.895 ... Validation loss: 1.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.1% ... Training loss: 0.899 ... Validation loss: 1.368(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.1% ... Training loss: 0.869 ... Validation loss: 1.295(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.1% ... Training loss: 0.856 ... Validation loss: 1.286(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.1% ... Training loss: 0.840 ... Validation loss: 1.260(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.1% ... Training loss: 0.838 ... Validation loss: 1.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.2% ... Training loss: 0.845 ... Validation loss: 1.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.2% ... Training loss: 0.791 ... Validation loss: 1.223(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.2% ... Training loss: 0.780 ... Validation loss: 1.260(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.2% ... Training loss: 0.757 ... Validation loss: 1.211(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.2% ... Training loss: 0.750 ... Validation loss: 1.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.2% ... Training loss: 0.737 ... Validation loss: 1.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.3% ... Training loss: 0.733 ... Validation loss: 1.228(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.3% ... Training loss: 0.719 ... Validation loss: 1.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.3% ... Training loss: 0.711 ... Validation loss: 1.121(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.3% ... Training loss: 0.750 ... Validation loss: 1.345(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.3% ... Training loss: 0.699 ... Validation loss: 1.083(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.3% ... Training loss: 0.683 ... Validation loss: 1.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.4% ... Training loss: 0.686 ... Validation loss: 1.193(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.4% ... Training loss: 0.685 ... Validation loss: 1.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.4% ... Training loss: 0.665 ... Validation loss: 1.088(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.4% ... Training loss: 0.671 ... Validation loss: 1.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.4% ... Training loss: 0.686 ... Validation loss: 1.009(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.5% ... Training loss: 0.660 ... Validation loss: 1.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.5% ... Training loss: 0.642 ... Validation loss: 1.065(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.5% ... Training loss: 0.637 ... Validation loss: 1.041(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.5% ... Training loss: 0.632 ... Validation loss: 1.063(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.5% ... Training loss: 0.635 ... Validation loss: 1.022(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.5% ... Training loss: 0.679 ... Validation loss: 1.257(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.6% ... Training loss: 0.625 ... Validation loss: 1.022(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.6% ... Training loss: 0.624 ... Validation loss: 1.001(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.6% ... Training loss: 0.621 ... Validation loss: 1.088(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.6% ... Training loss: 0.614 ... Validation loss: 0.999(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.6% ... Training loss: 0.631 ... Validation loss: 1.128(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.6% ... Training loss: 0.630 ... Validation loss: 0.937(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.7% ... Training loss: 0.605 ... Validation loss: 1.033(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.7% ... Training loss: 0.619 ... Validation loss: 1.093(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.7% ... Training loss: 0.599 ... Validation loss: 0.972(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.7% ... Training loss: 0.601 ... Validation loss: 1.042(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.7% ... Training loss: 0.603 ... Validation loss: 1.053(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.7% ... Training loss: 0.594 ... Validation loss: 1.019(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.8% ... Training loss: 0.590 ... Validation loss: 0.999(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.8% ... Training loss: 0.588 ... Validation loss: 1.007(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.8% ... Training loss: 0.610 ... Validation loss: 1.093(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.8% ... Training loss: 0.586 ... Validation loss: 1.013(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.8% ... Training loss: 0.577 ... Validation loss: 0.971(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.8% ... Training loss: 0.585 ... Validation loss: 0.905(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.8% ... Training loss: 0.572 ... Validation loss: 0.919(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.9% ... Training loss: 0.576 ... Validation loss: 0.892(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.9% ... Training loss: 0.565 ... Validation loss: 0.963(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.9% ... Training loss: 0.571 ... Validation loss: 0.988(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.9% ... Training loss: 0.561 ... Validation loss: 0.887(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.9% ... Training loss: 0.552 ... Validation loss: 0.913(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 0.9% ... Training loss: 0.559 ... Validation loss: 0.959(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.0% ... Training loss: 0.555 ... Validation loss: 0.871(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.0% ... Training loss: 0.544 ... Validation loss: 0.909(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.0% ... Training loss: 0.541 ... Validation loss: 0.883(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.0% ... Training loss: 0.538 ... Validation loss: 0.888(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.0% ... Training loss: 0.536 ... Validation loss: 0.897(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.1% ... Training loss: 0.532 ... Validation loss: 0.872(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.1% ... Training loss: 0.534 ... Validation loss: 0.903(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.1% ... Training loss: 0.527 ... Validation loss: 0.864(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.1% ... Training loss: 0.523 ... Validation loss: 0.860(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.1% ... Training loss: 0.532 ... Validation loss: 0.906(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.1% ... Training loss: 0.517 ... Validation loss: 0.857(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.1% ... Training loss: 0.518 ... Validation loss: 0.826(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.2% ... Training loss: 0.519 ... Validation loss: 0.875(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.2% ... Training loss: 0.510 ... Validation loss: 0.844(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.2% ... Training loss: 0.507 ... Validation loss: 0.843(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.2% ... Training loss: 0.506 ... Validation loss: 0.838(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.2% ... Training loss: 0.505 ... Validation loss: 0.834(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.2% ... Training loss: 0.503 ... Validation loss: 0.800(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.3% ... Training loss: 0.509 ... Validation loss: 0.851(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.3% ... Training loss: 0.501 ... Validation loss: 0.790(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.3% ... Training loss: 0.492 ... Validation loss: 0.816(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.3% ... Training loss: 0.493 ... Validation loss: 0.820(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.3% ... Training loss: 0.484 ... Validation loss: 0.796(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.4% ... Training loss: 0.482 ... Validation loss: 0.797(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.4% ... Training loss: 0.480 ... Validation loss: 0.782(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.4% ... Training loss: 0.475 ... Validation loss: 0.778(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.4% ... Training loss: 0.470 ... Validation loss: 0.769(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.4% ... Training loss: 0.500 ... Validation loss: 0.826(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.4% ... Training loss: 0.468 ... Validation loss: 0.759(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.4% ... Training loss: 0.469 ... Validation loss: 0.750(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.5% ... Training loss: 0.530 ... Validation loss: 0.858(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.5% ... Training loss: 0.470 ... Validation loss: 0.746(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.5% ... Training loss: 0.454 ... Validation loss: 0.741(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.5% ... Training loss: 0.452 ... Validation loss: 0.744(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.5% ... Training loss: 0.462 ... Validation loss: 0.741(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.6% ... Training loss: 0.446 ... Validation loss: 0.735(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1.6% ... Training loss: 0.443 ... Validation loss: 0.726(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.6% ... Training loss: 0.447 ... Validation loss: 0.734(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.6% ... Training loss: 0.438 ... Validation loss: 0.722(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.6% ... Training loss: 0.439 ... Validation loss: 0.720(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.6% ... Training loss: 0.473 ... Validation loss: 0.734(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.6% ... Training loss: 0.480 ... Validation loss: 0.761(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.7% ... Training loss: 0.447 ... Validation loss: 0.714(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.7% ... Training loss: 0.426 ... Validation loss: 0.701(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.7% ... Training loss: 0.426 ... Validation loss: 0.701(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.7% ... Training loss: 0.422 ... Validation loss: 0.698(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.7% ... Training loss: 0.421 ... Validation loss: 0.691(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.8% ... Training loss: 0.415 ... Validation loss: 0.684(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.8% ... Training loss: 0.413 ... Validation loss: 0.682(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.8% ... Training loss: 0.411 ... Validation loss: 0.675(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.8% ... Training loss: 0.409 ... Validation loss: 0.671(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.8% ... Training loss: 0.409 ... Validation loss: 0.674(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.8% ... Training loss: 0.405 ... Validation loss: 0.669(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.9% ... Training loss: 0.406 ... Validation loss: 0.665(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.9% ... Training loss: 0.413 ... Validation loss: 0.674(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.9% ... Training loss: 0.404 ... Validation loss: 0.647(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.9% ... Training loss: 0.396 ... Validation loss: 0.646(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.9% ... Training loss: 0.430 ... Validation loss: 0.690(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.9% ... Training loss: 0.394 ... Validation loss: 0.645(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 1.9% ... Training loss: 0.420 ... Validation loss: 0.682(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.0% ... Training loss: 0.407 ... Validation loss: 0.658(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.0% ... Training loss: 0.391 ... Validation loss: 0.640(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.0% ... Training loss: 0.381 ... Validation loss: 0.626(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.0% ... Training loss: 0.380 ... Validation loss: 0.624(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.0% ... Training loss: 0.380 ... Validation loss: 0.625(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.0% ... Training loss: 0.375 ... Validation loss: 0.618(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.1% ... Training loss: 0.397 ... Validation loss: 0.636(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.1% ... Training loss: 0.377 ... Validation loss: 0.622(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.1% ... Training loss: 0.381 ... Validation loss: 0.619(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.1% ... Training loss: 0.403 ... Validation loss: 0.652(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.1% ... Training loss: 0.403 ... Validation loss: 0.636(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.1% ... Training loss: 0.364 ... Validation loss: 0.601(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.2% ... Training loss: 0.390 ... Validation loss: 0.623(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.2% ... Training loss: 0.368 ... Validation loss: 0.611(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.2% ... Training loss: 0.399 ... Validation loss: 0.638(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.2% ... Training loss: 0.396 ... Validation loss: 0.612(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.2% ... Training loss: 0.361 ... Validation loss: 0.597(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.2% ... Training loss: 0.358 ... Validation loss: 0.588(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.3% ... Training loss: 0.358 ... Validation loss: 0.587(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.3% ... Training loss: 0.353 ... Validation loss: 0.586(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.3% ... Training loss: 0.351 ... Validation loss: 0.571(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.3% ... Training loss: 0.350 ... Validation loss: 0.577(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.3% ... Training loss: 0.350 ... Validation loss: 0.585(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.4% ... Training loss: 0.345 ... Validation loss: 0.564(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.4% ... Training loss: 0.343 ... Validation loss: 0.555(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.4% ... Training loss: 0.341 ... Validation loss: 0.559(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.4% ... Training loss: 0.350 ... Validation loss: 0.562(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.4% ... Training loss: 0.368 ... Validation loss: 0.576(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.4% ... Training loss: 0.339 ... Validation loss: 0.557(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.5% ... Training loss: 0.339 ... Validation loss: 0.555(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.5% ... Training loss: 0.341 ... Validation loss: 0.559(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.5% ... Training loss: 0.344 ... Validation loss: 0.558(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.5% ... Training loss: 0.366 ... Validation loss: 0.581(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.5% ... Training loss: 0.382 ... Validation loss: 0.577(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.5% ... Training loss: 0.337 ... Validation loss: 0.546(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.5% ... Training loss: 0.353 ... Validation loss: 0.563(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.6% ... Training loss: 0.348 ... Validation loss: 0.548(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.6% ... Training loss: 0.333 ... Validation loss: 0.540(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.6% ... Training loss: 0.342 ... Validation loss: 0.540(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.6% ... Training loss: 0.333 ... Validation loss: 0.539(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.6% ... Training loss: 0.331 ... Validation loss: 0.535(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.6% ... Training loss: 0.357 ... Validation loss: 0.557(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.7% ... Training loss: 0.330 ... Validation loss: 0.527(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.7% ... Training loss: 0.326 ... Validation loss: 0.528(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.7% ... Training loss: 0.327 ... Validation loss: 0.537(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.7% ... Training loss: 0.336 ... Validation loss: 0.535(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.7% ... Training loss: 0.329 ... Validation loss: 0.548(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.8% ... Training loss: 0.333 ... Validation loss: 0.535(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.8% ... Training loss: 0.335 ... Validation loss: 0.553(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.8% ... Training loss: 0.339 ... Validation loss: 0.533(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.8% ... Training loss: 0.380 ... Validation loss: 0.580(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.8% ... Training loss: 0.330 ... Validation loss: 0.515(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.8% ... Training loss: 0.331 ... Validation loss: 0.520(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.9% ... Training loss: 0.322 ... Validation loss: 0.509(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.9% ... Training loss: 0.342 ... Validation loss: 0.530(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.9% ... Training loss: 0.335 ... Validation loss: 0.527(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.9% ... Training loss: 0.316 ... Validation loss: 0.502(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.9% ... Training loss: 0.318 ... Validation loss: 0.500(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 2.9% ... Training loss: 0.317 ... Validation loss: 0.499(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.0% ... Training loss: 0.315 ... Validation loss: 0.506(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.0% ... Training loss: 0.316 ... Validation loss: 0.513(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.0% ... Training loss: 0.321 ... Validation loss: 0.513(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.0% ... Training loss: 0.320 ... Validation loss: 0.516(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.0% ... Training loss: 0.318 ... Validation loss: 0.505(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.0% ... Training loss: 0.330 ... Validation loss: 0.509(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.0% ... Training loss: 0.313 ... Validation loss: 0.492(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.1% ... Training loss: 0.319 ... Validation loss: 0.500(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.1% ... Training loss: 0.316 ... Validation loss: 0.499(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.1% ... Training loss: 0.310 ... Validation loss: 0.491(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.1% ... Training loss: 0.313 ... Validation loss: 0.489(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 3.1% ... Training loss: 0.324 ... Validation loss: 0.508(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.1% ... Training loss: 0.315 ... Validation loss: 0.500(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.2% ... Training loss: 0.310 ... Validation loss: 0.489(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.2% ... Training loss: 0.310 ... Validation loss: 0.497(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.2% ... Training loss: 0.312 ... Validation loss: 0.494(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.2% ... Training loss: 0.340 ... Validation loss: 0.527(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.2% ... Training loss: 0.334 ... Validation loss: 0.510(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.2% ... Training loss: 0.310 ... Validation loss: 0.498(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.3% ... Training loss: 0.308 ... Validation loss: 0.494(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.3% ... Training loss: 0.318 ... Validation loss: 0.508(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.3% ... Training loss: 0.310 ... Validation loss: 0.499(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.3% ... Training loss: 0.331 ... Validation loss: 0.496(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.3% ... Training loss: 0.312 ... Validation loss: 0.489(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.4% ... Training loss: 0.322 ... Validation loss: 0.476(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.4% ... Training loss: 0.324 ... Validation loss: 0.513(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.4% ... Training loss: 0.309 ... Validation loss: 0.488(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.4% ... Training loss: 0.305 ... Validation loss: 0.488(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.4% ... Training loss: 0.307 ... Validation loss: 0.486(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.4% ... Training loss: 0.318 ... Validation loss: 0.479(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.5% ... Training loss: 0.320 ... Validation loss: 0.500(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.5% ... Training loss: 0.317 ... Validation loss: 0.484(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.5% ... Training loss: 0.303 ... Validation loss: 0.474(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.5% ... Training loss: 0.303 ... Validation loss: 0.479(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.5% ... Training loss: 0.305 ... Validation loss: 0.473(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.5% ... Training loss: 0.320 ... Validation loss: 0.507(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.5% ... Training loss: 0.306 ... Validation loss: 0.482(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.6% ... Training loss: 0.308 ... Validation loss: 0.480(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.6% ... Training loss: 0.313 ... Validation loss: 0.481(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.6% ... Training loss: 0.308 ... Validation loss: 0.480(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.6% ... Training loss: 0.313 ... Validation loss: 0.485(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.6% ... Training loss: 0.307 ... Validation loss: 0.484(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.6% ... Training loss: 0.314 ... Validation loss: 0.493(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.7% ... Training loss: 0.322 ... Validation loss: 0.511(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.7% ... Training loss: 0.305 ... Validation loss: 0.482(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.7% ... Training loss: 0.376 ... Validation loss: 0.528(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.7% ... Training loss: 0.310 ... Validation loss: 0.480(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.7% ... Training loss: 0.307 ... Validation loss: 0.474(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.8% ... Training loss: 0.303 ... Validation loss: 0.473(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.8% ... Training loss: 0.303 ... Validation loss: 0.476(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.8% ... Training loss: 0.321 ... Validation loss: 0.495(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.8% ... Training loss: 0.320 ... Validation loss: 0.491(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.8% ... Training loss: 0.324 ... Validation loss: 0.496(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.8% ... Training loss: 0.312 ... Validation loss: 0.473(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.9% ... Training loss: 0.310 ... Validation loss: 0.490(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.9% ... Training loss: 0.299 ... Validation loss: 0.476(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.9% ... Training loss: 0.312 ... Validation loss: 0.477(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.9% ... Training loss: 0.305 ... Validation loss: 0.482(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.9% ... Training loss: 0.304 ... Validation loss: 0.484(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 3.9% ... Training loss: 0.311 ... Validation loss: 0.490(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.0% ... Training loss: 0.302 ... Validation loss: 0.483(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.0% ... Training loss: 0.303 ... Validation loss: 0.479(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.0% ... Training loss: 0.302 ... Validation loss: 0.482(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.0% ... Training loss: 0.301 ... Validation loss: 0.478(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.0% ... Training loss: 0.298 ... Validation loss: 0.473(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.0% ... Training loss: 0.322 ... Validation loss: 0.486(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.0% ... Training loss: 0.306 ... Validation loss: 0.481(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.1% ... Training loss: 0.298 ... Validation loss: 0.478(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.1% ... Training loss: 0.342 ... Validation loss: 0.520(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.1% ... Training loss: 0.306 ... Validation loss: 0.492(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.1% ... Training loss: 0.299 ... Validation loss: 0.476(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.1% ... Training loss: 0.296 ... Validation loss: 0.468(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.2% ... Training loss: 0.306 ... Validation loss: 0.480(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.2% ... Training loss: 0.301 ... Validation loss: 0.462(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.2% ... Training loss: 0.298 ... Validation loss: 0.463(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.2% ... Training loss: 0.299 ... Validation loss: 0.472(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.2% ... Training loss: 0.300 ... Validation loss: 0.467(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.2% ... Training loss: 0.325 ... Validation loss: 0.501(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.2% ... Training loss: 0.297 ... Validation loss: 0.483(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.3% ... Training loss: 0.314 ... Validation loss: 0.482(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.3% ... Training loss: 0.309 ... Validation loss: 0.487(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.3% ... Training loss: 0.293 ... Validation loss: 0.469(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.3% ... Training loss: 0.305 ... Validation loss: 0.484(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.3% ... Training loss: 0.298 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.3% ... Training loss: 0.306 ... Validation loss: 0.470(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.4% ... Training loss: 0.301 ... Validation loss: 0.471(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.4% ... Training loss: 0.294 ... Validation loss: 0.472(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.4% ... Training loss: 0.303 ... Validation loss: 0.476(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.4% ... Training loss: 0.294 ... Validation loss: 0.469(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.4% ... Training loss: 0.296 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.5% ... Training loss: 0.301 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.5% ... Training loss: 0.293 ... Validation loss: 0.462(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.5% ... Training loss: 0.293 ... Validation loss: 0.463(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.5% ... Training loss: 0.296 ... Validation loss: 0.465(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.5% ... Training loss: 0.293 ... Validation loss: 0.462(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.5% ... Training loss: 0.300 ... Validation loss: 0.469(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.5% ... Training loss: 0.292 ... Validation loss: 0.463(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.6% ... Training loss: 0.294 ... Validation loss: 0.463(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.6% ... Training loss: 0.304 ... Validation loss: 0.478(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.6% ... Training loss: 0.295 ... Validation loss: 0.457(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.6% ... Training loss: 0.295 ... Validation loss: 0.470(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.6% ... Training loss: 0.292 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.7% ... Training loss: 0.292 ... Validation loss: 0.462(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.7% ... Training loss: 0.293 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.7% ... Training loss: 0.291 ... Validation loss: 0.460(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 4.7% ... Training loss: 0.291 ... Validation loss: 0.461(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.7% ... Training loss: 0.293 ... Validation loss: 0.452(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.7% ... Training loss: 0.291 ... Validation loss: 0.459(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.8% ... Training loss: 0.291 ... Validation loss: 0.460(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.8% ... Training loss: 0.292 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.8% ... Training loss: 0.293 ... Validation loss: 0.465(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.8% ... Training loss: 0.292 ... Validation loss: 0.466(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.8% ... Training loss: 0.295 ... Validation loss: 0.469(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.8% ... Training loss: 0.307 ... Validation loss: 0.470(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.8% ... Training loss: 0.289 ... Validation loss: 0.453(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.9% ... Training loss: 0.296 ... Validation loss: 0.469(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.9% ... Training loss: 0.327 ... Validation loss: 0.483(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.9% ... Training loss: 0.312 ... Validation loss: 0.489(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.9% ... Training loss: 0.308 ... Validation loss: 0.466(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 4.9% ... Training loss: 0.289 ... Validation loss: 0.461(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.0% ... Training loss: 0.288 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.0% ... Training loss: 0.288 ... Validation loss: 0.460(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.0% ... Training loss: 0.306 ... Validation loss: 0.478(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.0% ... Training loss: 0.311 ... Validation loss: 0.474(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.0% ... Training loss: 0.290 ... Validation loss: 0.467(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.0% ... Training loss: 0.288 ... Validation loss: 0.458(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.0% ... Training loss: 0.291 ... Validation loss: 0.466(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.1% ... Training loss: 0.336 ... Validation loss: 0.490(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.1% ... Training loss: 0.296 ... Validation loss: 0.480(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.1% ... Training loss: 0.296 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.1% ... Training loss: 0.305 ... Validation loss: 0.474(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.1% ... Training loss: 0.287 ... Validation loss: 0.453(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.2% ... Training loss: 0.308 ... Validation loss: 0.469(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.2% ... Training loss: 0.341 ... Validation loss: 0.485(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.2% ... Training loss: 0.286 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.2% ... Training loss: 0.292 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.2% ... Training loss: 0.295 ... Validation loss: 0.473(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.2% ... Training loss: 0.288 ... Validation loss: 0.455(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.2% ... Training loss: 0.287 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.3% ... Training loss: 0.290 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.3% ... Training loss: 0.290 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.3% ... Training loss: 0.310 ... Validation loss: 0.461(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.3% ... Training loss: 0.285 ... Validation loss: 0.453(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.3% ... Training loss: 0.289 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.3% ... Training loss: 0.297 ... Validation loss: 0.450(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.4% ... Training loss: 0.287 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.4% ... Training loss: 0.293 ... Validation loss: 0.459(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.4% ... Training loss: 0.290 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.4% ... Training loss: 0.322 ... Validation loss: 0.477(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.4% ... Training loss: 0.289 ... Validation loss: 0.471(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.5% ... Training loss: 0.292 ... Validation loss: 0.478(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.5% ... Training loss: 0.290 ... Validation loss: 0.462(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.5% ... Training loss: 0.286 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.5% ... Training loss: 0.285 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.5% ... Training loss: 0.292 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.5% ... Training loss: 0.298 ... Validation loss: 0.470(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.5% ... Training loss: 0.292 ... Validation loss: 0.461(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.6% ... Training loss: 0.311 ... Validation loss: 0.471(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.6% ... Training loss: 0.299 ... Validation loss: 0.469(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.6% ... Training loss: 0.296 ... Validation loss: 0.455(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.6% ... Training loss: 0.287 ... Validation loss: 0.466(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.6% ... Training loss: 0.289 ... Validation loss: 0.458(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.7% ... Training loss: 0.310 ... Validation loss: 0.513(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.7% ... Training loss: 0.285 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.7% ... Training loss: 0.292 ... Validation loss: 0.490(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.7% ... Training loss: 0.295 ... Validation loss: 0.457(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.7% ... Training loss: 0.291 ... Validation loss: 0.491(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.7% ... Training loss: 0.293 ... Validation loss: 0.491(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.8% ... Training loss: 0.297 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.8% ... Training loss: 0.313 ... Validation loss: 0.506(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.8% ... Training loss: 0.313 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.8% ... Training loss: 0.289 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.8% ... Training loss: 0.292 ... Validation loss: 0.471(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.8% ... Training loss: 0.284 ... Validation loss: 0.459(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.8% ... Training loss: 0.286 ... Validation loss: 0.473(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.9% ... Training loss: 0.283 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.9% ... Training loss: 0.282 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.9% ... Training loss: 0.287 ... Validation loss: 0.467(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.9% ... Training loss: 0.282 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 5.9% ... Training loss: 0.286 ... Validation loss: 0.452(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.0% ... Training loss: 0.281 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.0% ... Training loss: 0.280 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.0% ... Training loss: 0.285 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.0% ... Training loss: 0.283 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.0% ... Training loss: 0.280 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.0% ... Training loss: 0.295 ... Validation loss: 0.463(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.0% ... Training loss: 0.324 ... Validation loss: 0.496(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.1% ... Training loss: 0.281 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.1% ... Training loss: 0.286 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.1% ... Training loss: 0.281 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.1% ... Training loss: 0.289 ... Validation loss: 0.458(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.1% ... Training loss: 0.284 ... Validation loss: 0.461(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.2% ... Training loss: 0.286 ... Validation loss: 0.455(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.2% ... Training loss: 0.281 ... Validation loss: 0.455(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.2% ... Training loss: 0.281 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.2% ... Training loss: 0.285 ... Validation loss: 0.462(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.2% ... Training loss: 0.307 ... Validation loss: 0.480(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.2% ... Training loss: 0.306 ... Validation loss: 0.473(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.2% ... Training loss: 0.282 ... Validation loss: 0.455(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 6.3% ... Training loss: 0.280 ... Validation loss: 0.458(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.3% ... Training loss: 0.284 ... Validation loss: 0.458(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.3% ... Training loss: 0.295 ... Validation loss: 0.466(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.3% ... Training loss: 0.284 ... Validation loss: 0.457(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.3% ... Training loss: 0.283 ... Validation loss: 0.446(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.3% ... Training loss: 0.280 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.4% ... Training loss: 0.278 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.4% ... Training loss: 0.294 ... Validation loss: 0.461(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.4% ... Training loss: 0.279 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.4% ... Training loss: 0.289 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.4% ... Training loss: 0.280 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.5% ... Training loss: 0.280 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.5% ... Training loss: 0.290 ... Validation loss: 0.465(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.5% ... Training loss: 0.304 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.5% ... Training loss: 0.295 ... Validation loss: 0.467(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.5% ... Training loss: 0.298 ... Validation loss: 0.460(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.5% ... Training loss: 0.297 ... Validation loss: 0.468(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.5% ... Training loss: 0.295 ... Validation loss: 0.458(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.6% ... Training loss: 0.318 ... Validation loss: 0.492(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.6% ... Training loss: 0.293 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.6% ... Training loss: 0.287 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.6% ... Training loss: 0.279 ... Validation loss: 0.453(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.6% ... Training loss: 0.278 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.7% ... Training loss: 0.279 ... Validation loss: 0.450(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.7% ... Training loss: 0.278 ... Validation loss: 0.446(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.7% ... Training loss: 0.276 ... Validation loss: 0.450(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.7% ... Training loss: 0.277 ... Validation loss: 0.452(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.7% ... Training loss: 0.288 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.7% ... Training loss: 0.283 ... Validation loss: 0.452(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.8% ... Training loss: 0.280 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.8% ... Training loss: 0.288 ... Validation loss: 0.463(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.8% ... Training loss: 0.280 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.8% ... Training loss: 0.279 ... Validation loss: 0.450(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.8% ... Training loss: 0.280 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.8% ... Training loss: 0.277 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.8% ... Training loss: 0.279 ... Validation loss: 0.453(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.9% ... Training loss: 0.291 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.9% ... Training loss: 0.283 ... Validation loss: 0.480(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.9% ... Training loss: 0.302 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.9% ... Training loss: 0.312 ... Validation loss: 0.507(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 6.9% ... Training loss: 0.281 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.0% ... Training loss: 0.288 ... Validation loss: 0.461(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.0% ... Training loss: 0.275 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.0% ... Training loss: 0.275 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.0% ... Training loss: 0.276 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.0% ... Training loss: 0.275 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.0% ... Training loss: 0.280 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.0% ... Training loss: 0.303 ... Validation loss: 0.485(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.1% ... Training loss: 0.295 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.1% ... Training loss: 0.301 ... Validation loss: 0.498(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.1% ... Training loss: 0.278 ... Validation loss: 0.446(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.1% ... Training loss: 0.274 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.1% ... Training loss: 0.276 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.2% ... Training loss: 0.275 ... Validation loss: 0.450(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.2% ... Training loss: 0.279 ... Validation loss: 0.461(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.2% ... Training loss: 0.278 ... Validation loss: 0.440(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.2% ... Training loss: 0.288 ... Validation loss: 0.452(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.2% ... Training loss: 0.290 ... Validation loss: 0.469(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.2% ... Training loss: 0.275 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.2% ... Training loss: 0.277 ... Validation loss: 0.446(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.3% ... Training loss: 0.273 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.3% ... Training loss: 0.294 ... Validation loss: 0.460(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.3% ... Training loss: 0.273 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.3% ... Training loss: 0.309 ... Validation loss: 0.486(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.3% ... Training loss: 0.278 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.3% ... Training loss: 0.279 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.4% ... Training loss: 0.278 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.4% ... Training loss: 0.280 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.4% ... Training loss: 0.292 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.4% ... Training loss: 0.281 ... Validation loss: 0.450(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.4% ... Training loss: 0.273 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.5% ... Training loss: 0.273 ... Validation loss: 0.440(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.5% ... Training loss: 0.274 ... Validation loss: 0.443(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.5% ... Training loss: 0.305 ... Validation loss: 0.475(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.5% ... Training loss: 0.273 ... Validation loss: 0.453(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.5% ... Training loss: 0.272 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.5% ... Training loss: 0.279 ... Validation loss: 0.446(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.5% ... Training loss: 0.317 ... Validation loss: 0.486(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.6% ... Training loss: 0.308 ... Validation loss: 0.470(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.6% ... Training loss: 0.292 ... Validation loss: 0.460(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.6% ... Training loss: 0.282 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.6% ... Training loss: 0.276 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.6% ... Training loss: 0.287 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.7% ... Training loss: 0.279 ... Validation loss: 0.438(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.7% ... Training loss: 0.276 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.7% ... Training loss: 0.276 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.7% ... Training loss: 0.272 ... Validation loss: 0.452(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.7% ... Training loss: 0.273 ... Validation loss: 0.455(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.7% ... Training loss: 0.275 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.8% ... Training loss: 0.276 ... Validation loss: 0.455(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.8% ... Training loss: 0.305 ... Validation loss: 0.471(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.8% ... Training loss: 0.306 ... Validation loss: 0.500(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.8% ... Training loss: 0.275 ... Validation loss: 0.450(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.8% ... Training loss: 0.272 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 7.8% ... Training loss: 0.271 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.8% ... Training loss: 0.270 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.9% ... Training loss: 0.273 ... Validation loss: 0.446(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.9% ... Training loss: 0.276 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.9% ... Training loss: 0.271 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.9% ... Training loss: 0.277 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 7.9% ... Training loss: 0.272 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.0% ... Training loss: 0.287 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.0% ... Training loss: 0.273 ... Validation loss: 0.443(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.0% ... Training loss: 0.270 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.0% ... Training loss: 0.271 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.0% ... Training loss: 0.275 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.0% ... Training loss: 0.290 ... Validation loss: 0.465(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.1% ... Training loss: 0.271 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.1% ... Training loss: 0.273 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.1% ... Training loss: 0.271 ... Validation loss: 0.438(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.1% ... Training loss: 0.272 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.1% ... Training loss: 0.294 ... Validation loss: 0.452(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.1% ... Training loss: 0.313 ... Validation loss: 0.473(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.2% ... Training loss: 0.287 ... Validation loss: 0.452(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.2% ... Training loss: 0.290 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.2% ... Training loss: 0.272 ... Validation loss: 0.440(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.2% ... Training loss: 0.277 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.2% ... Training loss: 0.270 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.2% ... Training loss: 0.274 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.2% ... Training loss: 0.286 ... Validation loss: 0.458(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.3% ... Training loss: 0.289 ... Validation loss: 0.468(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.3% ... Training loss: 0.303 ... Validation loss: 0.466(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.3% ... Training loss: 0.270 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.3% ... Training loss: 0.272 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.3% ... Training loss: 0.275 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.3% ... Training loss: 0.273 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.4% ... Training loss: 0.271 ... Validation loss: 0.450(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.4% ... Training loss: 0.279 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.4% ... Training loss: 0.272 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.4% ... Training loss: 0.271 ... Validation loss: 0.433(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.4% ... Training loss: 0.281 ... Validation loss: 0.443(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.4% ... Training loss: 0.284 ... Validation loss: 0.459(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.5% ... Training loss: 0.313 ... Validation loss: 0.485(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.5% ... Training loss: 0.269 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.5% ... Training loss: 0.272 ... Validation loss: 0.443(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.5% ... Training loss: 0.279 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.5% ... Training loss: 0.269 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.6% ... Training loss: 0.274 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.6% ... Training loss: 0.276 ... Validation loss: 0.455(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.6% ... Training loss: 0.278 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.6% ... Training loss: 0.270 ... Validation loss: 0.440(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.6% ... Training loss: 0.269 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.6% ... Training loss: 0.277 ... Validation loss: 0.450(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.7% ... Training loss: 0.272 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.7% ... Training loss: 0.276 ... Validation loss: 0.452(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.7% ... Training loss: 0.267 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.7% ... Training loss: 0.269 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.7% ... Training loss: 0.290 ... Validation loss: 0.470(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.7% ... Training loss: 0.272 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.8% ... Training loss: 0.268 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.8% ... Training loss: 0.276 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.8% ... Training loss: 0.268 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.8% ... Training loss: 0.270 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.8% ... Training loss: 0.279 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.8% ... Training loss: 0.284 ... Validation loss: 0.446(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.8% ... Training loss: 0.285 ... Validation loss: 0.452(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.9% ... Training loss: 0.305 ... Validation loss: 0.461(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.9% ... Training loss: 0.300 ... Validation loss: 0.474(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.9% ... Training loss: 0.270 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.9% ... Training loss: 0.286 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.9% ... Training loss: 0.279 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 8.9% ... Training loss: 0.270 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.0% ... Training loss: 0.268 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.0% ... Training loss: 0.280 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.0% ... Training loss: 0.326 ... Validation loss: 0.502(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.0% ... Training loss: 0.267 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.0% ... Training loss: 0.271 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.1% ... Training loss: 0.270 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.1% ... Training loss: 0.270 ... Validation loss: 0.443(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.1% ... Training loss: 0.315 ... Validation loss: 0.470(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.1% ... Training loss: 0.281 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.1% ... Training loss: 0.284 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.1% ... Training loss: 0.280 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.2% ... Training loss: 0.287 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.2% ... Training loss: 0.272 ... Validation loss: 0.443(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.2% ... Training loss: 0.275 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.2% ... Training loss: 0.301 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.2% ... Training loss: 0.281 ... Validation loss: 0.465(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.2% ... Training loss: 0.268 ... Validation loss: 0.432(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.2% ... Training loss: 0.271 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.3% ... Training loss: 0.288 ... Validation loss: 0.467(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.3% ... Training loss: 0.272 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.3% ... Training loss: 0.267 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.3% ... Training loss: 0.271 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.3% ... Training loss: 0.273 ... Validation loss: 0.446(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.3% ... Training loss: 0.268 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.4% ... Training loss: 0.274 ... Validation loss: 0.440(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.4% ... Training loss: 0.294 ... Validation loss: 0.469(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 9.4% ... Training loss: 0.277 ... Validation loss: 0.432(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.4% ... Training loss: 0.272 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.4% ... Training loss: 0.271 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.4% ... Training loss: 0.289 ... Validation loss: 0.458(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.5% ... Training loss: 0.275 ... Validation loss: 0.432(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.5% ... Training loss: 0.270 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.5% ... Training loss: 0.266 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.5% ... Training loss: 0.285 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.5% ... Training loss: 0.275 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.6% ... Training loss: 0.268 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.6% ... Training loss: 0.270 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.6% ... Training loss: 0.267 ... Validation loss: 0.432(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.6% ... Training loss: 0.267 ... Validation loss: 0.433(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.6% ... Training loss: 0.267 ... Validation loss: 0.438(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.6% ... Training loss: 0.274 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.7% ... Training loss: 0.267 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.7% ... Training loss: 0.277 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.7% ... Training loss: 0.297 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.7% ... Training loss: 0.293 ... Validation loss: 0.483(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.7% ... Training loss: 0.285 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.7% ... Training loss: 0.268 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.8% ... Training loss: 0.267 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.8% ... Training loss: 0.280 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.8% ... Training loss: 0.270 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.8% ... Training loss: 0.265 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.8% ... Training loss: 0.265 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.8% ... Training loss: 0.272 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.8% ... Training loss: 0.266 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.9% ... Training loss: 0.268 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.9% ... Training loss: 0.268 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.9% ... Training loss: 0.265 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.9% ... Training loss: 0.265 ... Validation loss: 0.440(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.9% ... Training loss: 0.270 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 9.9% ... Training loss: 0.265 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.0% ... Training loss: 0.266 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.0% ... Training loss: 0.266 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.0% ... Training loss: 0.265 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.0% ... Training loss: 0.311 ... Validation loss: 0.480(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.0% ... Training loss: 0.287 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.1% ... Training loss: 0.267 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.1% ... Training loss: 0.267 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.1% ... Training loss: 0.263 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.1% ... Training loss: 0.264 ... Validation loss: 0.440(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.1% ... Training loss: 0.265 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.1% ... Training loss: 0.301 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.2% ... Training loss: 0.276 ... Validation loss: 0.465(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.2% ... Training loss: 0.265 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.2% ... Training loss: 0.267 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.2% ... Training loss: 0.265 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.2% ... Training loss: 0.267 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.2% ... Training loss: 0.268 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.2% ... Training loss: 0.268 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.3% ... Training loss: 0.288 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.3% ... Training loss: 0.265 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.3% ... Training loss: 0.263 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.3% ... Training loss: 0.267 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.3% ... Training loss: 0.270 ... Validation loss: 0.440(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.3% ... Training loss: 0.274 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.4% ... Training loss: 0.271 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.4% ... Training loss: 0.266 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.4% ... Training loss: 0.264 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.4% ... Training loss: 0.270 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.4% ... Training loss: 0.350 ... Validation loss: 0.501(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.4% ... Training loss: 0.293 ... Validation loss: 0.457(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.5% ... Training loss: 0.264 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.5% ... Training loss: 0.297 ... Validation loss: 0.460(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.5% ... Training loss: 0.306 ... Validation loss: 0.473(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.5% ... Training loss: 0.264 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.5% ... Training loss: 0.276 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.6% ... Training loss: 0.274 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.6% ... Training loss: 0.275 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.6% ... Training loss: 0.267 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.6% ... Training loss: 0.265 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.6% ... Training loss: 0.264 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.6% ... Training loss: 0.274 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.7% ... Training loss: 0.272 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.7% ... Training loss: 0.279 ... Validation loss: 0.440(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.7% ... Training loss: 0.283 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.7% ... Training loss: 0.264 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.7% ... Training loss: 0.262 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.7% ... Training loss: 0.263 ... Validation loss: 0.432(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.8% ... Training loss: 0.285 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.8% ... Training loss: 0.265 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.8% ... Training loss: 0.265 ... Validation loss: 0.438(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.8% ... Training loss: 0.274 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.8% ... Training loss: 0.263 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.8% ... Training loss: 0.263 ... Validation loss: 0.438(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.8% ... Training loss: 0.271 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.9% ... Training loss: 0.266 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.9% ... Training loss: 0.265 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.9% ... Training loss: 0.269 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.9% ... Training loss: 0.269 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 10.9% ... Training loss: 0.266 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 10.9% ... Training loss: 0.263 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.0% ... Training loss: 0.264 ... Validation loss: 0.438(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.0% ... Training loss: 0.268 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.0% ... Training loss: 0.306 ... Validation loss: 0.463(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.0% ... Training loss: 0.300 ... Validation loss: 0.460(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.0% ... Training loss: 0.286 ... Validation loss: 0.476(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.1% ... Training loss: 0.299 ... Validation loss: 0.453(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.1% ... Training loss: 0.306 ... Validation loss: 0.503(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.1% ... Training loss: 0.261 ... Validation loss: 0.432(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.1% ... Training loss: 0.261 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.1% ... Training loss: 0.283 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.1% ... Training loss: 0.267 ... Validation loss: 0.417(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.2% ... Training loss: 0.267 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.2% ... Training loss: 0.276 ... Validation loss: 0.452(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.2% ... Training loss: 0.261 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.2% ... Training loss: 0.265 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.2% ... Training loss: 0.267 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.2% ... Training loss: 0.270 ... Validation loss: 0.433(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.2% ... Training loss: 0.294 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.3% ... Training loss: 0.300 ... Validation loss: 0.471(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.3% ... Training loss: 0.293 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.3% ... Training loss: 0.277 ... Validation loss: 0.443(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.3% ... Training loss: 0.265 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.3% ... Training loss: 0.261 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.3% ... Training loss: 0.276 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.4% ... Training loss: 0.266 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.4% ... Training loss: 0.264 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.4% ... Training loss: 0.261 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.4% ... Training loss: 0.264 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.4% ... Training loss: 0.261 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.4% ... Training loss: 0.263 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.5% ... Training loss: 0.283 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.5% ... Training loss: 0.261 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.5% ... Training loss: 0.271 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.5% ... Training loss: 0.266 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.5% ... Training loss: 0.260 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.6% ... Training loss: 0.298 ... Validation loss: 0.452(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.6% ... Training loss: 0.268 ... Validation loss: 0.438(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.6% ... Training loss: 0.262 ... Validation loss: 0.418(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.6% ... Training loss: 0.261 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.6% ... Training loss: 0.262 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.6% ... Training loss: 0.261 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.7% ... Training loss: 0.271 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.7% ... Training loss: 0.277 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.7% ... Training loss: 0.261 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.7% ... Training loss: 0.261 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.7% ... Training loss: 0.262 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.7% ... Training loss: 0.261 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.8% ... Training loss: 0.267 ... Validation loss: 0.440(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.8% ... Training loss: 0.262 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.8% ... Training loss: 0.261 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.8% ... Training loss: 0.267 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.8% ... Training loss: 0.274 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.8% ... Training loss: 0.270 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.8% ... Training loss: 0.261 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.9% ... Training loss: 0.267 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.9% ... Training loss: 0.274 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.9% ... Training loss: 0.269 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.9% ... Training loss: 0.266 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.9% ... Training loss: 0.279 ... Validation loss: 0.443(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 11.9% ... Training loss: 0.269 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.0% ... Training loss: 0.303 ... Validation loss: 0.458(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.0% ... Training loss: 0.279 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.0% ... Training loss: 0.261 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.0% ... Training loss: 0.263 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.0% ... Training loss: 0.265 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.1% ... Training loss: 0.260 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.1% ... Training loss: 0.273 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.1% ... Training loss: 0.261 ... Validation loss: 0.418(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.1% ... Training loss: 0.265 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.1% ... Training loss: 0.262 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.1% ... Training loss: 0.260 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.2% ... Training loss: 0.262 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.2% ... Training loss: 0.260 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.2% ... Training loss: 0.285 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.2% ... Training loss: 0.285 ... Validation loss: 0.455(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.2% ... Training loss: 0.260 ... Validation loss: 0.417(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.2% ... Training loss: 0.272 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.2% ... Training loss: 0.260 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.3% ... Training loss: 0.265 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.3% ... Training loss: 0.260 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.3% ... Training loss: 0.267 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.3% ... Training loss: 0.263 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.3% ... Training loss: 0.260 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.3% ... Training loss: 0.268 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.4% ... Training loss: 0.260 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.4% ... Training loss: 0.262 ... Validation loss: 0.417(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.4% ... Training loss: 0.261 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.4% ... Training loss: 0.261 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.4% ... Training loss: 0.260 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.4% ... Training loss: 0.259 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.5% ... Training loss: 0.266 ... Validation loss: 0.418(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.5% ... Training loss: 0.259 ... Validation loss: 0.418(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 12.5% ... Training loss: 0.289 ... Validation loss: 0.457(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.5% ... Training loss: 0.288 ... Validation loss: 0.454(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.5% ... Training loss: 0.266 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.6% ... Training loss: 0.259 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.6% ... Training loss: 0.264 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.6% ... Training loss: 0.260 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.6% ... Training loss: 0.270 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.6% ... Training loss: 0.263 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.6% ... Training loss: 0.262 ... Validation loss: 0.433(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.7% ... Training loss: 0.258 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.7% ... Training loss: 0.258 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.7% ... Training loss: 0.259 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.7% ... Training loss: 0.261 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.7% ... Training loss: 0.273 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.7% ... Training loss: 0.284 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.8% ... Training loss: 0.259 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.8% ... Training loss: 0.262 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.8% ... Training loss: 0.271 ... Validation loss: 0.443(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.8% ... Training loss: 0.259 ... Validation loss: 0.433(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.8% ... Training loss: 0.259 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.8% ... Training loss: 0.262 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.8% ... Training loss: 0.258 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.9% ... Training loss: 0.267 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.9% ... Training loss: 0.260 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.9% ... Training loss: 0.258 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.9% ... Training loss: 0.261 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.9% ... Training loss: 0.282 ... Validation loss: 0.462(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 12.9% ... Training loss: 0.274 ... Validation loss: 0.433(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.0% ... Training loss: 0.268 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.0% ... Training loss: 0.299 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.0% ... Training loss: 0.320 ... Validation loss: 0.497(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.0% ... Training loss: 0.282 ... Validation loss: 0.457(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.0% ... Training loss: 0.262 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.1% ... Training loss: 0.264 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.1% ... Training loss: 0.262 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.1% ... Training loss: 0.269 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.1% ... Training loss: 0.257 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.1% ... Training loss: 0.267 ... Validation loss: 0.433(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.1% ... Training loss: 0.264 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.2% ... Training loss: 0.267 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.2% ... Training loss: 0.259 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.2% ... Training loss: 0.258 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.2% ... Training loss: 0.259 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.2% ... Training loss: 0.257 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.2% ... Training loss: 0.257 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.2% ... Training loss: 0.258 ... Validation loss: 0.433(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.3% ... Training loss: 0.257 ... Validation loss: 0.432(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.3% ... Training loss: 0.257 ... Validation loss: 0.432(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.3% ... Training loss: 0.281 ... Validation loss: 0.463(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.3% ... Training loss: 0.262 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.3% ... Training loss: 0.261 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.3% ... Training loss: 0.258 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.4% ... Training loss: 0.260 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.4% ... Training loss: 0.278 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.4% ... Training loss: 0.259 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.4% ... Training loss: 0.256 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.4% ... Training loss: 0.256 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.4% ... Training loss: 0.256 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.5% ... Training loss: 0.261 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.5% ... Training loss: 0.255 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.5% ... Training loss: 0.277 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.5% ... Training loss: 0.269 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.5% ... Training loss: 0.273 ... Validation loss: 0.438(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.6% ... Training loss: 0.266 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.6% ... Training loss: 0.258 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.6% ... Training loss: 0.260 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.6% ... Training loss: 0.264 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.6% ... Training loss: 0.258 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.6% ... Training loss: 0.259 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.7% ... Training loss: 0.266 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.7% ... Training loss: 0.267 ... Validation loss: 0.443(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.7% ... Training loss: 0.282 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.7% ... Training loss: 0.265 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.7% ... Training loss: 0.259 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.7% ... Training loss: 0.262 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.8% ... Training loss: 0.299 ... Validation loss: 0.475(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.8% ... Training loss: 0.266 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.8% ... Training loss: 0.263 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.8% ... Training loss: 0.271 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.8% ... Training loss: 0.256 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.8% ... Training loss: 0.259 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.8% ... Training loss: 0.255 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.9% ... Training loss: 0.260 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.9% ... Training loss: 0.272 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.9% ... Training loss: 0.266 ... Validation loss: 0.437(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.9% ... Training loss: 0.270 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.9% ... Training loss: 0.258 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 13.9% ... Training loss: 0.272 ... Validation loss: 0.455(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.0% ... Training loss: 0.289 ... Validation loss: 0.456(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.0% ... Training loss: 0.264 ... Validation loss: 0.443(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.0% ... Training loss: 0.263 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.0% ... Training loss: 0.263 ... Validation loss: 0.443(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.0% ... Training loss: 0.260 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 14.1% ... Training loss: 0.260 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.1% ... Training loss: 0.271 ... Validation loss: 0.447(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.1% ... Training loss: 0.255 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.1% ... Training loss: 0.295 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.1% ... Training loss: 0.259 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.1% ... Training loss: 0.264 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.2% ... Training loss: 0.255 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.2% ... Training loss: 0.255 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.2% ... Training loss: 0.256 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.2% ... Training loss: 0.256 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.2% ... Training loss: 0.256 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.2% ... Training loss: 0.260 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.2% ... Training loss: 0.265 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.3% ... Training loss: 0.257 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.3% ... Training loss: 0.255 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.3% ... Training loss: 0.258 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.3% ... Training loss: 0.262 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.3% ... Training loss: 0.274 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.3% ... Training loss: 0.264 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.4% ... Training loss: 0.265 ... Validation loss: 0.449(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.4% ... Training loss: 0.255 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.4% ... Training loss: 0.255 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.4% ... Training loss: 0.261 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.4% ... Training loss: 0.260 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.4% ... Training loss: 0.281 ... Validation loss: 0.446(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.5% ... Training loss: 0.264 ... Validation loss: 0.433(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.5% ... Training loss: 0.276 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.5% ... Training loss: 0.257 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.5% ... Training loss: 0.266 ... Validation loss: 0.432(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.5% ... Training loss: 0.263 ... Validation loss: 0.433(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.6% ... Training loss: 0.256 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.6% ... Training loss: 0.254 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.6% ... Training loss: 0.256 ... Validation loss: 0.432(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.6% ... Training loss: 0.261 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.6% ... Training loss: 0.265 ... Validation loss: 0.448(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.6% ... Training loss: 0.268 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.7% ... Training loss: 0.269 ... Validation loss: 0.433(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.7% ... Training loss: 0.268 ... Validation loss: 0.440(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.7% ... Training loss: 0.256 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.7% ... Training loss: 0.258 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.7% ... Training loss: 0.255 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.7% ... Training loss: 0.255 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.8% ... Training loss: 0.258 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.8% ... Training loss: 0.257 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.8% ... Training loss: 0.256 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.8% ... Training loss: 0.255 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.8% ... Training loss: 0.253 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.8% ... Training loss: 0.257 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.8% ... Training loss: 0.258 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.9% ... Training loss: 0.257 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.9% ... Training loss: 0.256 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.9% ... Training loss: 0.254 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.9% ... Training loss: 0.309 ... Validation loss: 0.464(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.9% ... Training loss: 0.258 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 14.9% ... Training loss: 0.265 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.0% ... Training loss: 0.266 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.0% ... Training loss: 0.254 ... Validation loss: 0.417(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.0% ... Training loss: 0.254 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.0% ... Training loss: 0.252 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.0% ... Training loss: 0.254 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.1% ... Training loss: 0.253 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.1% ... Training loss: 0.270 ... Validation loss: 0.442(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.1% ... Training loss: 0.260 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.1% ... Training loss: 0.251 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.1% ... Training loss: 0.255 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.1% ... Training loss: 0.280 ... Validation loss: 0.446(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.2% ... Training loss: 0.251 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.2% ... Training loss: 0.256 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.2% ... Training loss: 0.251 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.2% ... Training loss: 0.252 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.2% ... Training loss: 0.251 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.2% ... Training loss: 0.253 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.2% ... Training loss: 0.275 ... Validation loss: 0.439(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.3% ... Training loss: 0.252 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.3% ... Training loss: 0.254 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.3% ... Training loss: 0.251 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.3% ... Training loss: 0.271 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.3% ... Training loss: 0.257 ... Validation loss: 0.434(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.3% ... Training loss: 0.252 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.4% ... Training loss: 0.255 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.4% ... Training loss: 0.257 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.4% ... Training loss: 0.258 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.4% ... Training loss: 0.253 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.4% ... Training loss: 0.256 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.4% ... Training loss: 0.258 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.5% ... Training loss: 0.255 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.5% ... Training loss: 0.255 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.5% ... Training loss: 0.253 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.5% ... Training loss: 0.261 ... Validation loss: 0.429(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.5% ... Training loss: 0.252 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.6% ... Training loss: 0.260 ... Validation loss: 0.418(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.6% ... Training loss: 0.276 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.6% ... Training loss: 0.259 ... Validation loss: 0.427(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 15.6% ... Training loss: 0.263 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.6% ... Training loss: 0.251 ... Validation loss: 0.411(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.6% ... Training loss: 0.261 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.7% ... Training loss: 0.254 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.7% ... Training loss: 0.255 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.7% ... Training loss: 0.250 ... Validation loss: 0.415(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.7% ... Training loss: 0.261 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.7% ... Training loss: 0.253 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.7% ... Training loss: 0.251 ... Validation loss: 0.416(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.8% ... Training loss: 0.263 ... Validation loss: 0.428(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.8% ... Training loss: 0.254 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.8% ... Training loss: 0.250 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.8% ... Training loss: 0.250 ... Validation loss: 0.411(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.8% ... Training loss: 0.249 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.8% ... Training loss: 0.256 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.8% ... Training loss: 0.281 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.9% ... Training loss: 0.251 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.9% ... Training loss: 0.252 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.9% ... Training loss: 0.250 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.9% ... Training loss: 0.250 ... Validation loss: 0.415(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.9% ... Training loss: 0.250 ... Validation loss: 0.417(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 15.9% ... Training loss: 0.252 ... Validation loss: 0.415(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.0% ... Training loss: 0.250 ... Validation loss: 0.415(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.0% ... Training loss: 0.254 ... Validation loss: 0.416(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.0% ... Training loss: 0.254 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.0% ... Training loss: 0.252 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.0% ... Training loss: 0.251 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.1% ... Training loss: 0.263 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.1% ... Training loss: 0.252 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.1% ... Training loss: 0.250 ... Validation loss: 0.406(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.1% ... Training loss: 0.263 ... Validation loss: 0.411(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.1% ... Training loss: 0.255 ... Validation loss: 0.418(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.1% ... Training loss: 0.261 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.1% ... Training loss: 0.250 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.2% ... Training loss: 0.251 ... Validation loss: 0.412(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.2% ... Training loss: 0.251 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.2% ... Training loss: 0.252 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.2% ... Training loss: 0.252 ... Validation loss: 0.412(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.2% ... Training loss: 0.256 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.2% ... Training loss: 0.267 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.3% ... Training loss: 0.295 ... Validation loss: 0.462(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.3% ... Training loss: 0.261 ... Validation loss: 0.432(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.3% ... Training loss: 0.249 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.3% ... Training loss: 0.248 ... Validation loss: 0.412(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.3% ... Training loss: 0.255 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.4% ... Training loss: 0.254 ... Validation loss: 0.426(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.4% ... Training loss: 0.262 ... Validation loss: 0.432(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.4% ... Training loss: 0.250 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.4% ... Training loss: 0.279 ... Validation loss: 0.445(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.4% ... Training loss: 0.250 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.4% ... Training loss: 0.252 ... Validation loss: 0.415(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.4% ... Training loss: 0.258 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.5% ... Training loss: 0.249 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.5% ... Training loss: 0.251 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.5% ... Training loss: 0.262 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.5% ... Training loss: 0.255 ... Validation loss: 0.418(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.5% ... Training loss: 0.271 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.6% ... Training loss: 0.254 ... Validation loss: 0.407(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.6% ... Training loss: 0.250 ... Validation loss: 0.411(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.6% ... Training loss: 0.254 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.6% ... Training loss: 0.253 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.6% ... Training loss: 0.248 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.6% ... Training loss: 0.253 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.6% ... Training loss: 0.253 ... Validation loss: 0.407(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.7% ... Training loss: 0.252 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.7% ... Training loss: 0.265 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.7% ... Training loss: 0.251 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.7% ... Training loss: 0.247 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.7% ... Training loss: 0.247 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.8% ... Training loss: 0.248 ... Validation loss: 0.411(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.8% ... Training loss: 0.248 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.8% ... Training loss: 0.255 ... Validation loss: 0.417(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.8% ... Training loss: 0.249 ... Validation loss: 0.412(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.8% ... Training loss: 0.250 ... Validation loss: 0.412(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.8% ... Training loss: 0.255 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.9% ... Training loss: 0.248 ... Validation loss: 0.417(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.9% ... Training loss: 0.252 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.9% ... Training loss: 0.247 ... Validation loss: 0.415(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.9% ... Training loss: 0.249 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.9% ... Training loss: 0.250 ... Validation loss: 0.409(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.9% ... Training loss: 0.246 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 16.9% ... Training loss: 0.247 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.0% ... Training loss: 0.246 ... Validation loss: 0.408(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.0% ... Training loss: 0.253 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.0% ... Training loss: 0.251 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.0% ... Training loss: 0.247 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.0% ... Training loss: 0.248 ... Validation loss: 0.402(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.1% ... Training loss: 0.251 ... Validation loss: 0.407(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.1% ... Training loss: 0.247 ... Validation loss: 0.400(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.1% ... Training loss: 0.247 ... Validation loss: 0.399(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.1% ... Training loss: 0.247 ... Validation loss: 0.397(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.1% ... Training loss: 0.246 ... Validation loss: 0.400(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.1% ... Training loss: 0.248 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 17.1% ... Training loss: 0.252 ... Validation loss: 0.408(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.2% ... Training loss: 0.254 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.2% ... Training loss: 0.246 ... Validation loss: 0.405(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.2% ... Training loss: 0.253 ... Validation loss: 0.411(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.2% ... Training loss: 0.249 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.2% ... Training loss: 0.264 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.2% ... Training loss: 0.249 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.3% ... Training loss: 0.245 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.3% ... Training loss: 0.251 ... Validation loss: 0.407(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.3% ... Training loss: 0.247 ... Validation loss: 0.408(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.3% ... Training loss: 0.248 ... Validation loss: 0.406(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.3% ... Training loss: 0.246 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.4% ... Training loss: 0.246 ... Validation loss: 0.411(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.4% ... Training loss: 0.247 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.4% ... Training loss: 0.246 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.4% ... Training loss: 0.249 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.4% ... Training loss: 0.248 ... Validation loss: 0.415(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.4% ... Training loss: 0.246 ... Validation loss: 0.412(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.4% ... Training loss: 0.250 ... Validation loss: 0.415(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.5% ... Training loss: 0.265 ... Validation loss: 0.431(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.5% ... Training loss: 0.246 ... Validation loss: 0.417(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.5% ... Training loss: 0.268 ... Validation loss: 0.438(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.5% ... Training loss: 0.267 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.5% ... Training loss: 0.271 ... Validation loss: 0.440(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.6% ... Training loss: 0.288 ... Validation loss: 0.444(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.6% ... Training loss: 0.244 ... Validation loss: 0.408(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.6% ... Training loss: 0.244 ... Validation loss: 0.409(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.6% ... Training loss: 0.246 ... Validation loss: 0.411(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.6% ... Training loss: 0.251 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.6% ... Training loss: 0.250 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.6% ... Training loss: 0.248 ... Validation loss: 0.407(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.7% ... Training loss: 0.243 ... Validation loss: 0.406(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.7% ... Training loss: 0.254 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.7% ... Training loss: 0.252 ... Validation loss: 0.406(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.7% ... Training loss: 0.248 ... Validation loss: 0.398(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.7% ... Training loss: 0.244 ... Validation loss: 0.398(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.8% ... Training loss: 0.246 ... Validation loss: 0.407(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.8% ... Training loss: 0.251 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.8% ... Training loss: 0.255 ... Validation loss: 0.398(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.8% ... Training loss: 0.247 ... Validation loss: 0.400(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.8% ... Training loss: 0.243 ... Validation loss: 0.400(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.8% ... Training loss: 0.243 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.9% ... Training loss: 0.245 ... Validation loss: 0.402(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.9% ... Training loss: 0.243 ... Validation loss: 0.402(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.9% ... Training loss: 0.244 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.9% ... Training loss: 0.267 ... Validation loss: 0.399(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.9% ... Training loss: 0.254 ... Validation loss: 0.404(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.9% ... Training loss: 0.288 ... Validation loss: 0.451(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 17.9% ... Training loss: 0.301 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.0% ... Training loss: 0.278 ... Validation loss: 0.436(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.0% ... Training loss: 0.247 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.0% ... Training loss: 0.290 ... Validation loss: 0.441(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.0% ... Training loss: 0.256 ... Validation loss: 0.417(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.0% ... Training loss: 0.245 ... Validation loss: 0.405(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.1% ... Training loss: 0.255 ... Validation loss: 0.424(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.1% ... Training loss: 0.245 ... Validation loss: 0.402(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.1% ... Training loss: 0.256 ... Validation loss: 0.408(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.1% ... Training loss: 0.252 ... Validation loss: 0.420(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.1% ... Training loss: 0.273 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.1% ... Training loss: 0.244 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.1% ... Training loss: 0.245 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.2% ... Training loss: 0.245 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.2% ... Training loss: 0.246 ... Validation loss: 0.412(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.2% ... Training loss: 0.244 ... Validation loss: 0.404(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.2% ... Training loss: 0.247 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.2% ... Training loss: 0.244 ... Validation loss: 0.404(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.2% ... Training loss: 0.242 ... Validation loss: 0.405(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.3% ... Training loss: 0.242 ... Validation loss: 0.405(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.3% ... Training loss: 0.241 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.3% ... Training loss: 0.247 ... Validation loss: 0.412(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.3% ... Training loss: 0.250 ... Validation loss: 0.404(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.3% ... Training loss: 0.242 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.4% ... Training loss: 0.243 ... Validation loss: 0.408(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.4% ... Training loss: 0.252 ... Validation loss: 0.405(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.4% ... Training loss: 0.240 ... Validation loss: 0.402(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.4% ... Training loss: 0.241 ... Validation loss: 0.402(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.4% ... Training loss: 0.244 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.4% ... Training loss: 0.246 ... Validation loss: 0.407(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.4% ... Training loss: 0.240 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.5% ... Training loss: 0.254 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.5% ... Training loss: 0.250 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.5% ... Training loss: 0.239 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.5% ... Training loss: 0.249 ... Validation loss: 0.412(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.5% ... Training loss: 0.250 ... Validation loss: 0.409(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.6% ... Training loss: 0.248 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.6% ... Training loss: 0.242 ... Validation loss: 0.404(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.6% ... Training loss: 0.244 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.6% ... Training loss: 0.249 ... Validation loss: 0.408(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.6% ... Training loss: 0.261 ... Validation loss: 0.435(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.6% ... Training loss: 0.241 ... Validation loss: 0.406(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.6% ... Training loss: 0.241 ... Validation loss: 0.405(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.7% ... Training loss: 0.241 ... Validation loss: 0.406(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.7% ... Training loss: 0.249 ... Validation loss: 0.418(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 18.7% ... Training loss: 0.241 ... Validation loss: 0.400(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.7% ... Training loss: 0.242 ... Validation loss: 0.408(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.7% ... Training loss: 0.241 ... Validation loss: 0.405(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.8% ... Training loss: 0.240 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.8% ... Training loss: 0.246 ... Validation loss: 0.412(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.8% ... Training loss: 0.242 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.8% ... Training loss: 0.244 ... Validation loss: 0.405(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.8% ... Training loss: 0.240 ... Validation loss: 0.398(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.8% ... Training loss: 0.249 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.9% ... Training loss: 0.240 ... Validation loss: 0.399(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.9% ... Training loss: 0.239 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.9% ... Training loss: 0.239 ... Validation loss: 0.399(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.9% ... Training loss: 0.240 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.9% ... Training loss: 0.250 ... Validation loss: 0.413(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.9% ... Training loss: 0.241 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 18.9% ... Training loss: 0.238 ... Validation loss: 0.400(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.0% ... Training loss: 0.238 ... Validation loss: 0.399(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.0% ... Training loss: 0.241 ... Validation loss: 0.404(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.0% ... Training loss: 0.240 ... Validation loss: 0.402(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.0% ... Training loss: 0.239 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.0% ... Training loss: 0.246 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.1% ... Training loss: 0.243 ... Validation loss: 0.406(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.1% ... Training loss: 0.270 ... Validation loss: 0.422(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.1% ... Training loss: 0.240 ... Validation loss: 0.399(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.1% ... Training loss: 0.242 ... Validation loss: 0.398(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.1% ... Training loss: 0.238 ... Validation loss: 0.393(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.1% ... Training loss: 0.252 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.1% ... Training loss: 0.238 ... Validation loss: 0.390(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.2% ... Training loss: 0.238 ... Validation loss: 0.393(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.2% ... Training loss: 0.238 ... Validation loss: 0.393(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.2% ... Training loss: 0.243 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.2% ... Training loss: 0.247 ... Validation loss: 0.402(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.2% ... Training loss: 0.243 ... Validation loss: 0.414(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.2% ... Training loss: 0.238 ... Validation loss: 0.398(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.3% ... Training loss: 0.242 ... Validation loss: 0.399(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.3% ... Training loss: 0.242 ... Validation loss: 0.404(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.3% ... Training loss: 0.237 ... Validation loss: 0.390(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.3% ... Training loss: 0.244 ... Validation loss: 0.396(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.3% ... Training loss: 0.242 ... Validation loss: 0.386(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.4% ... Training loss: 0.242 ... Validation loss: 0.388(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.4% ... Training loss: 0.241 ... Validation loss: 0.395(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.4% ... Training loss: 0.237 ... Validation loss: 0.395(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.4% ... Training loss: 0.238 ... Validation loss: 0.394(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.4% ... Training loss: 0.243 ... Validation loss: 0.400(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.4% ... Training loss: 0.241 ... Validation loss: 0.398(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.4% ... Training loss: 0.267 ... Validation loss: 0.423(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.5% ... Training loss: 0.237 ... Validation loss: 0.394(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.5% ... Training loss: 0.250 ... Validation loss: 0.412(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.5% ... Training loss: 0.236 ... Validation loss: 0.397(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.5% ... Training loss: 0.266 ... Validation loss: 0.430(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.5% ... Training loss: 0.262 ... Validation loss: 0.417(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.6% ... Training loss: 0.249 ... Validation loss: 0.411(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.6% ... Training loss: 0.245 ... Validation loss: 0.399(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.6% ... Training loss: 0.250 ... Validation loss: 0.405(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.6% ... Training loss: 0.244 ... Validation loss: 0.400(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.6% ... Training loss: 0.249 ... Validation loss: 0.402(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.6% ... Training loss: 0.238 ... Validation loss: 0.396(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.6% ... Training loss: 0.236 ... Validation loss: 0.398(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.7% ... Training loss: 0.250 ... Validation loss: 0.402(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.7% ... Training loss: 0.350 ... Validation loss: 0.485(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.7% ... Training loss: 0.249 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.7% ... Training loss: 0.239 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.7% ... Training loss: 0.244 ... Validation loss: 0.400(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.8% ... Training loss: 0.266 ... Validation loss: 0.419(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.8% ... Training loss: 0.257 ... Validation loss: 0.406(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.8% ... Training loss: 0.235 ... Validation loss: 0.393(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.8% ... Training loss: 0.235 ... Validation loss: 0.390(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.8% ... Training loss: 0.238 ... Validation loss: 0.392(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.8% ... Training loss: 0.267 ... Validation loss: 0.416(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.9% ... Training loss: 0.249 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.9% ... Training loss: 0.240 ... Validation loss: 0.388(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.9% ... Training loss: 0.235 ... Validation loss: 0.387(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.9% ... Training loss: 0.240 ... Validation loss: 0.384(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.9% ... Training loss: 0.241 ... Validation loss: 0.381(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.9% ... Training loss: 0.246 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 19.9% ... Training loss: 0.247 ... Validation loss: 0.405(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.0% ... Training loss: 0.242 ... Validation loss: 0.395(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.0% ... Training loss: 0.255 ... Validation loss: 0.411(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.0% ... Training loss: 0.248 ... Validation loss: 0.410(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.0% ... Training loss: 0.237 ... Validation loss: 0.394(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.0% ... Training loss: 0.262 ... Validation loss: 0.411(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.1% ... Training loss: 0.251 ... Validation loss: 0.407(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.1% ... Training loss: 0.234 ... Validation loss: 0.391(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.1% ... Training loss: 0.249 ... Validation loss: 0.405(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.1% ... Training loss: 0.242 ... Validation loss: 0.392(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.1% ... Training loss: 0.258 ... Validation loss: 0.425(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.1% ... Training loss: 0.234 ... Validation loss: 0.396(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.1% ... Training loss: 0.234 ... Validation loss: 0.397(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.2% ... Training loss: 0.243 ... Validation loss: 0.397(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.2% ... Training loss: 0.233 ... Validation loss: 0.390(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.2% ... Training loss: 0.233 ... Validation loss: 0.393(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.2% ... Training loss: 0.232 ... Validation loss: 0.391(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.2% ... Training loss: 0.232 ... Validation loss: 0.394(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 20.2% ... Training loss: 0.235 ... Validation loss: 0.396(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.3% ... Training loss: 0.248 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.3% ... Training loss: 0.241 ... Validation loss: 0.394(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.3% ... Training loss: 0.236 ... Validation loss: 0.392(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.3% ... Training loss: 0.234 ... Validation loss: 0.391(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.3% ... Training loss: 0.233 ... Validation loss: 0.392(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.4% ... Training loss: 0.232 ... Validation loss: 0.394(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.4% ... Training loss: 0.240 ... Validation loss: 0.394(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.4% ... Training loss: 0.266 ... Validation loss: 0.418(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.4% ... Training loss: 0.261 ... Validation loss: 0.421(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.4% ... Training loss: 0.270 ... Validation loss: 0.416(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.4% ... Training loss: 0.241 ... Validation loss: 0.403(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.4% ... Training loss: 0.234 ... Validation loss: 0.393(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.5% ... Training loss: 0.238 ... Validation loss: 0.387(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.5% ... Training loss: 0.232 ... Validation loss: 0.388(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.5% ... Training loss: 0.241 ... Validation loss: 0.391(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.5% ... Training loss: 0.235 ... Validation loss: 0.389(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.5% ... Training loss: 0.231 ... Validation loss: 0.386(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.6% ... Training loss: 0.235 ... Validation loss: 0.391(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.6% ... Training loss: 0.304 ... Validation loss: 0.472(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.6% ... Training loss: 0.239 ... Validation loss: 0.386(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.6% ... Training loss: 0.231 ... Validation loss: 0.382(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.6% ... Training loss: 0.234 ... Validation loss: 0.386(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.6% ... Training loss: 0.235 ... Validation loss: 0.387(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.6% ... Training loss: 0.235 ... Validation loss: 0.386(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.7% ... Training loss: 0.239 ... Validation loss: 0.386(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.7% ... Training loss: 0.235 ... Validation loss: 0.382(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.7% ... Training loss: 0.244 ... Validation loss: 0.392(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.7% ... Training loss: 0.238 ... Validation loss: 0.393(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.7% ... Training loss: 0.231 ... Validation loss: 0.386(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.8% ... Training loss: 0.235 ... Validation loss: 0.386(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.8% ... Training loss: 0.231 ... Validation loss: 0.386(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.8% ... Training loss: 0.232 ... Validation loss: 0.390(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.8% ... Training loss: 0.233 ... Validation loss: 0.383(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.8% ... Training loss: 0.238 ... Validation loss: 0.389(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.8% ... Training loss: 0.242 ... Validation loss: 0.397(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.9% ... Training loss: 0.231 ... Validation loss: 0.383(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.9% ... Training loss: 0.230 ... Validation loss: 0.385(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.9% ... Training loss: 0.242 ... Validation loss: 0.395(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.9% ... Training loss: 0.245 ... Validation loss: 0.395(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.9% ... Training loss: 0.234 ... Validation loss: 0.390(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.9% ... Training loss: 0.228 ... Validation loss: 0.379(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 20.9% ... Training loss: 0.229 ... Validation loss: 0.380(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.0% ... Training loss: 0.230 ... Validation loss: 0.381(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.0% ... Training loss: 0.228 ... Validation loss: 0.378(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.0% ... Training loss: 0.250 ... Validation loss: 0.395(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.0% ... Training loss: 0.236 ... Validation loss: 0.392(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.0% ... Training loss: 0.230 ... Validation loss: 0.380(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.1% ... Training loss: 0.230 ... Validation loss: 0.380(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.1% ... Training loss: 0.236 ... Validation loss: 0.390(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.1% ... Training loss: 0.232 ... Validation loss: 0.384(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.1% ... Training loss: 0.237 ... Validation loss: 0.383(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.1% ... Training loss: 0.233 ... Validation loss: 0.390(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.1% ... Training loss: 0.226 ... Validation loss: 0.379(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.1% ... Training loss: 0.242 ... Validation loss: 0.394(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.2% ... Training loss: 0.229 ... Validation loss: 0.382(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.2% ... Training loss: 0.228 ... Validation loss: 0.378(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.2% ... Training loss: 0.236 ... Validation loss: 0.380(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.2% ... Training loss: 0.232 ... Validation loss: 0.380(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.2% ... Training loss: 0.229 ... Validation loss: 0.379(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.2% ... Training loss: 0.229 ... Validation loss: 0.381(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.3% ... Training loss: 0.229 ... Validation loss: 0.381(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.3% ... Training loss: 0.256 ... Validation loss: 0.396(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.3% ... Training loss: 0.246 ... Validation loss: 0.398(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.3% ... Training loss: 0.269 ... Validation loss: 0.409(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.3% ... Training loss: 0.229 ... Validation loss: 0.381(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.4% ... Training loss: 0.227 ... Validation loss: 0.380(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.4% ... Training loss: 0.225 ... Validation loss: 0.378(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.4% ... Training loss: 0.225 ... Validation loss: 0.375(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.4% ... Training loss: 0.225 ... Validation loss: 0.375(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.4% ... Training loss: 0.228 ... Validation loss: 0.382(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.4% ... Training loss: 0.225 ... Validation loss: 0.376(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.4% ... Training loss: 0.231 ... Validation loss: 0.381(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.5% ... Training loss: 0.225 ... Validation loss: 0.381(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.5% ... Training loss: 0.228 ... Validation loss: 0.385(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.5% ... Training loss: 0.225 ... Validation loss: 0.377(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.5% ... Training loss: 0.228 ... Validation loss: 0.382(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.5% ... Training loss: 0.256 ... Validation loss: 0.402(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.6% ... Training loss: 0.230 ... Validation loss: 0.384(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.6% ... Training loss: 0.224 ... Validation loss: 0.376(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.6% ... Training loss: 0.224 ... Validation loss: 0.375(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.6% ... Training loss: 0.226 ... Validation loss: 0.380(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.6% ... Training loss: 0.224 ... Validation loss: 0.374(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.6% ... Training loss: 0.224 ... Validation loss: 0.375(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.6% ... Training loss: 0.240 ... Validation loss: 0.395(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.7% ... Training loss: 0.248 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.7% ... Training loss: 0.255 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.7% ... Training loss: 0.226 ... Validation loss: 0.378(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.7% ... Training loss: 0.225 ... Validation loss: 0.379(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.7% ... Training loss: 0.224 ... Validation loss: 0.376(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.8% ... Training loss: 0.225 ... Validation loss: 0.374(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.8% ... Training loss: 0.224 ... Validation loss: 0.372(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.8% ... Training loss: 0.229 ... Validation loss: 0.375(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 21.8% ... Training loss: 0.225 ... Validation loss: 0.371(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.8% ... Training loss: 0.228 ... Validation loss: 0.371(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.8% ... Training loss: 0.237 ... Validation loss: 0.392(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.9% ... Training loss: 0.224 ... Validation loss: 0.373(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.9% ... Training loss: 0.236 ... Validation loss: 0.376(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.9% ... Training loss: 0.227 ... Validation loss: 0.370(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.9% ... Training loss: 0.223 ... Validation loss: 0.374(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.9% ... Training loss: 0.229 ... Validation loss: 0.384(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.9% ... Training loss: 0.229 ... Validation loss: 0.371(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 21.9% ... Training loss: 0.221 ... Validation loss: 0.367(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.0% ... Training loss: 0.228 ... Validation loss: 0.379(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.0% ... Training loss: 0.222 ... Validation loss: 0.367(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.0% ... Training loss: 0.220 ... Validation loss: 0.368(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.0% ... Training loss: 0.222 ... Validation loss: 0.368(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.0% ... Training loss: 0.247 ... Validation loss: 0.401(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.1% ... Training loss: 0.239 ... Validation loss: 0.374(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.1% ... Training loss: 0.221 ... Validation loss: 0.370(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.1% ... Training loss: 0.221 ... Validation loss: 0.374(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.1% ... Training loss: 0.221 ... Validation loss: 0.368(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.1% ... Training loss: 0.225 ... Validation loss: 0.376(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.1% ... Training loss: 0.240 ... Validation loss: 0.385(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.1% ... Training loss: 0.228 ... Validation loss: 0.370(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.2% ... Training loss: 0.221 ... Validation loss: 0.366(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.2% ... Training loss: 0.231 ... Validation loss: 0.380(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.2% ... Training loss: 0.221 ... Validation loss: 0.368(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.2% ... Training loss: 0.231 ... Validation loss: 0.375(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.2% ... Training loss: 0.242 ... Validation loss: 0.383(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.2% ... Training loss: 0.226 ... Validation loss: 0.379(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.3% ... Training loss: 0.220 ... Validation loss: 0.369(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.3% ... Training loss: 0.220 ... Validation loss: 0.368(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.3% ... Training loss: 0.236 ... Validation loss: 0.390(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.3% ... Training loss: 0.224 ... Validation loss: 0.369(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.3% ... Training loss: 0.220 ... Validation loss: 0.369(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.4% ... Training loss: 0.219 ... Validation loss: 0.365(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.4% ... Training loss: 0.224 ... Validation loss: 0.372(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.4% ... Training loss: 0.219 ... Validation loss: 0.366(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.4% ... Training loss: 0.220 ... Validation loss: 0.371(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.4% ... Training loss: 0.224 ... Validation loss: 0.369(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.4% ... Training loss: 0.221 ... Validation loss: 0.367(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.4% ... Training loss: 0.243 ... Validation loss: 0.386(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.5% ... Training loss: 0.224 ... Validation loss: 0.377(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.5% ... Training loss: 0.220 ... Validation loss: 0.368(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.5% ... Training loss: 0.223 ... Validation loss: 0.377(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.5% ... Training loss: 0.223 ... Validation loss: 0.369(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.5% ... Training loss: 0.224 ... Validation loss: 0.374(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.6% ... Training loss: 0.232 ... Validation loss: 0.372(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.6% ... Training loss: 0.228 ... Validation loss: 0.371(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.6% ... Training loss: 0.227 ... Validation loss: 0.374(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.6% ... Training loss: 0.233 ... Validation loss: 0.380(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.6% ... Training loss: 0.219 ... Validation loss: 0.368(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.6% ... Training loss: 0.225 ... Validation loss: 0.374(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.6% ... Training loss: 0.225 ... Validation loss: 0.370(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.7% ... Training loss: 0.221 ... Validation loss: 0.372(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.7% ... Training loss: 0.221 ... Validation loss: 0.370(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.7% ... Training loss: 0.256 ... Validation loss: 0.391(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.7% ... Training loss: 0.222 ... Validation loss: 0.372(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.7% ... Training loss: 0.221 ... Validation loss: 0.359(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.8% ... Training loss: 0.218 ... Validation loss: 0.359(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.8% ... Training loss: 0.222 ... Validation loss: 0.365(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.8% ... Training loss: 0.216 ... Validation loss: 0.359(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.8% ... Training loss: 0.217 ... Validation loss: 0.360(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.8% ... Training loss: 0.218 ... Validation loss: 0.360(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.8% ... Training loss: 0.219 ... Validation loss: 0.364(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.9% ... Training loss: 0.218 ... Validation loss: 0.362(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.9% ... Training loss: 0.215 ... Validation loss: 0.361(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.9% ... Training loss: 0.233 ... Validation loss: 0.379(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.9% ... Training loss: 0.227 ... Validation loss: 0.371(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.9% ... Training loss: 0.217 ... Validation loss: 0.362(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.9% ... Training loss: 0.216 ... Validation loss: 0.360(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 22.9% ... Training loss: 0.214 ... Validation loss: 0.359(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.0% ... Training loss: 0.215 ... Validation loss: 0.360(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.0% ... Training loss: 0.218 ... Validation loss: 0.357(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.0% ... Training loss: 0.218 ... Validation loss: 0.361(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.0% ... Training loss: 0.216 ... Validation loss: 0.358(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.0% ... Training loss: 0.229 ... Validation loss: 0.364(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.1% ... Training loss: 0.220 ... Validation loss: 0.359(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.1% ... Training loss: 0.218 ... Validation loss: 0.358(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.1% ... Training loss: 0.215 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.1% ... Training loss: 0.216 ... Validation loss: 0.354(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.1% ... Training loss: 0.214 ... Validation loss: 0.353(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.1% ... Training loss: 0.216 ... Validation loss: 0.357(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.1% ... Training loss: 0.239 ... Validation loss: 0.386(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.2% ... Training loss: 0.223 ... Validation loss: 0.361(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.2% ... Training loss: 0.217 ... Validation loss: 0.358(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.2% ... Training loss: 0.216 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.2% ... Training loss: 0.216 ... Validation loss: 0.359(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.2% ... Training loss: 0.228 ... Validation loss: 0.361(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.2% ... Training loss: 0.216 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.3% ... Training loss: 0.216 ... Validation loss: 0.361(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.3% ... Training loss: 0.216 ... Validation loss: 0.361(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.3% ... Training loss: 0.224 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.3% ... Training loss: 0.214 ... Validation loss: 0.356(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.3% ... Training loss: 0.212 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 23.4% ... Training loss: 0.213 ... Validation loss: 0.353(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.4% ... Training loss: 0.212 ... Validation loss: 0.353(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.4% ... Training loss: 0.215 ... Validation loss: 0.360(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.4% ... Training loss: 0.227 ... Validation loss: 0.366(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.4% ... Training loss: 0.231 ... Validation loss: 0.383(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.4% ... Training loss: 0.224 ... Validation loss: 0.358(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.4% ... Training loss: 0.212 ... Validation loss: 0.358(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.5% ... Training loss: 0.213 ... Validation loss: 0.356(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.5% ... Training loss: 0.229 ... Validation loss: 0.376(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.5% ... Training loss: 0.234 ... Validation loss: 0.376(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.5% ... Training loss: 0.257 ... Validation loss: 0.391(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.5% ... Training loss: 0.225 ... Validation loss: 0.369(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.6% ... Training loss: 0.212 ... Validation loss: 0.356(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.6% ... Training loss: 0.215 ... Validation loss: 0.358(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.6% ... Training loss: 0.230 ... Validation loss: 0.371(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.6% ... Training loss: 0.230 ... Validation loss: 0.370(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.6% ... Training loss: 0.244 ... Validation loss: 0.380(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.6% ... Training loss: 0.213 ... Validation loss: 0.357(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.6% ... Training loss: 0.222 ... Validation loss: 0.360(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.7% ... Training loss: 0.238 ... Validation loss: 0.380(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.7% ... Training loss: 0.212 ... Validation loss: 0.354(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.7% ... Training loss: 0.210 ... Validation loss: 0.353(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.7% ... Training loss: 0.214 ... Validation loss: 0.360(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.7% ... Training loss: 0.212 ... Validation loss: 0.356(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.8% ... Training loss: 0.211 ... Validation loss: 0.353(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.8% ... Training loss: 0.218 ... Validation loss: 0.357(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.8% ... Training loss: 0.215 ... Validation loss: 0.367(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.8% ... Training loss: 0.211 ... Validation loss: 0.357(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.8% ... Training loss: 0.213 ... Validation loss: 0.353(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.8% ... Training loss: 0.209 ... Validation loss: 0.352(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.9% ... Training loss: 0.213 ... Validation loss: 0.362(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.9% ... Training loss: 0.211 ... Validation loss: 0.362(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.9% ... Training loss: 0.216 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.9% ... Training loss: 0.209 ... Validation loss: 0.357(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.9% ... Training loss: 0.211 ... Validation loss: 0.357(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.9% ... Training loss: 0.209 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 23.9% ... Training loss: 0.217 ... Validation loss: 0.359(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.0% ... Training loss: 0.211 ... Validation loss: 0.363(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.0% ... Training loss: 0.213 ... Validation loss: 0.360(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.0% ... Training loss: 0.208 ... Validation loss: 0.358(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.0% ... Training loss: 0.210 ... Validation loss: 0.358(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.0% ... Training loss: 0.207 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.1% ... Training loss: 0.209 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.1% ... Training loss: 0.208 ... Validation loss: 0.358(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.1% ... Training loss: 0.210 ... Validation loss: 0.359(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.1% ... Training loss: 0.208 ... Validation loss: 0.358(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.1% ... Training loss: 0.207 ... Validation loss: 0.356(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.1% ... Training loss: 0.207 ... Validation loss: 0.356(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.1% ... Training loss: 0.233 ... Validation loss: 0.369(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.2% ... Training loss: 0.209 ... Validation loss: 0.357(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.2% ... Training loss: 0.234 ... Validation loss: 0.362(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.2% ... Training loss: 0.209 ... Validation loss: 0.357(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.2% ... Training loss: 0.207 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.2% ... Training loss: 0.207 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.2% ... Training loss: 0.206 ... Validation loss: 0.353(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.3% ... Training loss: 0.205 ... Validation loss: 0.351(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.3% ... Training loss: 0.208 ... Validation loss: 0.352(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.3% ... Training loss: 0.208 ... Validation loss: 0.349(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.3% ... Training loss: 0.206 ... Validation loss: 0.347(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.3% ... Training loss: 0.208 ... Validation loss: 0.351(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.4% ... Training loss: 0.205 ... Validation loss: 0.352(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.4% ... Training loss: 0.205 ... Validation loss: 0.351(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.4% ... Training loss: 0.205 ... Validation loss: 0.350(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.4% ... Training loss: 0.210 ... Validation loss: 0.362(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.4% ... Training loss: 0.213 ... Validation loss: 0.352(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.4% ... Training loss: 0.207 ... Validation loss: 0.357(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.4% ... Training loss: 0.219 ... Validation loss: 0.358(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.5% ... Training loss: 0.252 ... Validation loss: 0.400(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.5% ... Training loss: 0.206 ... Validation loss: 0.353(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.5% ... Training loss: 0.214 ... Validation loss: 0.361(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.5% ... Training loss: 0.207 ... Validation loss: 0.352(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.5% ... Training loss: 0.207 ... Validation loss: 0.349(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.6% ... Training loss: 0.207 ... Validation loss: 0.352(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.6% ... Training loss: 0.204 ... Validation loss: 0.350(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.6% ... Training loss: 0.214 ... Validation loss: 0.366(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.6% ... Training loss: 0.203 ... Validation loss: 0.347(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.6% ... Training loss: 0.203 ... Validation loss: 0.349(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.6% ... Training loss: 0.211 ... Validation loss: 0.357(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.6% ... Training loss: 0.203 ... Validation loss: 0.345(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.7% ... Training loss: 0.211 ... Validation loss: 0.347(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.7% ... Training loss: 0.209 ... Validation loss: 0.354(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.7% ... Training loss: 0.204 ... Validation loss: 0.345(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.7% ... Training loss: 0.212 ... Validation loss: 0.362(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.7% ... Training loss: 0.203 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.8% ... Training loss: 0.202 ... Validation loss: 0.344(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.8% ... Training loss: 0.207 ... Validation loss: 0.352(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.8% ... Training loss: 0.204 ... Validation loss: 0.345(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.8% ... Training loss: 0.203 ... Validation loss: 0.349(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.8% ... Training loss: 0.202 ... Validation loss: 0.344(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.8% ... Training loss: 0.204 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.9% ... Training loss: 0.202 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.9% ... Training loss: 0.202 ... Validation loss: 0.343(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.9% ... Training loss: 0.201 ... Validation loss: 0.343(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 24.9% ... Training loss: 0.202 ... Validation loss: 0.341(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.9% ... Training loss: 0.204 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.9% ... Training loss: 0.201 ... Validation loss: 0.344(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 24.9% ... Training loss: 0.203 ... Validation loss: 0.343(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.0% ... Training loss: 0.202 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.0% ... Training loss: 0.201 ... Validation loss: 0.343(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.0% ... Training loss: 0.204 ... Validation loss: 0.347(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.0% ... Training loss: 0.203 ... Validation loss: 0.339(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.0% ... Training loss: 0.208 ... Validation loss: 0.341(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.1% ... Training loss: 0.204 ... Validation loss: 0.340(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.1% ... Training loss: 0.205 ... Validation loss: 0.346(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.1% ... Training loss: 0.205 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.1% ... Training loss: 0.203 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.1% ... Training loss: 0.201 ... Validation loss: 0.339(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.1% ... Training loss: 0.214 ... Validation loss: 0.352(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.1% ... Training loss: 0.208 ... Validation loss: 0.346(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.2% ... Training loss: 0.201 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.2% ... Training loss: 0.203 ... Validation loss: 0.344(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.2% ... Training loss: 0.204 ... Validation loss: 0.346(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.2% ... Training loss: 0.225 ... Validation loss: 0.364(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.2% ... Training loss: 0.213 ... Validation loss: 0.353(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.2% ... Training loss: 0.217 ... Validation loss: 0.368(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.3% ... Training loss: 0.199 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.3% ... Training loss: 0.199 ... Validation loss: 0.341(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.3% ... Training loss: 0.202 ... Validation loss: 0.343(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.3% ... Training loss: 0.198 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.3% ... Training loss: 0.199 ... Validation loss: 0.346(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.4% ... Training loss: 0.202 ... Validation loss: 0.347(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.4% ... Training loss: 0.197 ... Validation loss: 0.341(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.4% ... Training loss: 0.201 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.4% ... Training loss: 0.201 ... Validation loss: 0.345(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.4% ... Training loss: 0.197 ... Validation loss: 0.341(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.4% ... Training loss: 0.197 ... Validation loss: 0.340(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.4% ... Training loss: 0.196 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.5% ... Training loss: 0.196 ... Validation loss: 0.339(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.5% ... Training loss: 0.196 ... Validation loss: 0.339(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.5% ... Training loss: 0.205 ... Validation loss: 0.351(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.5% ... Training loss: 0.200 ... Validation loss: 0.340(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.5% ... Training loss: 0.204 ... Validation loss: 0.353(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.6% ... Training loss: 0.220 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.6% ... Training loss: 0.198 ... Validation loss: 0.341(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.6% ... Training loss: 0.228 ... Validation loss: 0.360(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.6% ... Training loss: 0.233 ... Validation loss: 0.367(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.6% ... Training loss: 0.211 ... Validation loss: 0.355(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.6% ... Training loss: 0.197 ... Validation loss: 0.337(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.6% ... Training loss: 0.197 ... Validation loss: 0.335(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.7% ... Training loss: 0.198 ... Validation loss: 0.337(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.7% ... Training loss: 0.197 ... Validation loss: 0.340(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.7% ... Training loss: 0.196 ... Validation loss: 0.337(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.7% ... Training loss: 0.204 ... Validation loss: 0.343(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.7% ... Training loss: 0.198 ... Validation loss: 0.333(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.8% ... Training loss: 0.222 ... Validation loss: 0.363(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.8% ... Training loss: 0.207 ... Validation loss: 0.343(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.8% ... Training loss: 0.195 ... Validation loss: 0.333(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.8% ... Training loss: 0.198 ... Validation loss: 0.336(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.8% ... Training loss: 0.195 ... Validation loss: 0.335(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.8% ... Training loss: 0.200 ... Validation loss: 0.338(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.9% ... Training loss: 0.194 ... Validation loss: 0.336(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.9% ... Training loss: 0.194 ... Validation loss: 0.336(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.9% ... Training loss: 0.194 ... Validation loss: 0.337(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.9% ... Training loss: 0.202 ... Validation loss: 0.345(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.9% ... Training loss: 0.197 ... Validation loss: 0.335(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.9% ... Training loss: 0.194 ... Validation loss: 0.334(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 25.9% ... Training loss: 0.196 ... Validation loss: 0.337(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.0% ... Training loss: 0.195 ... Validation loss: 0.333(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.0% ... Training loss: 0.195 ... Validation loss: 0.332(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.0% ... Training loss: 0.195 ... Validation loss: 0.340(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.0% ... Training loss: 0.209 ... Validation loss: 0.337(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.0% ... Training loss: 0.225 ... Validation loss: 0.374(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.1% ... Training loss: 0.213 ... Validation loss: 0.347(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.1% ... Training loss: 0.192 ... Validation loss: 0.332(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.1% ... Training loss: 0.193 ... Validation loss: 0.332(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.1% ... Training loss: 0.193 ... Validation loss: 0.334(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.1% ... Training loss: 0.193 ... Validation loss: 0.336(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.1% ... Training loss: 0.196 ... Validation loss: 0.338(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.1% ... Training loss: 0.196 ... Validation loss: 0.337(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.2% ... Training loss: 0.192 ... Validation loss: 0.331(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.2% ... Training loss: 0.192 ... Validation loss: 0.329(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.2% ... Training loss: 0.195 ... Validation loss: 0.332(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.2% ... Training loss: 0.192 ... Validation loss: 0.331(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.2% ... Training loss: 0.195 ... Validation loss: 0.333(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.2% ... Training loss: 0.199 ... Validation loss: 0.341(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.3% ... Training loss: 0.191 ... Validation loss: 0.328(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.3% ... Training loss: 0.197 ... Validation loss: 0.337(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.3% ... Training loss: 0.196 ... Validation loss: 0.329(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.3% ... Training loss: 0.201 ... Validation loss: 0.333(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.3% ... Training loss: 0.190 ... Validation loss: 0.325(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.4% ... Training loss: 0.209 ... Validation loss: 0.349(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.4% ... Training loss: 0.254 ... Validation loss: 0.370(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.4% ... Training loss: 0.213 ... Validation loss: 0.348(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.4% ... Training loss: 0.192 ... Validation loss: 0.325(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.4% ... Training loss: 0.193 ... Validation loss: 0.324(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.4% ... Training loss: 0.213 ... Validation loss: 0.358(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 26.4% ... Training loss: 0.190 ... Validation loss: 0.323(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.5% ... Training loss: 0.189 ... Validation loss: 0.324(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.5% ... Training loss: 0.190 ... Validation loss: 0.326(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.5% ... Training loss: 0.190 ... Validation loss: 0.324(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.5% ... Training loss: 0.189 ... Validation loss: 0.323(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.5% ... Training loss: 0.191 ... Validation loss: 0.326(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.6% ... Training loss: 0.189 ... Validation loss: 0.324(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.6% ... Training loss: 0.197 ... Validation loss: 0.333(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.6% ... Training loss: 0.193 ... Validation loss: 0.326(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.6% ... Training loss: 0.205 ... Validation loss: 0.343(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.6% ... Training loss: 0.197 ... Validation loss: 0.326(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.6% ... Training loss: 0.188 ... Validation loss: 0.323(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.6% ... Training loss: 0.197 ... Validation loss: 0.325(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.7% ... Training loss: 0.187 ... Validation loss: 0.323(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.7% ... Training loss: 0.188 ... Validation loss: 0.324(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.7% ... Training loss: 0.189 ... Validation loss: 0.324(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.7% ... Training loss: 0.197 ... Validation loss: 0.331(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.7% ... Training loss: 0.193 ... Validation loss: 0.327(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.8% ... Training loss: 0.206 ... Validation loss: 0.341(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.8% ... Training loss: 0.186 ... Validation loss: 0.321(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.8% ... Training loss: 0.191 ... Validation loss: 0.322(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.8% ... Training loss: 0.187 ... Validation loss: 0.319(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.8% ... Training loss: 0.187 ... Validation loss: 0.321(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.8% ... Training loss: 0.190 ... Validation loss: 0.325(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.9% ... Training loss: 0.191 ... Validation loss: 0.326(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.9% ... Training loss: 0.188 ... Validation loss: 0.324(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.9% ... Training loss: 0.186 ... Validation loss: 0.321(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.9% ... Training loss: 0.190 ... Validation loss: 0.322(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.9% ... Training loss: 0.188 ... Validation loss: 0.320(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.9% ... Training loss: 0.203 ... Validation loss: 0.346(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 26.9% ... Training loss: 0.185 ... Validation loss: 0.318(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.0% ... Training loss: 0.186 ... Validation loss: 0.319(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.0% ... Training loss: 0.187 ... Validation loss: 0.320(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.0% ... Training loss: 0.187 ... Validation loss: 0.319(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.0% ... Training loss: 0.185 ... Validation loss: 0.320(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.0% ... Training loss: 0.188 ... Validation loss: 0.325(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.1% ... Training loss: 0.189 ... Validation loss: 0.321(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.1% ... Training loss: 0.186 ... Validation loss: 0.323(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.1% ... Training loss: 0.189 ... Validation loss: 0.325(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.1% ... Training loss: 0.204 ... Validation loss: 0.330(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.1% ... Training loss: 0.194 ... Validation loss: 0.332(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.1% ... Training loss: 0.184 ... Validation loss: 0.322(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.1% ... Training loss: 0.187 ... Validation loss: 0.317(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.2% ... Training loss: 0.196 ... Validation loss: 0.320(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.2% ... Training loss: 0.193 ... Validation loss: 0.330(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.2% ... Training loss: 0.189 ... Validation loss: 0.319(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.2% ... Training loss: 0.188 ... Validation loss: 0.323(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.2% ... Training loss: 0.189 ... Validation loss: 0.318(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.2% ... Training loss: 0.183 ... Validation loss: 0.314(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.3% ... Training loss: 0.206 ... Validation loss: 0.332(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.3% ... Training loss: 0.186 ... Validation loss: 0.317(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.3% ... Training loss: 0.183 ... Validation loss: 0.314(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.3% ... Training loss: 0.201 ... Validation loss: 0.334(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.3% ... Training loss: 0.191 ... Validation loss: 0.319(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.4% ... Training loss: 0.190 ... Validation loss: 0.320(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.4% ... Training loss: 0.184 ... Validation loss: 0.315(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.4% ... Training loss: 0.183 ... Validation loss: 0.312(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.4% ... Training loss: 0.202 ... Validation loss: 0.325(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.4% ... Training loss: 0.185 ... Validation loss: 0.318(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.4% ... Training loss: 0.184 ... Validation loss: 0.314(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.4% ... Training loss: 0.184 ... Validation loss: 0.319(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.5% ... Training loss: 0.183 ... Validation loss: 0.315(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.5% ... Training loss: 0.181 ... Validation loss: 0.314(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.5% ... Training loss: 0.185 ... Validation loss: 0.316(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.5% ... Training loss: 0.183 ... Validation loss: 0.318(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.5% ... Training loss: 0.184 ... Validation loss: 0.322(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.6% ... Training loss: 0.187 ... Validation loss: 0.326(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.6% ... Training loss: 0.184 ... Validation loss: 0.322(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.6% ... Training loss: 0.185 ... Validation loss: 0.321(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.6% ... Training loss: 0.186 ... Validation loss: 0.322(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.6% ... Training loss: 0.179 ... Validation loss: 0.313(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.6% ... Training loss: 0.185 ... Validation loss: 0.323(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.6% ... Training loss: 0.181 ... Validation loss: 0.312(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.7% ... Training loss: 0.180 ... Validation loss: 0.315(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.7% ... Training loss: 0.185 ... Validation loss: 0.314(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.7% ... Training loss: 0.184 ... Validation loss: 0.319(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.7% ... Training loss: 0.181 ... Validation loss: 0.317(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.7% ... Training loss: 0.185 ... Validation loss: 0.313(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.8% ... Training loss: 0.199 ... Validation loss: 0.342(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.8% ... Training loss: 0.205 ... Validation loss: 0.320(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.8% ... Training loss: 0.188 ... Validation loss: 0.331(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.8% ... Training loss: 0.179 ... Validation loss: 0.313(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.8% ... Training loss: 0.187 ... Validation loss: 0.325(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.8% ... Training loss: 0.179 ... Validation loss: 0.316(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.9% ... Training loss: 0.208 ... Validation loss: 0.335(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.9% ... Training loss: 0.218 ... Validation loss: 0.339(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.9% ... Training loss: 0.207 ... Validation loss: 0.335(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.9% ... Training loss: 0.185 ... Validation loss: 0.317(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.9% ... Training loss: 0.183 ... Validation loss: 0.318(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.9% ... Training loss: 0.181 ... Validation loss: 0.315(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 27.9% ... Training loss: 0.181 ... Validation loss: 0.318(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.0% ... Training loss: 0.181 ... Validation loss: 0.316(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.0% ... Training loss: 0.196 ... Validation loss: 0.321(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 28.0% ... Training loss: 0.181 ... Validation loss: 0.315(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.0% ... Training loss: 0.181 ... Validation loss: 0.313(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.0% ... Training loss: 0.182 ... Validation loss: 0.314(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.1% ... Training loss: 0.181 ... Validation loss: 0.314(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.1% ... Training loss: 0.178 ... Validation loss: 0.307(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.1% ... Training loss: 0.177 ... Validation loss: 0.308(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.1% ... Training loss: 0.183 ... Validation loss: 0.322(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.1% ... Training loss: 0.178 ... Validation loss: 0.313(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.1% ... Training loss: 0.182 ... Validation loss: 0.321(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.1% ... Training loss: 0.186 ... Validation loss: 0.313(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.2% ... Training loss: 0.179 ... Validation loss: 0.307(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.2% ... Training loss: 0.176 ... Validation loss: 0.305(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.2% ... Training loss: 0.176 ... Validation loss: 0.305(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.2% ... Training loss: 0.179 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.2% ... Training loss: 0.186 ... Validation loss: 0.317(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.2% ... Training loss: 0.184 ... Validation loss: 0.309(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.3% ... Training loss: 0.177 ... Validation loss: 0.304(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.3% ... Training loss: 0.176 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.3% ... Training loss: 0.175 ... Validation loss: 0.305(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.3% ... Training loss: 0.174 ... Validation loss: 0.307(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.3% ... Training loss: 0.177 ... Validation loss: 0.309(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.4% ... Training loss: 0.174 ... Validation loss: 0.305(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.4% ... Training loss: 0.177 ... Validation loss: 0.309(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.4% ... Training loss: 0.175 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.4% ... Training loss: 0.180 ... Validation loss: 0.313(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.4% ... Training loss: 0.195 ... Validation loss: 0.313(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.4% ... Training loss: 0.176 ... Validation loss: 0.307(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.4% ... Training loss: 0.176 ... Validation loss: 0.303(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.5% ... Training loss: 0.176 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.5% ... Training loss: 0.176 ... Validation loss: 0.303(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.5% ... Training loss: 0.175 ... Validation loss: 0.304(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.5% ... Training loss: 0.183 ... Validation loss: 0.312(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.5% ... Training loss: 0.176 ... Validation loss: 0.305(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.6% ... Training loss: 0.178 ... Validation loss: 0.307(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.6% ... Training loss: 0.177 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.6% ... Training loss: 0.173 ... Validation loss: 0.303(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.6% ... Training loss: 0.173 ... Validation loss: 0.301(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.6% ... Training loss: 0.174 ... Validation loss: 0.301(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.6% ... Training loss: 0.173 ... Validation loss: 0.301(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.6% ... Training loss: 0.180 ... Validation loss: 0.304(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.7% ... Training loss: 0.178 ... Validation loss: 0.307(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.7% ... Training loss: 0.176 ... Validation loss: 0.303(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.7% ... Training loss: 0.186 ... Validation loss: 0.310(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.7% ... Training loss: 0.172 ... Validation loss: 0.300(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.7% ... Training loss: 0.174 ... Validation loss: 0.305(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.8% ... Training loss: 0.172 ... Validation loss: 0.304(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.8% ... Training loss: 0.176 ... Validation loss: 0.299(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.8% ... Training loss: 0.183 ... Validation loss: 0.317(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.8% ... Training loss: 0.172 ... Validation loss: 0.300(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.8% ... Training loss: 0.174 ... Validation loss: 0.303(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.8% ... Training loss: 0.180 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.9% ... Training loss: 0.171 ... Validation loss: 0.301(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.9% ... Training loss: 0.171 ... Validation loss: 0.300(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.9% ... Training loss: 0.174 ... Validation loss: 0.304(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.9% ... Training loss: 0.174 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.9% ... Training loss: 0.172 ... Validation loss: 0.305(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.9% ... Training loss: 0.172 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 28.9% ... Training loss: 0.185 ... Validation loss: 0.314(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.0% ... Training loss: 0.178 ... Validation loss: 0.311(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.0% ... Training loss: 0.170 ... Validation loss: 0.299(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.0% ... Training loss: 0.178 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.0% ... Training loss: 0.170 ... Validation loss: 0.298(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.0% ... Training loss: 0.169 ... Validation loss: 0.297(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.1% ... Training loss: 0.170 ... Validation loss: 0.298(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.1% ... Training loss: 0.170 ... Validation loss: 0.298(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.1% ... Training loss: 0.177 ... Validation loss: 0.298(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.1% ... Training loss: 0.177 ... Validation loss: 0.309(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.1% ... Training loss: 0.170 ... Validation loss: 0.296(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.1% ... Training loss: 0.173 ... Validation loss: 0.302(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.1% ... Training loss: 0.178 ... Validation loss: 0.305(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.2% ... Training loss: 0.169 ... Validation loss: 0.299(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.2% ... Training loss: 0.172 ... Validation loss: 0.303(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.2% ... Training loss: 0.170 ... Validation loss: 0.296(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.2% ... Training loss: 0.170 ... Validation loss: 0.296(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.2% ... Training loss: 0.169 ... Validation loss: 0.296(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.2% ... Training loss: 0.169 ... Validation loss: 0.296(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.3% ... Training loss: 0.168 ... Validation loss: 0.296(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.3% ... Training loss: 0.169 ... Validation loss: 0.297(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.3% ... Training loss: 0.167 ... Validation loss: 0.296(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.3% ... Training loss: 0.170 ... Validation loss: 0.300(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.3% ... Training loss: 0.168 ... Validation loss: 0.297(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.4% ... Training loss: 0.173 ... Validation loss: 0.300(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.4% ... Training loss: 0.170 ... Validation loss: 0.299(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.4% ... Training loss: 0.169 ... Validation loss: 0.299(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.4% ... Training loss: 0.169 ... Validation loss: 0.301(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.4% ... Training loss: 0.168 ... Validation loss: 0.301(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.4% ... Training loss: 0.174 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.4% ... Training loss: 0.170 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.5% ... Training loss: 0.169 ... Validation loss: 0.305(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.5% ... Training loss: 0.169 ... Validation loss: 0.304(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.5% ... Training loss: 0.171 ... Validation loss: 0.310(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.5% ... Training loss: 0.183 ... Validation loss: 0.311(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.5% ... Training loss: 0.168 ... Validation loss: 0.302(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 29.6% ... Training loss: 0.171 ... Validation loss: 0.307(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.6% ... Training loss: 0.165 ... Validation loss: 0.297(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.6% ... Training loss: 0.170 ... Validation loss: 0.297(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.6% ... Training loss: 0.169 ... Validation loss: 0.301(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.6% ... Training loss: 0.172 ... Validation loss: 0.298(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.6% ... Training loss: 0.193 ... Validation loss: 0.324(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.6% ... Training loss: 0.166 ... Validation loss: 0.297(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.7% ... Training loss: 0.165 ... Validation loss: 0.295(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.7% ... Training loss: 0.164 ... Validation loss: 0.293(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.7% ... Training loss: 0.165 ... Validation loss: 0.293(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.7% ... Training loss: 0.167 ... Validation loss: 0.292(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.7% ... Training loss: 0.175 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.8% ... Training loss: 0.171 ... Validation loss: 0.304(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.8% ... Training loss: 0.164 ... Validation loss: 0.294(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.8% ... Training loss: 0.173 ... Validation loss: 0.298(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.8% ... Training loss: 0.163 ... Validation loss: 0.294(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.8% ... Training loss: 0.163 ... Validation loss: 0.293(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.8% ... Training loss: 0.163 ... Validation loss: 0.292(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.9% ... Training loss: 0.167 ... Validation loss: 0.295(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.9% ... Training loss: 0.166 ... Validation loss: 0.296(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.9% ... Training loss: 0.166 ... Validation loss: 0.293(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.9% ... Training loss: 0.176 ... Validation loss: 0.309(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.9% ... Training loss: 0.170 ... Validation loss: 0.295(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.9% ... Training loss: 0.164 ... Validation loss: 0.294(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 29.9% ... Training loss: 0.166 ... Validation loss: 0.299(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.0% ... Training loss: 0.162 ... Validation loss: 0.294(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.0% ... Training loss: 0.163 ... Validation loss: 0.293(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.0% ... Training loss: 0.162 ... Validation loss: 0.293(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.0% ... Training loss: 0.171 ... Validation loss: 0.302(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.0% ... Training loss: 0.173 ... Validation loss: 0.301(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.1% ... Training loss: 0.178 ... Validation loss: 0.313(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.1% ... Training loss: 0.162 ... Validation loss: 0.294(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.1% ... Training loss: 0.169 ... Validation loss: 0.308(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.1% ... Training loss: 0.161 ... Validation loss: 0.295(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.1% ... Training loss: 0.162 ... Validation loss: 0.295(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.1% ... Training loss: 0.166 ... Validation loss: 0.297(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.1% ... Training loss: 0.163 ... Validation loss: 0.298(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.2% ... Training loss: 0.165 ... Validation loss: 0.299(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.2% ... Training loss: 0.167 ... Validation loss: 0.298(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.2% ... Training loss: 0.166 ... Validation loss: 0.298(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.2% ... Training loss: 0.161 ... Validation loss: 0.293(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.2% ... Training loss: 0.169 ... Validation loss: 0.296(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.2% ... Training loss: 0.160 ... Validation loss: 0.293(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.3% ... Training loss: 0.187 ... Validation loss: 0.306(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.3% ... Training loss: 0.168 ... Validation loss: 0.303(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.3% ... Training loss: 0.160 ... Validation loss: 0.292(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.3% ... Training loss: 0.159 ... Validation loss: 0.292(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.3% ... Training loss: 0.160 ... Validation loss: 0.292(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.4% ... Training loss: 0.159 ... Validation loss: 0.291(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.4% ... Training loss: 0.159 ... Validation loss: 0.292(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.4% ... Training loss: 0.159 ... Validation loss: 0.292(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.4% ... Training loss: 0.172 ... Validation loss: 0.300(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.4% ... Training loss: 0.164 ... Validation loss: 0.292(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.4% ... Training loss: 0.158 ... Validation loss: 0.290(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.4% ... Training loss: 0.161 ... Validation loss: 0.294(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.5% ... Training loss: 0.161 ... Validation loss: 0.290(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.5% ... Training loss: 0.161 ... Validation loss: 0.289(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.5% ... Training loss: 0.160 ... Validation loss: 0.287(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.5% ... Training loss: 0.165 ... Validation loss: 0.295(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.5% ... Training loss: 0.185 ... Validation loss: 0.299(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.6% ... Training loss: 0.166 ... Validation loss: 0.298(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.6% ... Training loss: 0.160 ... Validation loss: 0.284(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.6% ... Training loss: 0.160 ... Validation loss: 0.283(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.6% ... Training loss: 0.158 ... Validation loss: 0.283(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.6% ... Training loss: 0.157 ... Validation loss: 0.282(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.6% ... Training loss: 0.163 ... Validation loss: 0.290(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.6% ... Training loss: 0.168 ... Validation loss: 0.292(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.7% ... Training loss: 0.162 ... Validation loss: 0.290(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.7% ... Training loss: 0.157 ... Validation loss: 0.282(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.7% ... Training loss: 0.158 ... Validation loss: 0.284(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.7% ... Training loss: 0.174 ... Validation loss: 0.295(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.7% ... Training loss: 0.163 ... Validation loss: 0.286(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.8% ... Training loss: 0.185 ... Validation loss: 0.307(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.8% ... Training loss: 0.161 ... Validation loss: 0.279(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.8% ... Training loss: 0.159 ... Validation loss: 0.281(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.8% ... Training loss: 0.156 ... Validation loss: 0.285(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.8% ... Training loss: 0.156 ... Validation loss: 0.283(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.8% ... Training loss: 0.156 ... Validation loss: 0.281(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.9% ... Training loss: 0.169 ... Validation loss: 0.291(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.9% ... Training loss: 0.158 ... Validation loss: 0.284(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.9% ... Training loss: 0.164 ... Validation loss: 0.293(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.9% ... Training loss: 0.177 ... Validation loss: 0.300(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.9% ... Training loss: 0.163 ... Validation loss: 0.295(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.9% ... Training loss: 0.162 ... Validation loss: 0.293(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 30.9% ... Training loss: 0.158 ... Validation loss: 0.291(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.0% ... Training loss: 0.156 ... Validation loss: 0.287(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.0% ... Training loss: 0.156 ... Validation loss: 0.283(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.0% ... Training loss: 0.164 ... Validation loss: 0.286(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.0% ... Training loss: 0.155 ... Validation loss: 0.281(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.0% ... Training loss: 0.154 ... Validation loss: 0.279(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.1% ... Training loss: 0.155 ... Validation loss: 0.279(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.1% ... Training loss: 0.155 ... Validation loss: 0.280(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.1% ... Training loss: 0.156 ... Validation loss: 0.284(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 31.1% ... Training loss: 0.156 ... Validation loss: 0.285(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.1% ... Training loss: 0.154 ... Validation loss: 0.281(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.1% ... Training loss: 0.158 ... Validation loss: 0.286(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.1% ... Training loss: 0.156 ... Validation loss: 0.288(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.2% ... Training loss: 0.159 ... Validation loss: 0.296(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.2% ... Training loss: 0.157 ... Validation loss: 0.289(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.2% ... Training loss: 0.156 ... Validation loss: 0.291(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.2% ... Training loss: 0.154 ... Validation loss: 0.275(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.2% ... Training loss: 0.160 ... Validation loss: 0.277(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.2% ... Training loss: 0.153 ... Validation loss: 0.279(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.3% ... Training loss: 0.156 ... Validation loss: 0.278(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.3% ... Training loss: 0.154 ... Validation loss: 0.278(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.3% ... Training loss: 0.154 ... Validation loss: 0.279(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.3% ... Training loss: 0.161 ... Validation loss: 0.277(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.3% ... Training loss: 0.159 ... Validation loss: 0.279(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.4% ... Training loss: 0.162 ... Validation loss: 0.283(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.4% ... Training loss: 0.152 ... Validation loss: 0.276(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.4% ... Training loss: 0.152 ... Validation loss: 0.278(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.4% ... Training loss: 0.160 ... Validation loss: 0.283(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.4% ... Training loss: 0.155 ... Validation loss: 0.280(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.4% ... Training loss: 0.151 ... Validation loss: 0.279(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.4% ... Training loss: 0.154 ... Validation loss: 0.285(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.5% ... Training loss: 0.157 ... Validation loss: 0.290(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.5% ... Training loss: 0.153 ... Validation loss: 0.275(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.5% ... Training loss: 0.157 ... Validation loss: 0.293(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.5% ... Training loss: 0.170 ... Validation loss: 0.288(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.5% ... Training loss: 0.151 ... Validation loss: 0.282(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.6% ... Training loss: 0.152 ... Validation loss: 0.277(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.6% ... Training loss: 0.152 ... Validation loss: 0.279(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.6% ... Training loss: 0.160 ... Validation loss: 0.279(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.6% ... Training loss: 0.150 ... Validation loss: 0.276(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.6% ... Training loss: 0.153 ... Validation loss: 0.279(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.6% ... Training loss: 0.164 ... Validation loss: 0.286(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.6% ... Training loss: 0.165 ... Validation loss: 0.297(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.7% ... Training loss: 0.153 ... Validation loss: 0.275(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.7% ... Training loss: 0.151 ... Validation loss: 0.279(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.7% ... Training loss: 0.149 ... Validation loss: 0.274(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.7% ... Training loss: 0.151 ... Validation loss: 0.275(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.7% ... Training loss: 0.149 ... Validation loss: 0.270(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.8% ... Training loss: 0.151 ... Validation loss: 0.275(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.8% ... Training loss: 0.161 ... Validation loss: 0.286(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.8% ... Training loss: 0.150 ... Validation loss: 0.269(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.8% ... Training loss: 0.153 ... Validation loss: 0.276(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.8% ... Training loss: 0.149 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.8% ... Training loss: 0.152 ... Validation loss: 0.268(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.9% ... Training loss: 0.156 ... Validation loss: 0.272(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.9% ... Training loss: 0.150 ... Validation loss: 0.268(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.9% ... Training loss: 0.149 ... Validation loss: 0.266(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.9% ... Training loss: 0.155 ... Validation loss: 0.273(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.9% ... Training loss: 0.170 ... Validation loss: 0.282(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.9% ... Training loss: 0.149 ... Validation loss: 0.265(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 31.9% ... Training loss: 0.153 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.0% ... Training loss: 0.176 ... Validation loss: 0.290(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.0% ... Training loss: 0.174 ... Validation loss: 0.280(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.0% ... Training loss: 0.167 ... Validation loss: 0.291(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.0% ... Training loss: 0.157 ... Validation loss: 0.271(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.0% ... Training loss: 0.167 ... Validation loss: 0.289(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.0% ... Training loss: 0.147 ... Validation loss: 0.266(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.1% ... Training loss: 0.148 ... Validation loss: 0.268(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.1% ... Training loss: 0.147 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.1% ... Training loss: 0.146 ... Validation loss: 0.266(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.1% ... Training loss: 0.153 ... Validation loss: 0.274(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.1% ... Training loss: 0.151 ... Validation loss: 0.269(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.1% ... Training loss: 0.150 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.2% ... Training loss: 0.164 ... Validation loss: 0.275(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.2% ... Training loss: 0.163 ... Validation loss: 0.278(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.2% ... Training loss: 0.155 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.2% ... Training loss: 0.146 ... Validation loss: 0.263(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.2% ... Training loss: 0.146 ... Validation loss: 0.266(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.2% ... Training loss: 0.145 ... Validation loss: 0.263(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.3% ... Training loss: 0.147 ... Validation loss: 0.266(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.3% ... Training loss: 0.146 ... Validation loss: 0.268(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.3% ... Training loss: 0.146 ... Validation loss: 0.269(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.3% ... Training loss: 0.149 ... Validation loss: 0.268(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.3% ... Training loss: 0.146 ... Validation loss: 0.272(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.4% ... Training loss: 0.145 ... Validation loss: 0.270(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.4% ... Training loss: 0.157 ... Validation loss: 0.270(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.4% ... Training loss: 0.146 ... Validation loss: 0.269(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.4% ... Training loss: 0.146 ... Validation loss: 0.269(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.4% ... Training loss: 0.147 ... Validation loss: 0.270(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.4% ... Training loss: 0.146 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.5% ... Training loss: 0.152 ... Validation loss: 0.277(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.5% ... Training loss: 0.150 ... Validation loss: 0.265(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.5% ... Training loss: 0.144 ... Validation loss: 0.265(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.5% ... Training loss: 0.154 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.5% ... Training loss: 0.145 ... Validation loss: 0.268(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.5% ... Training loss: 0.155 ... Validation loss: 0.276(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.5% ... Training loss: 0.155 ... Validation loss: 0.269(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.6% ... Training loss: 0.152 ... Validation loss: 0.278(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.6% ... Training loss: 0.145 ... Validation loss: 0.268(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.6% ... Training loss: 0.147 ... Validation loss: 0.263(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.6% ... Training loss: 0.143 ... Validation loss: 0.264(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.6% ... Training loss: 0.149 ... Validation loss: 0.269(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 32.6% ... Training loss: 0.143 ... Validation loss: 0.263(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.7% ... Training loss: 0.156 ... Validation loss: 0.274(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.7% ... Training loss: 0.144 ... Validation loss: 0.264(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.7% ... Training loss: 0.147 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.7% ... Training loss: 0.146 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.7% ... Training loss: 0.143 ... Validation loss: 0.263(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.8% ... Training loss: 0.144 ... Validation loss: 0.265(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.8% ... Training loss: 0.143 ... Validation loss: 0.263(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.8% ... Training loss: 0.144 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.8% ... Training loss: 0.144 ... Validation loss: 0.264(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.8% ... Training loss: 0.150 ... Validation loss: 0.268(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.8% ... Training loss: 0.144 ... Validation loss: 0.265(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.9% ... Training loss: 0.145 ... Validation loss: 0.269(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.9% ... Training loss: 0.151 ... Validation loss: 0.270(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.9% ... Training loss: 0.170 ... Validation loss: 0.283(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.9% ... Training loss: 0.142 ... Validation loss: 0.261(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.9% ... Training loss: 0.141 ... Validation loss: 0.259(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 32.9% ... Training loss: 0.142 ... Validation loss: 0.259(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.0% ... Training loss: 0.158 ... Validation loss: 0.270(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.0% ... Training loss: 0.143 ... Validation loss: 0.260(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.0% ... Training loss: 0.142 ... Validation loss: 0.261(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.0% ... Training loss: 0.142 ... Validation loss: 0.260(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.0% ... Training loss: 0.148 ... Validation loss: 0.268(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.0% ... Training loss: 0.141 ... Validation loss: 0.258(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.0% ... Training loss: 0.143 ... Validation loss: 0.261(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.1% ... Training loss: 0.143 ... Validation loss: 0.263(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.1% ... Training loss: 0.151 ... Validation loss: 0.264(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.1% ... Training loss: 0.141 ... Validation loss: 0.260(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.1% ... Training loss: 0.152 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.1% ... Training loss: 0.139 ... Validation loss: 0.256(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.1% ... Training loss: 0.139 ... Validation loss: 0.256(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.2% ... Training loss: 0.140 ... Validation loss: 0.258(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.2% ... Training loss: 0.139 ... Validation loss: 0.258(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.2% ... Training loss: 0.141 ... Validation loss: 0.261(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.2% ... Training loss: 0.138 ... Validation loss: 0.257(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.2% ... Training loss: 0.146 ... Validation loss: 0.258(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.2% ... Training loss: 0.141 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.3% ... Training loss: 0.145 ... Validation loss: 0.255(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.3% ... Training loss: 0.141 ... Validation loss: 0.266(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.3% ... Training loss: 0.140 ... Validation loss: 0.253(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.3% ... Training loss: 0.139 ... Validation loss: 0.255(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.3% ... Training loss: 0.143 ... Validation loss: 0.267(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.4% ... Training loss: 0.140 ... Validation loss: 0.257(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.4% ... Training loss: 0.138 ... Validation loss: 0.257(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.4% ... Training loss: 0.140 ... Validation loss: 0.260(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.4% ... Training loss: 0.143 ... Validation loss: 0.256(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.4% ... Training loss: 0.149 ... Validation loss: 0.266(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.4% ... Training loss: 0.138 ... Validation loss: 0.254(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.5% ... Training loss: 0.143 ... Validation loss: 0.264(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.5% ... Training loss: 0.139 ... Validation loss: 0.253(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.5% ... Training loss: 0.136 ... Validation loss: 0.252(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.5% ... Training loss: 0.141 ... Validation loss: 0.262(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.5% ... Training loss: 0.138 ... Validation loss: 0.257(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.5% ... Training loss: 0.151 ... Validation loss: 0.257(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.5% ... Training loss: 0.155 ... Validation loss: 0.274(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.6% ... Training loss: 0.148 ... Validation loss: 0.258(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.6% ... Training loss: 0.139 ... Validation loss: 0.263(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.6% ... Training loss: 0.137 ... Validation loss: 0.254(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.6% ... Training loss: 0.137 ... Validation loss: 0.254(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.6% ... Training loss: 0.136 ... Validation loss: 0.253(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.6% ... Training loss: 0.136 ... Validation loss: 0.255(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.7% ... Training loss: 0.136 ... Validation loss: 0.258(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.7% ... Training loss: 0.135 ... Validation loss: 0.254(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.7% ... Training loss: 0.137 ... Validation loss: 0.258(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.7% ... Training loss: 0.138 ... Validation loss: 0.251(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.7% ... Training loss: 0.137 ... Validation loss: 0.260(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.8% ... Training loss: 0.139 ... Validation loss: 0.251(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.8% ... Training loss: 0.136 ... Validation loss: 0.253(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.8% ... Training loss: 0.144 ... Validation loss: 0.261(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.8% ... Training loss: 0.137 ... Validation loss: 0.255(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.8% ... Training loss: 0.136 ... Validation loss: 0.250(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.8% ... Training loss: 0.138 ... Validation loss: 0.249(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.9% ... Training loss: 0.138 ... Validation loss: 0.260(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.9% ... Training loss: 0.140 ... Validation loss: 0.263(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.9% ... Training loss: 0.141 ... Validation loss: 0.247(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.9% ... Training loss: 0.139 ... Validation loss: 0.263(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.9% ... Training loss: 0.140 ... Validation loss: 0.247(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 33.9% ... Training loss: 0.138 ... Validation loss: 0.264(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.0% ... Training loss: 0.134 ... Validation loss: 0.249(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.0% ... Training loss: 0.136 ... Validation loss: 0.255(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.0% ... Training loss: 0.137 ... Validation loss: 0.257(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.0% ... Training loss: 0.140 ... Validation loss: 0.247(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.0% ... Training loss: 0.135 ... Validation loss: 0.250(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.0% ... Training loss: 0.134 ... Validation loss: 0.250(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.0% ... Training loss: 0.139 ... Validation loss: 0.258(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.1% ... Training loss: 0.134 ... Validation loss: 0.249(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.1% ... Training loss: 0.139 ... Validation loss: 0.250(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.1% ... Training loss: 0.136 ... Validation loss: 0.255(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.1% ... Training loss: 0.140 ... Validation loss: 0.255(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.1% ... Training loss: 0.149 ... Validation loss: 0.263(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.1% ... Training loss: 0.143 ... Validation loss: 0.261(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.2% ... Training loss: 0.134 ... Validation loss: 0.248(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.2% ... Training loss: 0.136 ... Validation loss: 0.249(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 34.2% ... Training loss: 0.132 ... Validation loss: 0.247(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.2% ... Training loss: 0.133 ... Validation loss: 0.247(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.2% ... Training loss: 0.147 ... Validation loss: 0.254(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.2% ... Training loss: 0.137 ... Validation loss: 0.253(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.3% ... Training loss: 0.133 ... Validation loss: 0.248(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.3% ... Training loss: 0.132 ... Validation loss: 0.248(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.3% ... Training loss: 0.133 ... Validation loss: 0.247(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.3% ... Training loss: 0.131 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.3% ... Training loss: 0.138 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.4% ... Training loss: 0.142 ... Validation loss: 0.264(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.4% ... Training loss: 0.136 ... Validation loss: 0.248(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.4% ... Training loss: 0.132 ... Validation loss: 0.250(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.4% ... Training loss: 0.131 ... Validation loss: 0.247(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.4% ... Training loss: 0.131 ... Validation loss: 0.249(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.4% ... Training loss: 0.131 ... Validation loss: 0.247(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.5% ... Training loss: 0.132 ... Validation loss: 0.255(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.5% ... Training loss: 0.131 ... Validation loss: 0.244(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.5% ... Training loss: 0.131 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.5% ... Training loss: 0.130 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.5% ... Training loss: 0.131 ... Validation loss: 0.244(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.5% ... Training loss: 0.130 ... Validation loss: 0.251(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.5% ... Training loss: 0.131 ... Validation loss: 0.245(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.6% ... Training loss: 0.132 ... Validation loss: 0.245(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.6% ... Training loss: 0.146 ... Validation loss: 0.265(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.6% ... Training loss: 0.135 ... Validation loss: 0.245(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.6% ... Training loss: 0.139 ... Validation loss: 0.270(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.6% ... Training loss: 0.133 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.6% ... Training loss: 0.134 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.7% ... Training loss: 0.148 ... Validation loss: 0.270(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.7% ... Training loss: 0.165 ... Validation loss: 0.261(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.7% ... Training loss: 0.137 ... Validation loss: 0.255(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.7% ... Training loss: 0.136 ... Validation loss: 0.244(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.7% ... Training loss: 0.131 ... Validation loss: 0.245(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.8% ... Training loss: 0.130 ... Validation loss: 0.242(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.8% ... Training loss: 0.131 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.8% ... Training loss: 0.132 ... Validation loss: 0.242(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.8% ... Training loss: 0.144 ... Validation loss: 0.255(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.8% ... Training loss: 0.130 ... Validation loss: 0.244(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.8% ... Training loss: 0.129 ... Validation loss: 0.243(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.9% ... Training loss: 0.153 ... Validation loss: 0.260(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.9% ... Training loss: 0.135 ... Validation loss: 0.250(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.9% ... Training loss: 0.139 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.9% ... Training loss: 0.135 ... Validation loss: 0.253(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.9% ... Training loss: 0.139 ... Validation loss: 0.251(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 34.9% ... Training loss: 0.154 ... Validation loss: 0.263(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.0% ... Training loss: 0.137 ... Validation loss: 0.255(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.0% ... Training loss: 0.128 ... Validation loss: 0.243(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.0% ... Training loss: 0.142 ... Validation loss: 0.256(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.0% ... Training loss: 0.131 ... Validation loss: 0.249(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.0% ... Training loss: 0.129 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.0% ... Training loss: 0.132 ... Validation loss: 0.249(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.0% ... Training loss: 0.131 ... Validation loss: 0.243(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.1% ... Training loss: 0.130 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.1% ... Training loss: 0.131 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.1% ... Training loss: 0.133 ... Validation loss: 0.249(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.1% ... Training loss: 0.128 ... Validation loss: 0.240(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.1% ... Training loss: 0.132 ... Validation loss: 0.242(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.1% ... Training loss: 0.128 ... Validation loss: 0.241(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.2% ... Training loss: 0.129 ... Validation loss: 0.240(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.2% ... Training loss: 0.132 ... Validation loss: 0.243(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.2% ... Training loss: 0.127 ... Validation loss: 0.239(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.2% ... Training loss: 0.140 ... Validation loss: 0.248(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.2% ... Training loss: 0.137 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.2% ... Training loss: 0.137 ... Validation loss: 0.247(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.3% ... Training loss: 0.133 ... Validation loss: 0.242(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.3% ... Training loss: 0.130 ... Validation loss: 0.240(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.3% ... Training loss: 0.129 ... Validation loss: 0.242(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.3% ... Training loss: 0.140 ... Validation loss: 0.250(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.3% ... Training loss: 0.129 ... Validation loss: 0.248(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.4% ... Training loss: 0.128 ... Validation loss: 0.244(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.4% ... Training loss: 0.127 ... Validation loss: 0.241(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.4% ... Training loss: 0.136 ... Validation loss: 0.249(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.4% ... Training loss: 0.130 ... Validation loss: 0.243(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.4% ... Training loss: 0.127 ... Validation loss: 0.240(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.4% ... Training loss: 0.126 ... Validation loss: 0.237(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.5% ... Training loss: 0.124 ... Validation loss: 0.238(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.5% ... Training loss: 0.125 ... Validation loss: 0.239(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.5% ... Training loss: 0.125 ... Validation loss: 0.240(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.5% ... Training loss: 0.127 ... Validation loss: 0.245(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.5% ... Training loss: 0.127 ... Validation loss: 0.244(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.5% ... Training loss: 0.126 ... Validation loss: 0.243(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.5% ... Training loss: 0.133 ... Validation loss: 0.247(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.6% ... Training loss: 0.125 ... Validation loss: 0.239(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.6% ... Training loss: 0.125 ... Validation loss: 0.236(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.6% ... Training loss: 0.126 ... Validation loss: 0.236(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.6% ... Training loss: 0.125 ... Validation loss: 0.233(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.6% ... Training loss: 0.135 ... Validation loss: 0.246(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.6% ... Training loss: 0.136 ... Validation loss: 0.243(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.7% ... Training loss: 0.124 ... Validation loss: 0.237(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.7% ... Training loss: 0.127 ... Validation loss: 0.240(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.7% ... Training loss: 0.137 ... Validation loss: 0.248(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.7% ... Training loss: 0.128 ... Validation loss: 0.242(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.7% ... Training loss: 0.130 ... Validation loss: 0.238(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 35.8% ... Training loss: 0.131 ... Validation loss: 0.240(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.8% ... Training loss: 0.125 ... Validation loss: 0.233(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.8% ... Training loss: 0.129 ... Validation loss: 0.234(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.8% ... Training loss: 0.129 ... Validation loss: 0.239(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.8% ... Training loss: 0.124 ... Validation loss: 0.234(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.8% ... Training loss: 0.122 ... Validation loss: 0.233(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.9% ... Training loss: 0.124 ... Validation loss: 0.235(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.9% ... Training loss: 0.123 ... Validation loss: 0.235(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.9% ... Training loss: 0.125 ... Validation loss: 0.238(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.9% ... Training loss: 0.130 ... Validation loss: 0.241(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.9% ... Training loss: 0.128 ... Validation loss: 0.236(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 35.9% ... Training loss: 0.125 ... Validation loss: 0.238(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.0% ... Training loss: 0.122 ... Validation loss: 0.236(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.0% ... Training loss: 0.123 ... Validation loss: 0.235(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.0% ... Training loss: 0.128 ... Validation loss: 0.237(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.0% ... Training loss: 0.122 ... Validation loss: 0.234(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.0% ... Training loss: 0.127 ... Validation loss: 0.237(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.0% ... Training loss: 0.128 ... Validation loss: 0.236(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.0% ... Training loss: 0.122 ... Validation loss: 0.234(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.1% ... Training loss: 0.123 ... Validation loss: 0.233(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.1% ... Training loss: 0.121 ... Validation loss: 0.234(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.1% ... Training loss: 0.123 ... Validation loss: 0.236(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.1% ... Training loss: 0.121 ... Validation loss: 0.234(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.1% ... Training loss: 0.130 ... Validation loss: 0.248(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.1% ... Training loss: 0.121 ... Validation loss: 0.234(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.2% ... Training loss: 0.133 ... Validation loss: 0.244(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.2% ... Training loss: 0.121 ... Validation loss: 0.234(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.2% ... Training loss: 0.125 ... Validation loss: 0.239(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.2% ... Training loss: 0.122 ... Validation loss: 0.235(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.2% ... Training loss: 0.123 ... Validation loss: 0.236(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.2% ... Training loss: 0.121 ... Validation loss: 0.235(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.3% ... Training loss: 0.120 ... Validation loss: 0.233(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.3% ... Training loss: 0.120 ... Validation loss: 0.232(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.3% ... Training loss: 0.123 ... Validation loss: 0.233(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.3% ... Training loss: 0.122 ... Validation loss: 0.231(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.3% ... Training loss: 0.121 ... Validation loss: 0.234(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.4% ... Training loss: 0.121 ... Validation loss: 0.231(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.4% ... Training loss: 0.122 ... Validation loss: 0.233(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.4% ... Training loss: 0.124 ... Validation loss: 0.234(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.4% ... Training loss: 0.122 ... Validation loss: 0.231(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.4% ... Training loss: 0.124 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.4% ... Training loss: 0.124 ... Validation loss: 0.231(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.5% ... Training loss: 0.129 ... Validation loss: 0.235(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.5% ... Training loss: 0.134 ... Validation loss: 0.242(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.5% ... Training loss: 0.126 ... Validation loss: 0.236(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.5% ... Training loss: 0.124 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.5% ... Training loss: 0.126 ... Validation loss: 0.240(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.5% ... Training loss: 0.119 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.5% ... Training loss: 0.121 ... Validation loss: 0.228(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.6% ... Training loss: 0.120 ... Validation loss: 0.232(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.6% ... Training loss: 0.119 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.6% ... Training loss: 0.121 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.6% ... Training loss: 0.121 ... Validation loss: 0.233(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.6% ... Training loss: 0.119 ... Validation loss: 0.232(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.6% ... Training loss: 0.120 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.7% ... Training loss: 0.123 ... Validation loss: 0.227(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.7% ... Training loss: 0.122 ... Validation loss: 0.231(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.7% ... Training loss: 0.121 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.7% ... Training loss: 0.121 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.7% ... Training loss: 0.119 ... Validation loss: 0.228(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.8% ... Training loss: 0.118 ... Validation loss: 0.224(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.8% ... Training loss: 0.118 ... Validation loss: 0.223(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.8% ... Training loss: 0.118 ... Validation loss: 0.224(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.8% ... Training loss: 0.118 ... Validation loss: 0.223(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.8% ... Training loss: 0.117 ... Validation loss: 0.223(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.8% ... Training loss: 0.134 ... Validation loss: 0.239(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.9% ... Training loss: 0.146 ... Validation loss: 0.243(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.9% ... Training loss: 0.160 ... Validation loss: 0.252(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.9% ... Training loss: 0.123 ... Validation loss: 0.227(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.9% ... Training loss: 0.118 ... Validation loss: 0.222(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.9% ... Training loss: 0.120 ... Validation loss: 0.226(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 36.9% ... Training loss: 0.117 ... Validation loss: 0.224(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.0% ... Training loss: 0.123 ... Validation loss: 0.232(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.0% ... Training loss: 0.117 ... Validation loss: 0.223(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.0% ... Training loss: 0.119 ... Validation loss: 0.225(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.0% ... Training loss: 0.121 ... Validation loss: 0.225(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.0% ... Training loss: 0.122 ... Validation loss: 0.226(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.0% ... Training loss: 0.119 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.0% ... Training loss: 0.120 ... Validation loss: 0.223(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.1% ... Training loss: 0.127 ... Validation loss: 0.234(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.1% ... Training loss: 0.123 ... Validation loss: 0.224(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.1% ... Training loss: 0.121 ... Validation loss: 0.226(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.1% ... Training loss: 0.125 ... Validation loss: 0.234(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.1% ... Training loss: 0.120 ... Validation loss: 0.227(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.1% ... Training loss: 0.118 ... Validation loss: 0.228(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.2% ... Training loss: 0.116 ... Validation loss: 0.225(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.2% ... Training loss: 0.120 ... Validation loss: 0.226(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.2% ... Training loss: 0.120 ... Validation loss: 0.226(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.2% ... Training loss: 0.116 ... Validation loss: 0.224(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.2% ... Training loss: 0.124 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.2% ... Training loss: 0.119 ... Validation loss: 0.231(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.3% ... Training loss: 0.117 ... Validation loss: 0.225(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.3% ... Training loss: 0.123 ... Validation loss: 0.233(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 37.3% ... Training loss: 0.115 ... Validation loss: 0.220(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.3% ... Training loss: 0.116 ... Validation loss: 0.224(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.3% ... Training loss: 0.115 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.4% ... Training loss: 0.117 ... Validation loss: 0.223(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.4% ... Training loss: 0.115 ... Validation loss: 0.220(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.4% ... Training loss: 0.116 ... Validation loss: 0.225(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.4% ... Training loss: 0.115 ... Validation loss: 0.223(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.4% ... Training loss: 0.114 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.4% ... Training loss: 0.115 ... Validation loss: 0.226(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.5% ... Training loss: 0.125 ... Validation loss: 0.237(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.5% ... Training loss: 0.115 ... Validation loss: 0.220(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.5% ... Training loss: 0.120 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.5% ... Training loss: 0.117 ... Validation loss: 0.220(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.5% ... Training loss: 0.117 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.5% ... Training loss: 0.125 ... Validation loss: 0.227(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.5% ... Training loss: 0.117 ... Validation loss: 0.220(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.6% ... Training loss: 0.114 ... Validation loss: 0.220(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.6% ... Training loss: 0.115 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.6% ... Training loss: 0.117 ... Validation loss: 0.225(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.6% ... Training loss: 0.120 ... Validation loss: 0.227(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.6% ... Training loss: 0.115 ... Validation loss: 0.222(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.6% ... Training loss: 0.126 ... Validation loss: 0.226(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.7% ... Training loss: 0.130 ... Validation loss: 0.241(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.7% ... Training loss: 0.118 ... Validation loss: 0.224(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.7% ... Training loss: 0.119 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.7% ... Training loss: 0.117 ... Validation loss: 0.222(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.7% ... Training loss: 0.122 ... Validation loss: 0.232(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.8% ... Training loss: 0.123 ... Validation loss: 0.227(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.8% ... Training loss: 0.131 ... Validation loss: 0.242(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.8% ... Training loss: 0.122 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.8% ... Training loss: 0.113 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.8% ... Training loss: 0.115 ... Validation loss: 0.226(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.8% ... Training loss: 0.113 ... Validation loss: 0.223(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.9% ... Training loss: 0.114 ... Validation loss: 0.220(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.9% ... Training loss: 0.118 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.9% ... Training loss: 0.120 ... Validation loss: 0.225(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.9% ... Training loss: 0.120 ... Validation loss: 0.228(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.9% ... Training loss: 0.114 ... Validation loss: 0.222(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 37.9% ... Training loss: 0.121 ... Validation loss: 0.219(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.0% ... Training loss: 0.113 ... Validation loss: 0.218(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.0% ... Training loss: 0.112 ... Validation loss: 0.216(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.0% ... Training loss: 0.112 ... Validation loss: 0.218(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.0% ... Training loss: 0.125 ... Validation loss: 0.224(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.0% ... Training loss: 0.119 ... Validation loss: 0.225(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.0% ... Training loss: 0.125 ... Validation loss: 0.228(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.0% ... Training loss: 0.111 ... Validation loss: 0.216(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.1% ... Training loss: 0.115 ... Validation loss: 0.218(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.1% ... Training loss: 0.111 ... Validation loss: 0.215(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.1% ... Training loss: 0.119 ... Validation loss: 0.220(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.1% ... Training loss: 0.120 ... Validation loss: 0.223(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.1% ... Training loss: 0.111 ... Validation loss: 0.214(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.1% ... Training loss: 0.115 ... Validation loss: 0.216(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.2% ... Training loss: 0.124 ... Validation loss: 0.228(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.2% ... Training loss: 0.122 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.2% ... Training loss: 0.111 ... Validation loss: 0.214(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.2% ... Training loss: 0.112 ... Validation loss: 0.218(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.2% ... Training loss: 0.111 ... Validation loss: 0.212(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.2% ... Training loss: 0.111 ... Validation loss: 0.212(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.3% ... Training loss: 0.116 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.3% ... Training loss: 0.111 ... Validation loss: 0.212(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.3% ... Training loss: 0.111 ... Validation loss: 0.218(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.3% ... Training loss: 0.113 ... Validation loss: 0.212(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.3% ... Training loss: 0.116 ... Validation loss: 0.215(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.4% ... Training loss: 0.113 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.4% ... Training loss: 0.110 ... Validation loss: 0.211(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.4% ... Training loss: 0.119 ... Validation loss: 0.214(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.4% ... Training loss: 0.127 ... Validation loss: 0.240(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.4% ... Training loss: 0.112 ... Validation loss: 0.213(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.4% ... Training loss: 0.112 ... Validation loss: 0.222(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.5% ... Training loss: 0.121 ... Validation loss: 0.214(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.5% ... Training loss: 0.110 ... Validation loss: 0.216(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.5% ... Training loss: 0.110 ... Validation loss: 0.215(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.5% ... Training loss: 0.111 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.5% ... Training loss: 0.112 ... Validation loss: 0.224(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.5% ... Training loss: 0.109 ... Validation loss: 0.215(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.5% ... Training loss: 0.127 ... Validation loss: 0.240(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.6% ... Training loss: 0.114 ... Validation loss: 0.215(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.6% ... Training loss: 0.112 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.6% ... Training loss: 0.110 ... Validation loss: 0.217(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.6% ... Training loss: 0.109 ... Validation loss: 0.217(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.6% ... Training loss: 0.118 ... Validation loss: 0.237(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.6% ... Training loss: 0.109 ... Validation loss: 0.213(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.7% ... Training loss: 0.115 ... Validation loss: 0.226(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.7% ... Training loss: 0.109 ... Validation loss: 0.216(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.7% ... Training loss: 0.121 ... Validation loss: 0.233(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.7% ... Training loss: 0.118 ... Validation loss: 0.216(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.7% ... Training loss: 0.109 ... Validation loss: 0.224(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.8% ... Training loss: 0.111 ... Validation loss: 0.228(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.8% ... Training loss: 0.110 ... Validation loss: 0.227(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.8% ... Training loss: 0.112 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.8% ... Training loss: 0.109 ... Validation loss: 0.218(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.8% ... Training loss: 0.108 ... Validation loss: 0.219(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.8% ... Training loss: 0.108 ... Validation loss: 0.219(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 38.9% ... Training loss: 0.110 ... Validation loss: 0.222(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.9% ... Training loss: 0.110 ... Validation loss: 0.211(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.9% ... Training loss: 0.108 ... Validation loss: 0.215(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.9% ... Training loss: 0.110 ... Validation loss: 0.217(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.9% ... Training loss: 0.109 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 38.9% ... Training loss: 0.108 ... Validation loss: 0.211(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.0% ... Training loss: 0.113 ... Validation loss: 0.215(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.0% ... Training loss: 0.109 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.0% ... Training loss: 0.115 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.0% ... Training loss: 0.108 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.0% ... Training loss: 0.111 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.0% ... Training loss: 0.118 ... Validation loss: 0.223(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.0% ... Training loss: 0.113 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.1% ... Training loss: 0.108 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.1% ... Training loss: 0.118 ... Validation loss: 0.220(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.1% ... Training loss: 0.113 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.1% ... Training loss: 0.109 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.1% ... Training loss: 0.107 ... Validation loss: 0.211(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.1% ... Training loss: 0.108 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.2% ... Training loss: 0.108 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.2% ... Training loss: 0.116 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.2% ... Training loss: 0.108 ... Validation loss: 0.211(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.2% ... Training loss: 0.118 ... Validation loss: 0.219(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.2% ... Training loss: 0.108 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.2% ... Training loss: 0.111 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.3% ... Training loss: 0.112 ... Validation loss: 0.218(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.3% ... Training loss: 0.106 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.3% ... Training loss: 0.107 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.3% ... Training loss: 0.111 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.3% ... Training loss: 0.110 ... Validation loss: 0.219(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.4% ... Training loss: 0.119 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.4% ... Training loss: 0.126 ... Validation loss: 0.232(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.4% ... Training loss: 0.127 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.4% ... Training loss: 0.107 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.4% ... Training loss: 0.105 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.4% ... Training loss: 0.108 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.5% ... Training loss: 0.111 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.5% ... Training loss: 0.121 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.5% ... Training loss: 0.115 ... Validation loss: 0.214(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.5% ... Training loss: 0.120 ... Validation loss: 0.212(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.5% ... Training loss: 0.111 ... Validation loss: 0.211(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.5% ... Training loss: 0.106 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.5% ... Training loss: 0.108 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.6% ... Training loss: 0.105 ... Validation loss: 0.205(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.6% ... Training loss: 0.104 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.6% ... Training loss: 0.110 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.6% ... Training loss: 0.111 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.6% ... Training loss: 0.117 ... Validation loss: 0.212(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.6% ... Training loss: 0.106 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.7% ... Training loss: 0.108 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.7% ... Training loss: 0.105 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.7% ... Training loss: 0.105 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.7% ... Training loss: 0.109 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.7% ... Training loss: 0.104 ... Validation loss: 0.205(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.8% ... Training loss: 0.118 ... Validation loss: 0.218(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.8% ... Training loss: 0.110 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.8% ... Training loss: 0.111 ... Validation loss: 0.213(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.8% ... Training loss: 0.109 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.8% ... Training loss: 0.103 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.8% ... Training loss: 0.107 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.9% ... Training loss: 0.115 ... Validation loss: 0.213(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.9% ... Training loss: 0.112 ... Validation loss: 0.216(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.9% ... Training loss: 0.117 ... Validation loss: 0.213(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.9% ... Training loss: 0.110 ... Validation loss: 0.215(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.9% ... Training loss: 0.118 ... Validation loss: 0.219(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 39.9% ... Training loss: 0.105 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.0% ... Training loss: 0.107 ... Validation loss: 0.212(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.0% ... Training loss: 0.104 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.0% ... Training loss: 0.106 ... Validation loss: 0.212(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.0% ... Training loss: 0.107 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.0% ... Training loss: 0.104 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.0% ... Training loss: 0.106 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.0% ... Training loss: 0.107 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.1% ... Training loss: 0.124 ... Validation loss: 0.220(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.1% ... Training loss: 0.102 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.1% ... Training loss: 0.102 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.1% ... Training loss: 0.107 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.1% ... Training loss: 0.111 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.1% ... Training loss: 0.103 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.2% ... Training loss: 0.106 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.2% ... Training loss: 0.118 ... Validation loss: 0.219(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.2% ... Training loss: 0.105 ... Validation loss: 0.205(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.2% ... Training loss: 0.128 ... Validation loss: 0.223(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.2% ... Training loss: 0.126 ... Validation loss: 0.224(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.2% ... Training loss: 0.120 ... Validation loss: 0.213(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.3% ... Training loss: 0.104 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.3% ... Training loss: 0.102 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.3% ... Training loss: 0.114 ... Validation loss: 0.212(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.3% ... Training loss: 0.105 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.3% ... Training loss: 0.101 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.4% ... Training loss: 0.102 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.4% ... Training loss: 0.101 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.4% ... Training loss: 0.104 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 40.4% ... Training loss: 0.105 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.4% ... Training loss: 0.114 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.4% ... Training loss: 0.109 ... Validation loss: 0.213(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.5% ... Training loss: 0.122 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.5% ... Training loss: 0.110 ... Validation loss: 0.215(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.5% ... Training loss: 0.104 ... Validation loss: 0.198(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.5% ... Training loss: 0.106 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.5% ... Training loss: 0.102 ... Validation loss: 0.199(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.5% ... Training loss: 0.103 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.5% ... Training loss: 0.101 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.6% ... Training loss: 0.101 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.6% ... Training loss: 0.100 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.6% ... Training loss: 0.100 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.6% ... Training loss: 0.100 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.6% ... Training loss: 0.101 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.6% ... Training loss: 0.100 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.7% ... Training loss: 0.100 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.7% ... Training loss: 0.101 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.7% ... Training loss: 0.105 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.7% ... Training loss: 0.101 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.7% ... Training loss: 0.103 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.8% ... Training loss: 0.106 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.8% ... Training loss: 0.106 ... Validation loss: 0.211(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.8% ... Training loss: 0.102 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.8% ... Training loss: 0.103 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.8% ... Training loss: 0.101 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.8% ... Training loss: 0.104 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.9% ... Training loss: 0.100 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.9% ... Training loss: 0.100 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.9% ... Training loss: 0.101 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.9% ... Training loss: 0.099 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.9% ... Training loss: 0.102 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 40.9% ... Training loss: 0.099 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.0% ... Training loss: 0.103 ... Validation loss: 0.205(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.0% ... Training loss: 0.114 ... Validation loss: 0.225(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.0% ... Training loss: 0.103 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.0% ... Training loss: 0.102 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.0% ... Training loss: 0.100 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.0% ... Training loss: 0.099 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.0% ... Training loss: 0.099 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.1% ... Training loss: 0.101 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.1% ... Training loss: 0.098 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.1% ... Training loss: 0.099 ... Validation loss: 0.198(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.1% ... Training loss: 0.099 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.1% ... Training loss: 0.099 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.1% ... Training loss: 0.099 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.2% ... Training loss: 0.098 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.2% ... Training loss: 0.101 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.2% ... Training loss: 0.105 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.2% ... Training loss: 0.098 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.2% ... Training loss: 0.100 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.2% ... Training loss: 0.097 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.3% ... Training loss: 0.098 ... Validation loss: 0.199(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.3% ... Training loss: 0.097 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.3% ... Training loss: 0.099 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.3% ... Training loss: 0.100 ... Validation loss: 0.212(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.3% ... Training loss: 0.098 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.4% ... Training loss: 0.097 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.4% ... Training loss: 0.099 ... Validation loss: 0.198(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.4% ... Training loss: 0.116 ... Validation loss: 0.230(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.4% ... Training loss: 0.099 ... Validation loss: 0.198(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.4% ... Training loss: 0.102 ... Validation loss: 0.212(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.4% ... Training loss: 0.100 ... Validation loss: 0.199(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.5% ... Training loss: 0.098 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.5% ... Training loss: 0.099 ... Validation loss: 0.205(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.5% ... Training loss: 0.097 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.5% ... Training loss: 0.099 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.5% ... Training loss: 0.102 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.5% ... Training loss: 0.106 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.5% ... Training loss: 0.099 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.6% ... Training loss: 0.102 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.6% ... Training loss: 0.099 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.6% ... Training loss: 0.099 ... Validation loss: 0.205(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.6% ... Training loss: 0.101 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.6% ... Training loss: 0.101 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.6% ... Training loss: 0.097 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.7% ... Training loss: 0.098 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.7% ... Training loss: 0.100 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.7% ... Training loss: 0.096 ... Validation loss: 0.198(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.7% ... Training loss: 0.107 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.7% ... Training loss: 0.105 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.8% ... Training loss: 0.099 ... Validation loss: 0.198(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.8% ... Training loss: 0.097 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.8% ... Training loss: 0.097 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.8% ... Training loss: 0.097 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.8% ... Training loss: 0.097 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.8% ... Training loss: 0.098 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.9% ... Training loss: 0.101 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.9% ... Training loss: 0.096 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.9% ... Training loss: 0.107 ... Validation loss: 0.217(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.9% ... Training loss: 0.097 ... Validation loss: 0.193(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.9% ... Training loss: 0.101 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 41.9% ... Training loss: 0.097 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 42.0% ... Training loss: 0.098 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.0% ... Training loss: 0.104 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.0% ... Training loss: 0.103 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.0% ... Training loss: 0.096 ... Validation loss: 0.193(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.0% ... Training loss: 0.100 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.0% ... Training loss: 0.096 ... Validation loss: 0.199(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.0% ... Training loss: 0.100 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.1% ... Training loss: 0.097 ... Validation loss: 0.198(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.1% ... Training loss: 0.095 ... Validation loss: 0.194(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.1% ... Training loss: 0.107 ... Validation loss: 0.205(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.1% ... Training loss: 0.105 ... Validation loss: 0.199(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.1% ... Training loss: 0.110 ... Validation loss: 0.218(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.1% ... Training loss: 0.097 ... Validation loss: 0.193(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.2% ... Training loss: 0.115 ... Validation loss: 0.215(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.2% ... Training loss: 0.095 ... Validation loss: 0.191(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.2% ... Training loss: 0.096 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.2% ... Training loss: 0.096 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.2% ... Training loss: 0.097 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.2% ... Training loss: 0.098 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.3% ... Training loss: 0.097 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.3% ... Training loss: 0.095 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.3% ... Training loss: 0.095 ... Validation loss: 0.193(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.3% ... Training loss: 0.102 ... Validation loss: 0.193(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.3% ... Training loss: 0.097 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.4% ... Training loss: 0.096 ... Validation loss: 0.191(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.4% ... Training loss: 0.095 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.4% ... Training loss: 0.101 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.4% ... Training loss: 0.099 ... Validation loss: 0.193(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.4% ... Training loss: 0.096 ... Validation loss: 0.193(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.4% ... Training loss: 0.095 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.5% ... Training loss: 0.096 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.5% ... Training loss: 0.095 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.5% ... Training loss: 0.096 ... Validation loss: 0.191(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.5% ... Training loss: 0.107 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.5% ... Training loss: 0.102 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.5% ... Training loss: 0.108 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.5% ... Training loss: 0.094 ... Validation loss: 0.194(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.6% ... Training loss: 0.097 ... Validation loss: 0.194(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.6% ... Training loss: 0.097 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.6% ... Training loss: 0.094 ... Validation loss: 0.194(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.6% ... Training loss: 0.094 ... Validation loss: 0.199(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.6% ... Training loss: 0.094 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.6% ... Training loss: 0.096 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.7% ... Training loss: 0.095 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.7% ... Training loss: 0.097 ... Validation loss: 0.194(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.7% ... Training loss: 0.095 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.7% ... Training loss: 0.095 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.7% ... Training loss: 0.093 ... Validation loss: 0.198(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.8% ... Training loss: 0.095 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.8% ... Training loss: 0.094 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.8% ... Training loss: 0.098 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.8% ... Training loss: 0.097 ... Validation loss: 0.209(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.8% ... Training loss: 0.096 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.8% ... Training loss: 0.094 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.9% ... Training loss: 0.095 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.9% ... Training loss: 0.094 ... Validation loss: 0.194(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.9% ... Training loss: 0.094 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.9% ... Training loss: 0.094 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.9% ... Training loss: 0.094 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 42.9% ... Training loss: 0.094 ... Validation loss: 0.191(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.0% ... Training loss: 0.097 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.0% ... Training loss: 0.094 ... Validation loss: 0.189(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.0% ... Training loss: 0.102 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.0% ... Training loss: 0.097 ... Validation loss: 0.188(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.0% ... Training loss: 0.105 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.0% ... Training loss: 0.098 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.0% ... Training loss: 0.102 ... Validation loss: 0.214(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.1% ... Training loss: 0.095 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.1% ... Training loss: 0.096 ... Validation loss: 0.189(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.1% ... Training loss: 0.093 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.1% ... Training loss: 0.099 ... Validation loss: 0.205(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.1% ... Training loss: 0.118 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.1% ... Training loss: 0.112 ... Validation loss: 0.231(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.2% ... Training loss: 0.122 ... Validation loss: 0.199(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.2% ... Training loss: 0.097 ... Validation loss: 0.214(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.2% ... Training loss: 0.092 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.2% ... Training loss: 0.093 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.2% ... Training loss: 0.092 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.2% ... Training loss: 0.091 ... Validation loss: 0.188(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.3% ... Training loss: 0.092 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.3% ... Training loss: 0.092 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.3% ... Training loss: 0.091 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.3% ... Training loss: 0.092 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.3% ... Training loss: 0.091 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.4% ... Training loss: 0.106 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.4% ... Training loss: 0.096 ... Validation loss: 0.188(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.4% ... Training loss: 0.092 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.4% ... Training loss: 0.098 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.4% ... Training loss: 0.091 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.4% ... Training loss: 0.093 ... Validation loss: 0.194(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.5% ... Training loss: 0.093 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.5% ... Training loss: 0.092 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.5% ... Training loss: 0.093 ... Validation loss: 0.194(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 43.5% ... Training loss: 0.092 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.5% ... Training loss: 0.092 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.5% ... Training loss: 0.090 ... Validation loss: 0.189(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.5% ... Training loss: 0.090 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.6% ... Training loss: 0.091 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.6% ... Training loss: 0.094 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.6% ... Training loss: 0.093 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.6% ... Training loss: 0.091 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.6% ... Training loss: 0.093 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.6% ... Training loss: 0.090 ... Validation loss: 0.189(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.7% ... Training loss: 0.090 ... Validation loss: 0.191(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.7% ... Training loss: 0.098 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.7% ... Training loss: 0.100 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.7% ... Training loss: 0.098 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.7% ... Training loss: 0.092 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.8% ... Training loss: 0.094 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.8% ... Training loss: 0.098 ... Validation loss: 0.202(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.8% ... Training loss: 0.092 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.8% ... Training loss: 0.090 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.8% ... Training loss: 0.091 ... Validation loss: 0.191(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.8% ... Training loss: 0.091 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.9% ... Training loss: 0.090 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.9% ... Training loss: 0.095 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.9% ... Training loss: 0.092 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.9% ... Training loss: 0.090 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.9% ... Training loss: 0.096 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 43.9% ... Training loss: 0.099 ... Validation loss: 0.203(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.0% ... Training loss: 0.090 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.0% ... Training loss: 0.091 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.0% ... Training loss: 0.093 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.0% ... Training loss: 0.092 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.0% ... Training loss: 0.090 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.0% ... Training loss: 0.089 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.0% ... Training loss: 0.096 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.1% ... Training loss: 0.095 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.1% ... Training loss: 0.090 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.1% ... Training loss: 0.090 ... Validation loss: 0.189(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.1% ... Training loss: 0.092 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.1% ... Training loss: 0.093 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.1% ... Training loss: 0.090 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.2% ... Training loss: 0.090 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.2% ... Training loss: 0.089 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.2% ... Training loss: 0.092 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.2% ... Training loss: 0.090 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.2% ... Training loss: 0.097 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.2% ... Training loss: 0.092 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.3% ... Training loss: 0.096 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.3% ... Training loss: 0.094 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.3% ... Training loss: 0.091 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.3% ... Training loss: 0.089 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.3% ... Training loss: 0.088 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.4% ... Training loss: 0.096 ... Validation loss: 0.188(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.4% ... Training loss: 0.089 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.4% ... Training loss: 0.092 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.4% ... Training loss: 0.092 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.4% ... Training loss: 0.089 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.4% ... Training loss: 0.091 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.5% ... Training loss: 0.101 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.5% ... Training loss: 0.095 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.5% ... Training loss: 0.098 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.5% ... Training loss: 0.088 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.5% ... Training loss: 0.090 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.5% ... Training loss: 0.094 ... Validation loss: 0.188(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.5% ... Training loss: 0.092 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.6% ... Training loss: 0.096 ... Validation loss: 0.189(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.6% ... Training loss: 0.092 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.6% ... Training loss: 0.091 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.6% ... Training loss: 0.098 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.6% ... Training loss: 0.090 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.6% ... Training loss: 0.091 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.7% ... Training loss: 0.088 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.7% ... Training loss: 0.088 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.7% ... Training loss: 0.088 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.7% ... Training loss: 0.092 ... Validation loss: 0.188(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.7% ... Training loss: 0.088 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.8% ... Training loss: 0.089 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.8% ... Training loss: 0.088 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.8% ... Training loss: 0.090 ... Validation loss: 0.189(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.8% ... Training loss: 0.087 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.8% ... Training loss: 0.087 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.8% ... Training loss: 0.087 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.9% ... Training loss: 0.087 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.9% ... Training loss: 0.089 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.9% ... Training loss: 0.088 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.9% ... Training loss: 0.096 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.9% ... Training loss: 0.091 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 44.9% ... Training loss: 0.087 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.0% ... Training loss: 0.091 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.0% ... Training loss: 0.087 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.0% ... Training loss: 0.087 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.0% ... Training loss: 0.089 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.0% ... Training loss: 0.095 ... Validation loss: 0.188(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.0% ... Training loss: 0.094 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 45.0% ... Training loss: 0.088 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.1% ... Training loss: 0.090 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.1% ... Training loss: 0.088 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.1% ... Training loss: 0.087 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.1% ... Training loss: 0.086 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.1% ... Training loss: 0.088 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.1% ... Training loss: 0.087 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.2% ... Training loss: 0.089 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.2% ... Training loss: 0.101 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.2% ... Training loss: 0.088 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.2% ... Training loss: 0.087 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.2% ... Training loss: 0.087 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.2% ... Training loss: 0.087 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.3% ... Training loss: 0.097 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.3% ... Training loss: 0.103 ... Validation loss: 0.191(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.3% ... Training loss: 0.090 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.3% ... Training loss: 0.088 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.3% ... Training loss: 0.088 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.4% ... Training loss: 0.105 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.4% ... Training loss: 0.104 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.4% ... Training loss: 0.100 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.4% ... Training loss: 0.105 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.4% ... Training loss: 0.102 ... Validation loss: 0.193(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.4% ... Training loss: 0.104 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.5% ... Training loss: 0.087 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.5% ... Training loss: 0.089 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.5% ... Training loss: 0.086 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.5% ... Training loss: 0.092 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.5% ... Training loss: 0.087 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.5% ... Training loss: 0.090 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.5% ... Training loss: 0.086 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.6% ... Training loss: 0.085 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.6% ... Training loss: 0.092 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.6% ... Training loss: 0.087 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.6% ... Training loss: 0.085 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.6% ... Training loss: 0.093 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.6% ... Training loss: 0.089 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.7% ... Training loss: 0.094 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.7% ... Training loss: 0.087 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.7% ... Training loss: 0.087 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.7% ... Training loss: 0.086 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.7% ... Training loss: 0.094 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.8% ... Training loss: 0.087 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.8% ... Training loss: 0.090 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.8% ... Training loss: 0.094 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.8% ... Training loss: 0.087 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.8% ... Training loss: 0.087 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.8% ... Training loss: 0.086 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.9% ... Training loss: 0.086 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.9% ... Training loss: 0.089 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.9% ... Training loss: 0.086 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.9% ... Training loss: 0.089 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.9% ... Training loss: 0.085 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 45.9% ... Training loss: 0.086 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.0% ... Training loss: 0.091 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.0% ... Training loss: 0.084 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.0% ... Training loss: 0.087 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.0% ... Training loss: 0.085 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.0% ... Training loss: 0.084 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.0% ... Training loss: 0.086 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.0% ... Training loss: 0.096 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.1% ... Training loss: 0.099 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.1% ... Training loss: 0.098 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.1% ... Training loss: 0.091 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.1% ... Training loss: 0.087 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.1% ... Training loss: 0.085 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.1% ... Training loss: 0.089 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.2% ... Training loss: 0.088 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.2% ... Training loss: 0.085 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.2% ... Training loss: 0.086 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.2% ... Training loss: 0.084 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.2% ... Training loss: 0.084 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.2% ... Training loss: 0.085 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.3% ... Training loss: 0.089 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.3% ... Training loss: 0.086 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.3% ... Training loss: 0.084 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.3% ... Training loss: 0.084 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.3% ... Training loss: 0.084 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.4% ... Training loss: 0.088 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.4% ... Training loss: 0.089 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.4% ... Training loss: 0.084 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.4% ... Training loss: 0.086 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.4% ... Training loss: 0.084 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.4% ... Training loss: 0.084 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.5% ... Training loss: 0.085 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.5% ... Training loss: 0.085 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.5% ... Training loss: 0.088 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.5% ... Training loss: 0.085 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.5% ... Training loss: 0.087 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.5% ... Training loss: 0.084 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.5% ... Training loss: 0.086 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.6% ... Training loss: 0.085 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.6% ... Training loss: 0.084 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 46.6% ... Training loss: 0.083 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.6% ... Training loss: 0.086 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.6% ... Training loss: 0.084 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.6% ... Training loss: 0.083 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.7% ... Training loss: 0.084 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.7% ... Training loss: 0.084 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.7% ... Training loss: 0.087 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.7% ... Training loss: 0.083 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.7% ... Training loss: 0.088 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.8% ... Training loss: 0.085 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.8% ... Training loss: 0.082 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.8% ... Training loss: 0.083 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.8% ... Training loss: 0.082 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.8% ... Training loss: 0.083 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.8% ... Training loss: 0.085 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.9% ... Training loss: 0.085 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.9% ... Training loss: 0.083 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.9% ... Training loss: 0.083 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.9% ... Training loss: 0.084 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.9% ... Training loss: 0.082 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 46.9% ... Training loss: 0.082 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.0% ... Training loss: 0.083 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.0% ... Training loss: 0.084 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.0% ... Training loss: 0.082 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.0% ... Training loss: 0.086 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.0% ... Training loss: 0.086 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.0% ... Training loss: 0.084 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.0% ... Training loss: 0.083 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.1% ... Training loss: 0.088 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.1% ... Training loss: 0.089 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.1% ... Training loss: 0.084 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.1% ... Training loss: 0.084 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.1% ... Training loss: 0.084 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.1% ... Training loss: 0.090 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.2% ... Training loss: 0.083 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.2% ... Training loss: 0.084 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.2% ... Training loss: 0.083 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.2% ... Training loss: 0.084 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.2% ... Training loss: 0.093 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.2% ... Training loss: 0.084 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.3% ... Training loss: 0.083 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.3% ... Training loss: 0.082 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.3% ... Training loss: 0.086 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.3% ... Training loss: 0.085 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.3% ... Training loss: 0.085 ... Validation loss: 0.188(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.4% ... Training loss: 0.082 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.4% ... Training loss: 0.081 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.4% ... Training loss: 0.082 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.4% ... Training loss: 0.082 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.4% ... Training loss: 0.082 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.4% ... Training loss: 0.084 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.5% ... Training loss: 0.083 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.5% ... Training loss: 0.095 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.5% ... Training loss: 0.090 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.5% ... Training loss: 0.096 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.5% ... Training loss: 0.094 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.5% ... Training loss: 0.085 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.5% ... Training loss: 0.082 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.6% ... Training loss: 0.082 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.6% ... Training loss: 0.084 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.6% ... Training loss: 0.088 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.6% ... Training loss: 0.084 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.6% ... Training loss: 0.083 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.6% ... Training loss: 0.093 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.7% ... Training loss: 0.086 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.7% ... Training loss: 0.088 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.7% ... Training loss: 0.095 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.7% ... Training loss: 0.086 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.7% ... Training loss: 0.087 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.8% ... Training loss: 0.083 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.8% ... Training loss: 0.082 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.8% ... Training loss: 0.081 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.8% ... Training loss: 0.083 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.8% ... Training loss: 0.083 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.8% ... Training loss: 0.095 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.9% ... Training loss: 0.086 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.9% ... Training loss: 0.081 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.9% ... Training loss: 0.081 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.9% ... Training loss: 0.083 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.9% ... Training loss: 0.081 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 47.9% ... Training loss: 0.083 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.0% ... Training loss: 0.083 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.0% ... Training loss: 0.081 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.0% ... Training loss: 0.083 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.0% ... Training loss: 0.084 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.0% ... Training loss: 0.084 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.0% ... Training loss: 0.081 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.0% ... Training loss: 0.082 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.1% ... Training loss: 0.081 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.1% ... Training loss: 0.090 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.1% ... Training loss: 0.084 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.1% ... Training loss: 0.083 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.1% ... Training loss: 0.081 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 48.1% ... Training loss: 0.081 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.2% ... Training loss: 0.082 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.2% ... Training loss: 0.081 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.2% ... Training loss: 0.082 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.2% ... Training loss: 0.090 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.2% ... Training loss: 0.111 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.2% ... Training loss: 0.092 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.3% ... Training loss: 0.082 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.3% ... Training loss: 0.084 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.3% ... Training loss: 0.080 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.3% ... Training loss: 0.081 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.3% ... Training loss: 0.080 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.4% ... Training loss: 0.082 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.4% ... Training loss: 0.081 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.4% ... Training loss: 0.083 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.4% ... Training loss: 0.082 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.4% ... Training loss: 0.080 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.4% ... Training loss: 0.080 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.5% ... Training loss: 0.082 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.5% ... Training loss: 0.083 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.5% ... Training loss: 0.094 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.5% ... Training loss: 0.107 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.5% ... Training loss: 0.107 ... Validation loss: 0.205(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.5% ... Training loss: 0.093 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.5% ... Training loss: 0.085 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.6% ... Training loss: 0.080 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.6% ... Training loss: 0.083 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.6% ... Training loss: 0.088 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.6% ... Training loss: 0.082 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.6% ... Training loss: 0.079 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.6% ... Training loss: 0.079 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.7% ... Training loss: 0.082 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.7% ... Training loss: 0.084 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.7% ... Training loss: 0.082 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.7% ... Training loss: 0.080 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.7% ... Training loss: 0.080 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.8% ... Training loss: 0.082 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.8% ... Training loss: 0.082 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.8% ... Training loss: 0.079 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.8% ... Training loss: 0.093 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.8% ... Training loss: 0.082 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.8% ... Training loss: 0.084 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.9% ... Training loss: 0.079 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.9% ... Training loss: 0.081 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.9% ... Training loss: 0.079 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.9% ... Training loss: 0.080 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.9% ... Training loss: 0.080 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 48.9% ... Training loss: 0.082 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.0% ... Training loss: 0.083 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.0% ... Training loss: 0.080 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.0% ... Training loss: 0.081 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.0% ... Training loss: 0.084 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.0% ... Training loss: 0.079 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.0% ... Training loss: 0.079 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.0% ... Training loss: 0.087 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.1% ... Training loss: 0.088 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.1% ... Training loss: 0.082 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.1% ... Training loss: 0.078 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.1% ... Training loss: 0.082 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.1% ... Training loss: 0.080 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.1% ... Training loss: 0.096 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.2% ... Training loss: 0.086 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.2% ... Training loss: 0.087 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.2% ... Training loss: 0.092 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.2% ... Training loss: 0.081 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.2% ... Training loss: 0.079 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.2% ... Training loss: 0.081 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.3% ... Training loss: 0.079 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.3% ... Training loss: 0.080 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.3% ... Training loss: 0.080 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.3% ... Training loss: 0.086 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.3% ... Training loss: 0.081 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.4% ... Training loss: 0.082 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.4% ... Training loss: 0.084 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.4% ... Training loss: 0.078 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.4% ... Training loss: 0.082 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.4% ... Training loss: 0.089 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.4% ... Training loss: 0.089 ... Validation loss: 0.189(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.5% ... Training loss: 0.099 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.5% ... Training loss: 0.081 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.5% ... Training loss: 0.080 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.5% ... Training loss: 0.078 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.5% ... Training loss: 0.078 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.5% ... Training loss: 0.080 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.5% ... Training loss: 0.084 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.6% ... Training loss: 0.083 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.6% ... Training loss: 0.078 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.6% ... Training loss: 0.078 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.6% ... Training loss: 0.077 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.6% ... Training loss: 0.079 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.6% ... Training loss: 0.082 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.7% ... Training loss: 0.078 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.7% ... Training loss: 0.085 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 49.7% ... Training loss: 0.080 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.7% ... Training loss: 0.077 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.7% ... Training loss: 0.078 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.8% ... Training loss: 0.079 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.8% ... Training loss: 0.085 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.8% ... Training loss: 0.110 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.8% ... Training loss: 0.114 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.8% ... Training loss: 0.095 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.8% ... Training loss: 0.096 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.9% ... Training loss: 0.087 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.9% ... Training loss: 0.083 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.9% ... Training loss: 0.080 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.9% ... Training loss: 0.088 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.9% ... Training loss: 0.081 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 49.9% ... Training loss: 0.080 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.0% ... Training loss: 0.088 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.0% ... Training loss: 0.078 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.0% ... Training loss: 0.082 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.0% ... Training loss: 0.089 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.0% ... Training loss: 0.088 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.0% ... Training loss: 0.081 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.0% ... Training loss: 0.082 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.1% ... Training loss: 0.082 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.1% ... Training loss: 0.082 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.1% ... Training loss: 0.089 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.1% ... Training loss: 0.091 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.1% ... Training loss: 0.089 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.1% ... Training loss: 0.084 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.2% ... Training loss: 0.082 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.2% ... Training loss: 0.092 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.2% ... Training loss: 0.081 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.2% ... Training loss: 0.080 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.2% ... Training loss: 0.081 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.2% ... Training loss: 0.078 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.3% ... Training loss: 0.076 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.3% ... Training loss: 0.077 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.3% ... Training loss: 0.077 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.3% ... Training loss: 0.078 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.3% ... Training loss: 0.083 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.4% ... Training loss: 0.077 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.4% ... Training loss: 0.078 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.4% ... Training loss: 0.084 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.4% ... Training loss: 0.082 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.4% ... Training loss: 0.077 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.4% ... Training loss: 0.077 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.5% ... Training loss: 0.080 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.5% ... Training loss: 0.076 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.5% ... Training loss: 0.080 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.5% ... Training loss: 0.076 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.5% ... Training loss: 0.076 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.5% ... Training loss: 0.076 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.5% ... Training loss: 0.076 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.6% ... Training loss: 0.076 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.6% ... Training loss: 0.082 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.6% ... Training loss: 0.093 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.6% ... Training loss: 0.084 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.6% ... Training loss: 0.077 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.6% ... Training loss: 0.078 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.7% ... Training loss: 0.077 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.7% ... Training loss: 0.082 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.7% ... Training loss: 0.081 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.7% ... Training loss: 0.079 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.7% ... Training loss: 0.079 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.8% ... Training loss: 0.076 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.8% ... Training loss: 0.078 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.8% ... Training loss: 0.076 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.8% ... Training loss: 0.084 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.8% ... Training loss: 0.084 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.8% ... Training loss: 0.084 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.9% ... Training loss: 0.076 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.9% ... Training loss: 0.080 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.9% ... Training loss: 0.078 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.9% ... Training loss: 0.081 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.9% ... Training loss: 0.077 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 50.9% ... Training loss: 0.077 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.0% ... Training loss: 0.081 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.0% ... Training loss: 0.080 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.0% ... Training loss: 0.084 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.0% ... Training loss: 0.083 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.0% ... Training loss: 0.094 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.0% ... Training loss: 0.088 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.0% ... Training loss: 0.079 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.1% ... Training loss: 0.082 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.1% ... Training loss: 0.086 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.1% ... Training loss: 0.079 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.1% ... Training loss: 0.077 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.1% ... Training loss: 0.076 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.1% ... Training loss: 0.077 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.2% ... Training loss: 0.075 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.2% ... Training loss: 0.076 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.2% ... Training loss: 0.075 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.2% ... Training loss: 0.075 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.2% ... Training loss: 0.076 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 51.2% ... Training loss: 0.078 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.3% ... Training loss: 0.078 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.3% ... Training loss: 0.076 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.3% ... Training loss: 0.077 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.3% ... Training loss: 0.076 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.3% ... Training loss: 0.080 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.4% ... Training loss: 0.076 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.4% ... Training loss: 0.075 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.4% ... Training loss: 0.075 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.4% ... Training loss: 0.076 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.4% ... Training loss: 0.076 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.4% ... Training loss: 0.076 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.5% ... Training loss: 0.081 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.5% ... Training loss: 0.094 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.5% ... Training loss: 0.092 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.5% ... Training loss: 0.100 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.5% ... Training loss: 0.083 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.5% ... Training loss: 0.075 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.5% ... Training loss: 0.075 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.6% ... Training loss: 0.075 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.6% ... Training loss: 0.084 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.6% ... Training loss: 0.079 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.6% ... Training loss: 0.080 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.6% ... Training loss: 0.089 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.6% ... Training loss: 0.076 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.7% ... Training loss: 0.077 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.7% ... Training loss: 0.082 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.7% ... Training loss: 0.091 ... Validation loss: 0.191(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.7% ... Training loss: 0.086 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.7% ... Training loss: 0.078 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.8% ... Training loss: 0.076 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.8% ... Training loss: 0.086 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.8% ... Training loss: 0.078 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.8% ... Training loss: 0.076 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.8% ... Training loss: 0.095 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.8% ... Training loss: 0.090 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.9% ... Training loss: 0.085 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.9% ... Training loss: 0.078 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.9% ... Training loss: 0.074 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.9% ... Training loss: 0.075 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.9% ... Training loss: 0.075 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 51.9% ... Training loss: 0.074 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.0% ... Training loss: 0.078 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.0% ... Training loss: 0.077 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.0% ... Training loss: 0.081 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.0% ... Training loss: 0.081 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.0% ... Training loss: 0.078 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.0% ... Training loss: 0.083 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.0% ... Training loss: 0.080 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.1% ... Training loss: 0.079 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.1% ... Training loss: 0.082 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.1% ... Training loss: 0.077 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.1% ... Training loss: 0.076 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.1% ... Training loss: 0.076 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.1% ... Training loss: 0.075 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.2% ... Training loss: 0.076 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.2% ... Training loss: 0.089 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.2% ... Training loss: 0.081 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.2% ... Training loss: 0.073 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.2% ... Training loss: 0.074 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.2% ... Training loss: 0.074 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.3% ... Training loss: 0.073 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.3% ... Training loss: 0.073 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.3% ... Training loss: 0.074 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.3% ... Training loss: 0.079 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.3% ... Training loss: 0.074 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.4% ... Training loss: 0.075 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.4% ... Training loss: 0.075 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.4% ... Training loss: 0.076 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.4% ... Training loss: 0.077 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.4% ... Training loss: 0.074 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.4% ... Training loss: 0.073 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.5% ... Training loss: 0.074 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.5% ... Training loss: 0.074 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.5% ... Training loss: 0.076 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.5% ... Training loss: 0.074 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.5% ... Training loss: 0.073 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.5% ... Training loss: 0.074 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.5% ... Training loss: 0.073 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.6% ... Training loss: 0.075 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.6% ... Training loss: 0.077 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.6% ... Training loss: 0.076 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.6% ... Training loss: 0.075 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.6% ... Training loss: 0.074 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.6% ... Training loss: 0.077 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.7% ... Training loss: 0.074 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.7% ... Training loss: 0.077 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.7% ... Training loss: 0.073 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.7% ... Training loss: 0.073 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.7% ... Training loss: 0.073 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.8% ... Training loss: 0.075 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.8% ... Training loss: 0.074 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.8% ... Training loss: 0.073 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 52.8% ... Training loss: 0.077 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.8% ... Training loss: 0.074 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.8% ... Training loss: 0.074 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.9% ... Training loss: 0.074 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.9% ... Training loss: 0.074 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.9% ... Training loss: 0.076 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.9% ... Training loss: 0.075 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.9% ... Training loss: 0.073 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 52.9% ... Training loss: 0.077 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.0% ... Training loss: 0.096 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.0% ... Training loss: 0.083 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.0% ... Training loss: 0.074 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.0% ... Training loss: 0.076 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.0% ... Training loss: 0.076 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.0% ... Training loss: 0.076 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.0% ... Training loss: 0.074 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.1% ... Training loss: 0.076 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.1% ... Training loss: 0.081 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.1% ... Training loss: 0.079 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.1% ... Training loss: 0.082 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.1% ... Training loss: 0.080 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.1% ... Training loss: 0.074 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.2% ... Training loss: 0.075 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.2% ... Training loss: 0.073 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.2% ... Training loss: 0.073 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.2% ... Training loss: 0.073 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.2% ... Training loss: 0.074 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.2% ... Training loss: 0.073 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.3% ... Training loss: 0.074 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.3% ... Training loss: 0.077 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.3% ... Training loss: 0.077 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.3% ... Training loss: 0.076 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.3% ... Training loss: 0.076 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.4% ... Training loss: 0.073 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.4% ... Training loss: 0.074 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.4% ... Training loss: 0.078 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.4% ... Training loss: 0.074 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.4% ... Training loss: 0.073 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.4% ... Training loss: 0.073 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.5% ... Training loss: 0.072 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.5% ... Training loss: 0.074 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.5% ... Training loss: 0.072 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.5% ... Training loss: 0.073 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.5% ... Training loss: 0.077 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.5% ... Training loss: 0.075 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.5% ... Training loss: 0.073 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.6% ... Training loss: 0.074 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.6% ... Training loss: 0.073 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.6% ... Training loss: 0.075 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.6% ... Training loss: 0.072 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.6% ... Training loss: 0.072 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.6% ... Training loss: 0.073 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.7% ... Training loss: 0.074 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.7% ... Training loss: 0.073 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.7% ... Training loss: 0.074 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.7% ... Training loss: 0.073 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.7% ... Training loss: 0.076 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.8% ... Training loss: 0.073 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.8% ... Training loss: 0.074 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.8% ... Training loss: 0.073 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.8% ... Training loss: 0.073 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.8% ... Training loss: 0.082 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.8% ... Training loss: 0.083 ... Validation loss: 0.189(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.9% ... Training loss: 0.073 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.9% ... Training loss: 0.072 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.9% ... Training loss: 0.076 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.9% ... Training loss: 0.087 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.9% ... Training loss: 0.079 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 53.9% ... Training loss: 0.076 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.0% ... Training loss: 0.072 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.0% ... Training loss: 0.074 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.0% ... Training loss: 0.072 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.0% ... Training loss: 0.077 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.0% ... Training loss: 0.080 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.0% ... Training loss: 0.076 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.0% ... Training loss: 0.079 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.1% ... Training loss: 0.074 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.1% ... Training loss: 0.074 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.1% ... Training loss: 0.075 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.1% ... Training loss: 0.073 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.1% ... Training loss: 0.072 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.1% ... Training loss: 0.072 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.2% ... Training loss: 0.072 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.2% ... Training loss: 0.072 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.2% ... Training loss: 0.080 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.2% ... Training loss: 0.078 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.2% ... Training loss: 0.073 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.2% ... Training loss: 0.072 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.3% ... Training loss: 0.074 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.3% ... Training loss: 0.079 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.3% ... Training loss: 0.087 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.3% ... Training loss: 0.082 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.3% ... Training loss: 0.089 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 54.4% ... Training loss: 0.091 ... Validation loss: 0.194(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.4% ... Training loss: 0.077 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.4% ... Training loss: 0.072 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.4% ... Training loss: 0.073 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.4% ... Training loss: 0.073 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.4% ... Training loss: 0.072 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.5% ... Training loss: 0.072 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.5% ... Training loss: 0.073 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.5% ... Training loss: 0.075 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.5% ... Training loss: 0.087 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.5% ... Training loss: 0.078 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.5% ... Training loss: 0.076 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.5% ... Training loss: 0.073 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.6% ... Training loss: 0.072 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.6% ... Training loss: 0.073 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.6% ... Training loss: 0.075 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.6% ... Training loss: 0.074 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.6% ... Training loss: 0.072 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.6% ... Training loss: 0.094 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.7% ... Training loss: 0.089 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.7% ... Training loss: 0.072 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.7% ... Training loss: 0.073 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.7% ... Training loss: 0.073 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.7% ... Training loss: 0.074 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.8% ... Training loss: 0.071 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.8% ... Training loss: 0.071 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.8% ... Training loss: 0.071 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.8% ... Training loss: 0.071 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.8% ... Training loss: 0.071 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.8% ... Training loss: 0.074 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.9% ... Training loss: 0.075 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.9% ... Training loss: 0.076 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.9% ... Training loss: 0.071 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.9% ... Training loss: 0.084 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.9% ... Training loss: 0.081 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 54.9% ... Training loss: 0.077 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.0% ... Training loss: 0.078 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.0% ... Training loss: 0.092 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.0% ... Training loss: 0.085 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.0% ... Training loss: 0.081 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.0% ... Training loss: 0.089 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.0% ... Training loss: 0.075 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.0% ... Training loss: 0.073 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.1% ... Training loss: 0.072 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.1% ... Training loss: 0.074 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.1% ... Training loss: 0.076 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.1% ... Training loss: 0.079 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.1% ... Training loss: 0.090 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.1% ... Training loss: 0.077 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.2% ... Training loss: 0.072 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.2% ... Training loss: 0.071 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.2% ... Training loss: 0.071 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.2% ... Training loss: 0.071 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.2% ... Training loss: 0.071 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.2% ... Training loss: 0.078 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.3% ... Training loss: 0.075 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.3% ... Training loss: 0.073 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.3% ... Training loss: 0.080 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.3% ... Training loss: 0.081 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.3% ... Training loss: 0.084 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.4% ... Training loss: 0.084 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.4% ... Training loss: 0.079 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.4% ... Training loss: 0.073 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.4% ... Training loss: 0.071 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.4% ... Training loss: 0.071 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.4% ... Training loss: 0.071 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.5% ... Training loss: 0.071 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.5% ... Training loss: 0.070 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.5% ... Training loss: 0.074 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.5% ... Training loss: 0.072 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.5% ... Training loss: 0.071 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.5% ... Training loss: 0.071 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.5% ... Training loss: 0.072 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.6% ... Training loss: 0.074 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.6% ... Training loss: 0.074 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.6% ... Training loss: 0.072 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.6% ... Training loss: 0.078 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.6% ... Training loss: 0.070 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.6% ... Training loss: 0.070 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.7% ... Training loss: 0.075 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.7% ... Training loss: 0.076 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.7% ... Training loss: 0.070 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.7% ... Training loss: 0.077 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.7% ... Training loss: 0.071 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.8% ... Training loss: 0.075 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.8% ... Training loss: 0.110 ... Validation loss: 0.212(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.8% ... Training loss: 0.074 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.8% ... Training loss: 0.073 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.8% ... Training loss: 0.073 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.8% ... Training loss: 0.073 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.9% ... Training loss: 0.071 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.9% ... Training loss: 0.070 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.9% ... Training loss: 0.072 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 55.9% ... Training loss: 0.071 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.9% ... Training loss: 0.071 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 55.9% ... Training loss: 0.071 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.0% ... Training loss: 0.070 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.0% ... Training loss: 0.074 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.0% ... Training loss: 0.074 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.0% ... Training loss: 0.075 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.0% ... Training loss: 0.075 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.0% ... Training loss: 0.073 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.0% ... Training loss: 0.071 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.1% ... Training loss: 0.072 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.1% ... Training loss: 0.075 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.1% ... Training loss: 0.076 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.1% ... Training loss: 0.070 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.1% ... Training loss: 0.074 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.1% ... Training loss: 0.084 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.2% ... Training loss: 0.077 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.2% ... Training loss: 0.071 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.2% ... Training loss: 0.071 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.2% ... Training loss: 0.073 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.2% ... Training loss: 0.071 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.2% ... Training loss: 0.071 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.3% ... Training loss: 0.075 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.3% ... Training loss: 0.084 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.3% ... Training loss: 0.075 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.3% ... Training loss: 0.078 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.3% ... Training loss: 0.070 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.4% ... Training loss: 0.070 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.4% ... Training loss: 0.071 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.4% ... Training loss: 0.070 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.4% ... Training loss: 0.077 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.4% ... Training loss: 0.099 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.4% ... Training loss: 0.081 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.5% ... Training loss: 0.070 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.5% ... Training loss: 0.070 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.5% ... Training loss: 0.070 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.5% ... Training loss: 0.070 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.5% ... Training loss: 0.070 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.5% ... Training loss: 0.071 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.5% ... Training loss: 0.071 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.6% ... Training loss: 0.070 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.6% ... Training loss: 0.075 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.6% ... Training loss: 0.079 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.6% ... Training loss: 0.070 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.6% ... Training loss: 0.072 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.6% ... Training loss: 0.070 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.7% ... Training loss: 0.070 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.7% ... Training loss: 0.070 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.7% ... Training loss: 0.070 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.7% ... Training loss: 0.070 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.7% ... Training loss: 0.073 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.8% ... Training loss: 0.076 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.8% ... Training loss: 0.075 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.8% ... Training loss: 0.080 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.8% ... Training loss: 0.082 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.8% ... Training loss: 0.074 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.8% ... Training loss: 0.074 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.9% ... Training loss: 0.071 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.9% ... Training loss: 0.071 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.9% ... Training loss: 0.070 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.9% ... Training loss: 0.070 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.9% ... Training loss: 0.072 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 56.9% ... Training loss: 0.070 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.0% ... Training loss: 0.069 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.0% ... Training loss: 0.072 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.0% ... Training loss: 0.072 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.0% ... Training loss: 0.070 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.0% ... Training loss: 0.075 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.0% ... Training loss: 0.072 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.0% ... Training loss: 0.071 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.1% ... Training loss: 0.069 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.1% ... Training loss: 0.071 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.1% ... Training loss: 0.090 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.1% ... Training loss: 0.082 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.1% ... Training loss: 0.073 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.1% ... Training loss: 0.078 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.2% ... Training loss: 0.069 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.2% ... Training loss: 0.072 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.2% ... Training loss: 0.074 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.2% ... Training loss: 0.075 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.2% ... Training loss: 0.070 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.2% ... Training loss: 0.069 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.3% ... Training loss: 0.069 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.3% ... Training loss: 0.071 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.3% ... Training loss: 0.069 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.3% ... Training loss: 0.073 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.3% ... Training loss: 0.082 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.4% ... Training loss: 0.079 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.4% ... Training loss: 0.080 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.4% ... Training loss: 0.071 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.4% ... Training loss: 0.072 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.4% ... Training loss: 0.072 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.4% ... Training loss: 0.086 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 57.5% ... Training loss: 0.080 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.5% ... Training loss: 0.073 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.5% ... Training loss: 0.078 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.5% ... Training loss: 0.073 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.5% ... Training loss: 0.081 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.5% ... Training loss: 0.071 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.5% ... Training loss: 0.071 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.6% ... Training loss: 0.070 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.6% ... Training loss: 0.069 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.6% ... Training loss: 0.073 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.6% ... Training loss: 0.074 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.6% ... Training loss: 0.079 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.6% ... Training loss: 0.077 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.7% ... Training loss: 0.077 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.7% ... Training loss: 0.077 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.7% ... Training loss: 0.084 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.7% ... Training loss: 0.091 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.7% ... Training loss: 0.097 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.8% ... Training loss: 0.088 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.8% ... Training loss: 0.075 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.8% ... Training loss: 0.072 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.8% ... Training loss: 0.069 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.8% ... Training loss: 0.070 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.8% ... Training loss: 0.075 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.9% ... Training loss: 0.077 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.9% ... Training loss: 0.077 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.9% ... Training loss: 0.070 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.9% ... Training loss: 0.071 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.9% ... Training loss: 0.069 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 57.9% ... Training loss: 0.070 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.0% ... Training loss: 0.076 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.0% ... Training loss: 0.070 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.0% ... Training loss: 0.070 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.0% ... Training loss: 0.069 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.0% ... Training loss: 0.070 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.0% ... Training loss: 0.069 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.0% ... Training loss: 0.073 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.1% ... Training loss: 0.070 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.1% ... Training loss: 0.075 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.1% ... Training loss: 0.071 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.1% ... Training loss: 0.069 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.1% ... Training loss: 0.069 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.1% ... Training loss: 0.071 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.2% ... Training loss: 0.084 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.2% ... Training loss: 0.075 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.2% ... Training loss: 0.078 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.2% ... Training loss: 0.076 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.2% ... Training loss: 0.073 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.2% ... Training loss: 0.069 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.3% ... Training loss: 0.073 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.3% ... Training loss: 0.072 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.3% ... Training loss: 0.073 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.3% ... Training loss: 0.069 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.3% ... Training loss: 0.069 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.4% ... Training loss: 0.069 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.4% ... Training loss: 0.069 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.4% ... Training loss: 0.073 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.4% ... Training loss: 0.070 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.4% ... Training loss: 0.070 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.4% ... Training loss: 0.070 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.5% ... Training loss: 0.071 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.5% ... Training loss: 0.069 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.5% ... Training loss: 0.069 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.5% ... Training loss: 0.069 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.5% ... Training loss: 0.081 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.5% ... Training loss: 0.081 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.5% ... Training loss: 0.077 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.6% ... Training loss: 0.089 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.6% ... Training loss: 0.071 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.6% ... Training loss: 0.068 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.6% ... Training loss: 0.069 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.6% ... Training loss: 0.070 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.6% ... Training loss: 0.068 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.7% ... Training loss: 0.070 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.7% ... Training loss: 0.069 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.7% ... Training loss: 0.069 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.7% ... Training loss: 0.074 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.7% ... Training loss: 0.074 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.8% ... Training loss: 0.069 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.8% ... Training loss: 0.070 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.8% ... Training loss: 0.072 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.8% ... Training loss: 0.080 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.8% ... Training loss: 0.072 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.8% ... Training loss: 0.075 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.9% ... Training loss: 0.069 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.9% ... Training loss: 0.076 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.9% ... Training loss: 0.073 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.9% ... Training loss: 0.071 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.9% ... Training loss: 0.071 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 58.9% ... Training loss: 0.077 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.0% ... Training loss: 0.083 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.0% ... Training loss: 0.076 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.0% ... Training loss: 0.095 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 59.0% ... Training loss: 0.089 ... Validation loss: 0.199(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.0% ... Training loss: 0.072 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.0% ... Training loss: 0.068 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.0% ... Training loss: 0.068 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.1% ... Training loss: 0.069 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.1% ... Training loss: 0.070 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.1% ... Training loss: 0.076 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.1% ... Training loss: 0.069 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.1% ... Training loss: 0.069 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.1% ... Training loss: 0.072 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.2% ... Training loss: 0.075 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.2% ... Training loss: 0.076 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.2% ... Training loss: 0.081 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.2% ... Training loss: 0.081 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.2% ... Training loss: 0.070 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.2% ... Training loss: 0.070 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.3% ... Training loss: 0.071 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.3% ... Training loss: 0.075 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.3% ... Training loss: 0.070 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.3% ... Training loss: 0.070 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.3% ... Training loss: 0.071 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.4% ... Training loss: 0.069 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.4% ... Training loss: 0.071 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.4% ... Training loss: 0.078 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.4% ... Training loss: 0.069 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.4% ... Training loss: 0.068 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.4% ... Training loss: 0.071 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.5% ... Training loss: 0.069 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.5% ... Training loss: 0.069 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.5% ... Training loss: 0.068 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.5% ... Training loss: 0.075 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.5% ... Training loss: 0.074 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.5% ... Training loss: 0.075 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.5% ... Training loss: 0.072 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.6% ... Training loss: 0.072 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.6% ... Training loss: 0.073 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.6% ... Training loss: 0.074 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.6% ... Training loss: 0.080 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.6% ... Training loss: 0.080 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.6% ... Training loss: 0.075 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.7% ... Training loss: 0.072 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.7% ... Training loss: 0.079 ... Validation loss: 0.191(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.7% ... Training loss: 0.080 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.7% ... Training loss: 0.082 ... Validation loss: 0.194(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.7% ... Training loss: 0.073 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.8% ... Training loss: 0.078 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.8% ... Training loss: 0.068 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.8% ... Training loss: 0.068 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.8% ... Training loss: 0.068 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.8% ... Training loss: 0.070 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.8% ... Training loss: 0.068 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.9% ... Training loss: 0.075 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.9% ... Training loss: 0.068 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.9% ... Training loss: 0.075 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.9% ... Training loss: 0.069 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.9% ... Training loss: 0.069 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 59.9% ... Training loss: 0.068 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.0% ... Training loss: 0.068 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.0% ... Training loss: 0.072 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.0% ... Training loss: 0.068 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.0% ... Training loss: 0.071 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.0% ... Training loss: 0.068 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.0% ... Training loss: 0.081 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.0% ... Training loss: 0.075 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.1% ... Training loss: 0.078 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.1% ... Training loss: 0.069 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.1% ... Training loss: 0.070 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.1% ... Training loss: 0.072 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.1% ... Training loss: 0.077 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.1% ... Training loss: 0.070 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.2% ... Training loss: 0.077 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.2% ... Training loss: 0.082 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.2% ... Training loss: 0.074 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.2% ... Training loss: 0.073 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.2% ... Training loss: 0.072 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.2% ... Training loss: 0.068 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.3% ... Training loss: 0.067 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.3% ... Training loss: 0.067 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.3% ... Training loss: 0.070 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.3% ... Training loss: 0.069 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.3% ... Training loss: 0.073 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.4% ... Training loss: 0.086 ... Validation loss: 0.206(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.4% ... Training loss: 0.075 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.4% ... Training loss: 0.071 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.4% ... Training loss: 0.067 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.4% ... Training loss: 0.068 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.4% ... Training loss: 0.071 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.5% ... Training loss: 0.075 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.5% ... Training loss: 0.068 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.5% ... Training loss: 0.070 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.5% ... Training loss: 0.075 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.5% ... Training loss: 0.075 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.5% ... Training loss: 0.069 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 60.5% ... Training loss: 0.067 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.6% ... Training loss: 0.073 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.6% ... Training loss: 0.072 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.6% ... Training loss: 0.068 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.6% ... Training loss: 0.069 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.6% ... Training loss: 0.067 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.6% ... Training loss: 0.093 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.7% ... Training loss: 0.072 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.7% ... Training loss: 0.069 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.7% ... Training loss: 0.069 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.7% ... Training loss: 0.070 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.7% ... Training loss: 0.068 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.8% ... Training loss: 0.071 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.8% ... Training loss: 0.069 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.8% ... Training loss: 0.068 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.8% ... Training loss: 0.073 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.8% ... Training loss: 0.070 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.8% ... Training loss: 0.072 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.9% ... Training loss: 0.079 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.9% ... Training loss: 0.069 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.9% ... Training loss: 0.067 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.9% ... Training loss: 0.067 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.9% ... Training loss: 0.068 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 60.9% ... Training loss: 0.067 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.0% ... Training loss: 0.068 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.0% ... Training loss: 0.072 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.0% ... Training loss: 0.067 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.0% ... Training loss: 0.068 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.0% ... Training loss: 0.069 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.0% ... Training loss: 0.072 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.0% ... Training loss: 0.067 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.1% ... Training loss: 0.068 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.1% ... Training loss: 0.067 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.1% ... Training loss: 0.067 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.1% ... Training loss: 0.067 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.1% ... Training loss: 0.066 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.1% ... Training loss: 0.066 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.2% ... Training loss: 0.067 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.2% ... Training loss: 0.070 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.2% ... Training loss: 0.071 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.2% ... Training loss: 0.070 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.2% ... Training loss: 0.072 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.2% ... Training loss: 0.068 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.3% ... Training loss: 0.070 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.3% ... Training loss: 0.068 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.3% ... Training loss: 0.074 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.3% ... Training loss: 0.071 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.3% ... Training loss: 0.067 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.4% ... Training loss: 0.067 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.4% ... Training loss: 0.071 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.4% ... Training loss: 0.072 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.4% ... Training loss: 0.073 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.4% ... Training loss: 0.076 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.4% ... Training loss: 0.073 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.5% ... Training loss: 0.068 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.5% ... Training loss: 0.073 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.5% ... Training loss: 0.079 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.5% ... Training loss: 0.084 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.5% ... Training loss: 0.072 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.5% ... Training loss: 0.071 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.5% ... Training loss: 0.073 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.6% ... Training loss: 0.069 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.6% ... Training loss: 0.069 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.6% ... Training loss: 0.067 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.6% ... Training loss: 0.067 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.6% ... Training loss: 0.070 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.6% ... Training loss: 0.068 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.7% ... Training loss: 0.067 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.7% ... Training loss: 0.081 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.7% ... Training loss: 0.071 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.7% ... Training loss: 0.071 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.7% ... Training loss: 0.070 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.8% ... Training loss: 0.073 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.8% ... Training loss: 0.067 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.8% ... Training loss: 0.074 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.8% ... Training loss: 0.071 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.8% ... Training loss: 0.068 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.8% ... Training loss: 0.070 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.9% ... Training loss: 0.073 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.9% ... Training loss: 0.067 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.9% ... Training loss: 0.066 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.9% ... Training loss: 0.066 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.9% ... Training loss: 0.074 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 61.9% ... Training loss: 0.067 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.0% ... Training loss: 0.081 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.0% ... Training loss: 0.088 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.0% ... Training loss: 0.071 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.0% ... Training loss: 0.067 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.0% ... Training loss: 0.067 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.0% ... Training loss: 0.066 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.0% ... Training loss: 0.070 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.1% ... Training loss: 0.067 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.1% ... Training loss: 0.067 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 62.1% ... Training loss: 0.068 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.1% ... Training loss: 0.069 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.1% ... Training loss: 0.066 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.1% ... Training loss: 0.077 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.2% ... Training loss: 0.078 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.2% ... Training loss: 0.072 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.2% ... Training loss: 0.072 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.2% ... Training loss: 0.068 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.2% ... Training loss: 0.067 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.2% ... Training loss: 0.069 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.3% ... Training loss: 0.067 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.3% ... Training loss: 0.089 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.3% ... Training loss: 0.072 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.3% ... Training loss: 0.071 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.3% ... Training loss: 0.074 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.4% ... Training loss: 0.077 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.4% ... Training loss: 0.080 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.4% ... Training loss: 0.074 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.4% ... Training loss: 0.067 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.4% ... Training loss: 0.066 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.4% ... Training loss: 0.067 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.5% ... Training loss: 0.072 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.5% ... Training loss: 0.076 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.5% ... Training loss: 0.066 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.5% ... Training loss: 0.068 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.5% ... Training loss: 0.066 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.5% ... Training loss: 0.068 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.5% ... Training loss: 0.066 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.6% ... Training loss: 0.066 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.6% ... Training loss: 0.066 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.6% ... Training loss: 0.067 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.6% ... Training loss: 0.069 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.6% ... Training loss: 0.068 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.6% ... Training loss: 0.067 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.7% ... Training loss: 0.066 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.7% ... Training loss: 0.066 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.7% ... Training loss: 0.067 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.7% ... Training loss: 0.068 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.7% ... Training loss: 0.068 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.8% ... Training loss: 0.067 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.8% ... Training loss: 0.066 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.8% ... Training loss: 0.067 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.8% ... Training loss: 0.066 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.8% ... Training loss: 0.067 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.8% ... Training loss: 0.066 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.9% ... Training loss: 0.066 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.9% ... Training loss: 0.068 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.9% ... Training loss: 0.067 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.9% ... Training loss: 0.068 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.9% ... Training loss: 0.066 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 62.9% ... Training loss: 0.069 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.0% ... Training loss: 0.071 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.0% ... Training loss: 0.068 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.0% ... Training loss: 0.070 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.0% ... Training loss: 0.067 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.0% ... Training loss: 0.070 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.0% ... Training loss: 0.066 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.0% ... Training loss: 0.072 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.1% ... Training loss: 0.070 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.1% ... Training loss: 0.071 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.1% ... Training loss: 0.081 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.1% ... Training loss: 0.075 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.1% ... Training loss: 0.078 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.1% ... Training loss: 0.068 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.2% ... Training loss: 0.066 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.2% ... Training loss: 0.066 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.2% ... Training loss: 0.065 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.2% ... Training loss: 0.067 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.2% ... Training loss: 0.071 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.2% ... Training loss: 0.076 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.3% ... Training loss: 0.077 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.3% ... Training loss: 0.066 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.3% ... Training loss: 0.066 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.3% ... Training loss: 0.073 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.3% ... Training loss: 0.082 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.4% ... Training loss: 0.106 ... Validation loss: 0.208(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.4% ... Training loss: 0.076 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.4% ... Training loss: 0.072 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.4% ... Training loss: 0.067 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.4% ... Training loss: 0.068 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.4% ... Training loss: 0.067 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.5% ... Training loss: 0.066 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.5% ... Training loss: 0.068 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.5% ... Training loss: 0.066 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.5% ... Training loss: 0.066 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.5% ... Training loss: 0.067 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.5% ... Training loss: 0.066 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.5% ... Training loss: 0.066 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.6% ... Training loss: 0.066 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.6% ... Training loss: 0.070 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.6% ... Training loss: 0.076 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.6% ... Training loss: 0.072 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.6% ... Training loss: 0.068 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 63.6% ... Training loss: 0.067 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.7% ... Training loss: 0.066 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.7% ... Training loss: 0.066 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.7% ... Training loss: 0.067 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.7% ... Training loss: 0.067 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.7% ... Training loss: 0.066 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.8% ... Training loss: 0.071 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.8% ... Training loss: 0.068 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.8% ... Training loss: 0.080 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.8% ... Training loss: 0.073 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.8% ... Training loss: 0.106 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.8% ... Training loss: 0.105 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.9% ... Training loss: 0.102 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.9% ... Training loss: 0.091 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.9% ... Training loss: 0.072 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.9% ... Training loss: 0.070 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.9% ... Training loss: 0.072 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 63.9% ... Training loss: 0.067 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.0% ... Training loss: 0.070 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.0% ... Training loss: 0.067 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.0% ... Training loss: 0.068 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.0% ... Training loss: 0.070 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.0% ... Training loss: 0.066 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.0% ... Training loss: 0.065 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.0% ... Training loss: 0.071 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.1% ... Training loss: 0.066 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.1% ... Training loss: 0.066 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.1% ... Training loss: 0.066 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.1% ... Training loss: 0.073 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.1% ... Training loss: 0.076 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.2% ... Training loss: 0.068 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.2% ... Training loss: 0.081 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.2% ... Training loss: 0.077 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.2% ... Training loss: 0.083 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.2% ... Training loss: 0.071 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.2% ... Training loss: 0.066 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.2% ... Training loss: 0.066 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.3% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.3% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.3% ... Training loss: 0.065 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.3% ... Training loss: 0.073 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.3% ... Training loss: 0.067 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.3% ... Training loss: 0.068 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.4% ... Training loss: 0.067 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.4% ... Training loss: 0.066 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.4% ... Training loss: 0.067 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.4% ... Training loss: 0.065 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.4% ... Training loss: 0.065 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.5% ... Training loss: 0.065 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.5% ... Training loss: 0.072 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.5% ... Training loss: 0.067 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.5% ... Training loss: 0.065 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.5% ... Training loss: 0.066 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.5% ... Training loss: 0.065 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.5% ... Training loss: 0.067 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.6% ... Training loss: 0.077 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.6% ... Training loss: 0.074 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.6% ... Training loss: 0.072 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.6% ... Training loss: 0.081 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.6% ... Training loss: 0.077 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.7% ... Training loss: 0.077 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.7% ... Training loss: 0.077 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.7% ... Training loss: 0.069 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.7% ... Training loss: 0.068 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.7% ... Training loss: 0.065 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.7% ... Training loss: 0.069 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.8% ... Training loss: 0.078 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.8% ... Training loss: 0.067 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.8% ... Training loss: 0.066 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.8% ... Training loss: 0.083 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.8% ... Training loss: 0.067 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.8% ... Training loss: 0.067 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.8% ... Training loss: 0.065 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.9% ... Training loss: 0.065 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.9% ... Training loss: 0.066 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.9% ... Training loss: 0.070 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.9% ... Training loss: 0.067 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 64.9% ... Training loss: 0.065 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.0% ... Training loss: 0.066 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.0% ... Training loss: 0.065 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.0% ... Training loss: 0.065 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.0% ... Training loss: 0.066 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.0% ... Training loss: 0.070 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.0% ... Training loss: 0.065 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.0% ... Training loss: 0.070 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.1% ... Training loss: 0.080 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.1% ... Training loss: 0.066 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.1% ... Training loss: 0.069 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.1% ... Training loss: 0.074 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.1% ... Training loss: 0.066 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.2% ... Training loss: 0.073 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.2% ... Training loss: 0.076 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.2% ... Training loss: 0.072 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 65.2% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.2% ... Training loss: 0.068 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.2% ... Training loss: 0.068 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.2% ... Training loss: 0.065 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.3% ... Training loss: 0.066 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.3% ... Training loss: 0.071 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.3% ... Training loss: 0.073 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.3% ... Training loss: 0.073 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.3% ... Training loss: 0.072 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.3% ... Training loss: 0.065 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.4% ... Training loss: 0.065 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.4% ... Training loss: 0.065 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.4% ... Training loss: 0.066 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.4% ... Training loss: 0.065 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.4% ... Training loss: 0.066 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.5% ... Training loss: 0.065 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.5% ... Training loss: 0.068 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.5% ... Training loss: 0.065 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.5% ... Training loss: 0.068 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.5% ... Training loss: 0.065 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.5% ... Training loss: 0.065 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.5% ... Training loss: 0.065 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.6% ... Training loss: 0.065 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.6% ... Training loss: 0.070 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.6% ... Training loss: 0.076 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.6% ... Training loss: 0.080 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.6% ... Training loss: 0.079 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.7% ... Training loss: 0.075 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.7% ... Training loss: 0.070 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.7% ... Training loss: 0.065 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.7% ... Training loss: 0.077 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.7% ... Training loss: 0.066 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.7% ... Training loss: 0.065 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.8% ... Training loss: 0.065 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.8% ... Training loss: 0.064 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.8% ... Training loss: 0.069 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.8% ... Training loss: 0.070 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.8% ... Training loss: 0.066 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.8% ... Training loss: 0.066 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.8% ... Training loss: 0.066 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.9% ... Training loss: 0.067 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.9% ... Training loss: 0.066 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.9% ... Training loss: 0.066 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.9% ... Training loss: 0.072 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 65.9% ... Training loss: 0.067 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.0% ... Training loss: 0.067 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.0% ... Training loss: 0.064 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.0% ... Training loss: 0.072 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.0% ... Training loss: 0.072 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.0% ... Training loss: 0.066 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.0% ... Training loss: 0.069 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.0% ... Training loss: 0.079 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.1% ... Training loss: 0.083 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.1% ... Training loss: 0.084 ... Validation loss: 0.195(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.1% ... Training loss: 0.077 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.1% ... Training loss: 0.070 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.1% ... Training loss: 0.066 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.2% ... Training loss: 0.066 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.2% ... Training loss: 0.064 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.2% ... Training loss: 0.065 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.2% ... Training loss: 0.067 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.2% ... Training loss: 0.089 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.2% ... Training loss: 0.081 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.2% ... Training loss: 0.070 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.3% ... Training loss: 0.069 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.3% ... Training loss: 0.064 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.3% ... Training loss: 0.074 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.3% ... Training loss: 0.067 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.3% ... Training loss: 0.065 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.3% ... Training loss: 0.065 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.4% ... Training loss: 0.066 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.4% ... Training loss: 0.065 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.4% ... Training loss: 0.066 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.4% ... Training loss: 0.067 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.4% ... Training loss: 0.065 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.5% ... Training loss: 0.065 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.5% ... Training loss: 0.072 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.5% ... Training loss: 0.068 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.5% ... Training loss: 0.069 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.5% ... Training loss: 0.065 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.5% ... Training loss: 0.064 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.5% ... Training loss: 0.065 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.6% ... Training loss: 0.064 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.6% ... Training loss: 0.065 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.6% ... Training loss: 0.064 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.6% ... Training loss: 0.066 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.6% ... Training loss: 0.065 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.7% ... Training loss: 0.071 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.7% ... Training loss: 0.083 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.7% ... Training loss: 0.073 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.7% ... Training loss: 0.067 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.7% ... Training loss: 0.097 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.7% ... Training loss: 0.075 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 66.8% ... Training loss: 0.066 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.8% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.8% ... Training loss: 0.064 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.8% ... Training loss: 0.070 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.8% ... Training loss: 0.066 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.8% ... Training loss: 0.066 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.8% ... Training loss: 0.064 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.9% ... Training loss: 0.068 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.9% ... Training loss: 0.068 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.9% ... Training loss: 0.066 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.9% ... Training loss: 0.072 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 66.9% ... Training loss: 0.068 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.0% ... Training loss: 0.065 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.0% ... Training loss: 0.072 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.0% ... Training loss: 0.065 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.0% ... Training loss: 0.065 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.0% ... Training loss: 0.078 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.0% ... Training loss: 0.072 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.0% ... Training loss: 0.065 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.1% ... Training loss: 0.068 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.1% ... Training loss: 0.067 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.1% ... Training loss: 0.075 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.1% ... Training loss: 0.069 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.1% ... Training loss: 0.069 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.2% ... Training loss: 0.064 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.2% ... Training loss: 0.065 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.2% ... Training loss: 0.064 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.2% ... Training loss: 0.064 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.2% ... Training loss: 0.064 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.2% ... Training loss: 0.065 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.2% ... Training loss: 0.065 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.3% ... Training loss: 0.063 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.3% ... Training loss: 0.065 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.3% ... Training loss: 0.064 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.3% ... Training loss: 0.064 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.3% ... Training loss: 0.064 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.3% ... Training loss: 0.065 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.4% ... Training loss: 0.065 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.4% ... Training loss: 0.070 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.4% ... Training loss: 0.065 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.4% ... Training loss: 0.064 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.4% ... Training loss: 0.064 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.5% ... Training loss: 0.064 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.5% ... Training loss: 0.064 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.5% ... Training loss: 0.064 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.5% ... Training loss: 0.074 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.5% ... Training loss: 0.079 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.5% ... Training loss: 0.070 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.5% ... Training loss: 0.066 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.6% ... Training loss: 0.068 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.6% ... Training loss: 0.067 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.6% ... Training loss: 0.067 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.6% ... Training loss: 0.068 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.6% ... Training loss: 0.064 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.7% ... Training loss: 0.064 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.7% ... Training loss: 0.071 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.7% ... Training loss: 0.072 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.7% ... Training loss: 0.068 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.7% ... Training loss: 0.064 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.7% ... Training loss: 0.067 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.8% ... Training loss: 0.067 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.8% ... Training loss: 0.065 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.8% ... Training loss: 0.067 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.8% ... Training loss: 0.064 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.8% ... Training loss: 0.064 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.8% ... Training loss: 0.066 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.8% ... Training loss: 0.063 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.9% ... Training loss: 0.078 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.9% ... Training loss: 0.066 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.9% ... Training loss: 0.066 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.9% ... Training loss: 0.069 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 67.9% ... Training loss: 0.066 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.0% ... Training loss: 0.064 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.0% ... Training loss: 0.067 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.0% ... Training loss: 0.063 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.0% ... Training loss: 0.064 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.0% ... Training loss: 0.064 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.0% ... Training loss: 0.066 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.0% ... Training loss: 0.067 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.1% ... Training loss: 0.074 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.1% ... Training loss: 0.063 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.1% ... Training loss: 0.069 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.1% ... Training loss: 0.064 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.1% ... Training loss: 0.073 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.2% ... Training loss: 0.066 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.2% ... Training loss: 0.064 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.2% ... Training loss: 0.064 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.2% ... Training loss: 0.064 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.2% ... Training loss: 0.064 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.2% ... Training loss: 0.065 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.2% ... Training loss: 0.064 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.3% ... Training loss: 0.064 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.3% ... Training loss: 0.065 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 68.3% ... Training loss: 0.064 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.3% ... Training loss: 0.064 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.3% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.3% ... Training loss: 0.066 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.4% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.4% ... Training loss: 0.066 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.4% ... Training loss: 0.067 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.4% ... Training loss: 0.065 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.4% ... Training loss: 0.072 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.5% ... Training loss: 0.064 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.5% ... Training loss: 0.069 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.5% ... Training loss: 0.070 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.5% ... Training loss: 0.068 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.5% ... Training loss: 0.067 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.5% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.5% ... Training loss: 0.064 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.6% ... Training loss: 0.064 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.6% ... Training loss: 0.066 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.6% ... Training loss: 0.068 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.6% ... Training loss: 0.070 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.6% ... Training loss: 0.071 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.7% ... Training loss: 0.070 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.7% ... Training loss: 0.072 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.7% ... Training loss: 0.068 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.7% ... Training loss: 0.070 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.7% ... Training loss: 0.071 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.7% ... Training loss: 0.084 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.8% ... Training loss: 0.081 ... Validation loss: 0.198(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.8% ... Training loss: 0.064 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.8% ... Training loss: 0.063 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.8% ... Training loss: 0.068 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.8% ... Training loss: 0.071 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.8% ... Training loss: 0.071 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.8% ... Training loss: 0.068 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.9% ... Training loss: 0.071 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.9% ... Training loss: 0.072 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.9% ... Training loss: 0.069 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.9% ... Training loss: 0.089 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 68.9% ... Training loss: 0.069 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.0% ... Training loss: 0.073 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.0% ... Training loss: 0.063 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.0% ... Training loss: 0.070 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.0% ... Training loss: 0.076 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.0% ... Training loss: 0.066 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.0% ... Training loss: 0.069 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.0% ... Training loss: 0.063 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.1% ... Training loss: 0.067 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.1% ... Training loss: 0.066 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.1% ... Training loss: 0.067 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.1% ... Training loss: 0.068 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.1% ... Training loss: 0.065 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.2% ... Training loss: 0.067 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.2% ... Training loss: 0.064 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.2% ... Training loss: 0.071 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.2% ... Training loss: 0.069 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.2% ... Training loss: 0.066 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.2% ... Training loss: 0.065 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.2% ... Training loss: 0.063 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.3% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.3% ... Training loss: 0.065 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.3% ... Training loss: 0.064 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.3% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.3% ... Training loss: 0.069 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.3% ... Training loss: 0.088 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.4% ... Training loss: 0.086 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.4% ... Training loss: 0.069 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.4% ... Training loss: 0.064 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.4% ... Training loss: 0.064 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.4% ... Training loss: 0.064 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.5% ... Training loss: 0.064 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.5% ... Training loss: 0.065 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.5% ... Training loss: 0.067 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.5% ... Training loss: 0.064 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.5% ... Training loss: 0.063 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.5% ... Training loss: 0.066 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.5% ... Training loss: 0.071 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.6% ... Training loss: 0.065 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.6% ... Training loss: 0.064 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.6% ... Training loss: 0.063 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.6% ... Training loss: 0.066 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.6% ... Training loss: 0.083 ... Validation loss: 0.188(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.7% ... Training loss: 0.067 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.7% ... Training loss: 0.067 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.7% ... Training loss: 0.065 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.7% ... Training loss: 0.067 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.7% ... Training loss: 0.063 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.7% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.8% ... Training loss: 0.063 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.8% ... Training loss: 0.064 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.8% ... Training loss: 0.064 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.8% ... Training loss: 0.064 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.8% ... Training loss: 0.064 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.8% ... Training loss: 0.064 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 69.8% ... Training loss: 0.064 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.9% ... Training loss: 0.065 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.9% ... Training loss: 0.067 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.9% ... Training loss: 0.066 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.9% ... Training loss: 0.066 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 69.9% ... Training loss: 0.064 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.0% ... Training loss: 0.064 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.0% ... Training loss: 0.071 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.0% ... Training loss: 0.065 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.0% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.0% ... Training loss: 0.084 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.0% ... Training loss: 0.073 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.0% ... Training loss: 0.064 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.1% ... Training loss: 0.070 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.1% ... Training loss: 0.065 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.1% ... Training loss: 0.063 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.1% ... Training loss: 0.064 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.1% ... Training loss: 0.075 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.2% ... Training loss: 0.069 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.2% ... Training loss: 0.066 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.2% ... Training loss: 0.063 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.2% ... Training loss: 0.071 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.2% ... Training loss: 0.076 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.2% ... Training loss: 0.065 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.2% ... Training loss: 0.067 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.3% ... Training loss: 0.066 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.3% ... Training loss: 0.072 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.3% ... Training loss: 0.064 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.3% ... Training loss: 0.064 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.3% ... Training loss: 0.064 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.3% ... Training loss: 0.064 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.4% ... Training loss: 0.070 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.4% ... Training loss: 0.073 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.4% ... Training loss: 0.073 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.4% ... Training loss: 0.067 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.4% ... Training loss: 0.068 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.5% ... Training loss: 0.067 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.5% ... Training loss: 0.065 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.5% ... Training loss: 0.064 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.5% ... Training loss: 0.065 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.5% ... Training loss: 0.066 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.5% ... Training loss: 0.070 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.5% ... Training loss: 0.070 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.6% ... Training loss: 0.085 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.6% ... Training loss: 0.085 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.6% ... Training loss: 0.083 ... Validation loss: 0.198(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.6% ... Training loss: 0.065 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.6% ... Training loss: 0.068 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.7% ... Training loss: 0.064 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.7% ... Training loss: 0.068 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.7% ... Training loss: 0.075 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.7% ... Training loss: 0.065 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.7% ... Training loss: 0.063 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.7% ... Training loss: 0.064 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.8% ... Training loss: 0.066 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.8% ... Training loss: 0.068 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.8% ... Training loss: 0.067 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.8% ... Training loss: 0.067 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.8% ... Training loss: 0.069 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.8% ... Training loss: 0.065 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.8% ... Training loss: 0.063 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.9% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.9% ... Training loss: 0.071 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.9% ... Training loss: 0.066 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.9% ... Training loss: 0.074 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 70.9% ... Training loss: 0.066 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.0% ... Training loss: 0.064 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.0% ... Training loss: 0.073 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.0% ... Training loss: 0.063 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.0% ... Training loss: 0.065 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.0% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.0% ... Training loss: 0.071 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.0% ... Training loss: 0.067 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.1% ... Training loss: 0.063 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.1% ... Training loss: 0.066 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.1% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.1% ... Training loss: 0.064 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.1% ... Training loss: 0.064 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.2% ... Training loss: 0.068 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.2% ... Training loss: 0.065 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.2% ... Training loss: 0.068 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.2% ... Training loss: 0.070 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.2% ... Training loss: 0.066 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.2% ... Training loss: 0.064 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.2% ... Training loss: 0.068 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.3% ... Training loss: 0.063 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.3% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.3% ... Training loss: 0.063 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.3% ... Training loss: 0.067 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.3% ... Training loss: 0.063 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.3% ... Training loss: 0.062 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.4% ... Training loss: 0.063 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.4% ... Training loss: 0.062 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 71.4% ... Training loss: 0.068 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.4% ... Training loss: 0.063 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.4% ... Training loss: 0.064 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.5% ... Training loss: 0.062 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.5% ... Training loss: 0.065 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.5% ... Training loss: 0.065 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.5% ... Training loss: 0.064 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.5% ... Training loss: 0.063 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.5% ... Training loss: 0.062 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.5% ... Training loss: 0.064 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.6% ... Training loss: 0.062 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.6% ... Training loss: 0.062 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.6% ... Training loss: 0.063 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.6% ... Training loss: 0.068 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.6% ... Training loss: 0.064 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.7% ... Training loss: 0.064 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.7% ... Training loss: 0.063 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.7% ... Training loss: 0.066 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.7% ... Training loss: 0.064 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.7% ... Training loss: 0.065 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.7% ... Training loss: 0.069 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.8% ... Training loss: 0.077 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.8% ... Training loss: 0.067 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.8% ... Training loss: 0.065 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.8% ... Training loss: 0.062 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.8% ... Training loss: 0.063 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.8% ... Training loss: 0.063 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.8% ... Training loss: 0.063 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.9% ... Training loss: 0.065 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.9% ... Training loss: 0.063 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.9% ... Training loss: 0.063 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.9% ... Training loss: 0.063 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 71.9% ... Training loss: 0.063 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.0% ... Training loss: 0.065 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.0% ... Training loss: 0.064 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.0% ... Training loss: 0.063 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.0% ... Training loss: 0.062 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.0% ... Training loss: 0.062 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.0% ... Training loss: 0.063 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.0% ... Training loss: 0.071 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.1% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.1% ... Training loss: 0.063 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.1% ... Training loss: 0.063 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.1% ... Training loss: 0.063 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.1% ... Training loss: 0.063 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.2% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.2% ... Training loss: 0.063 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.2% ... Training loss: 0.062 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.2% ... Training loss: 0.062 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.2% ... Training loss: 0.067 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.2% ... Training loss: 0.063 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.2% ... Training loss: 0.062 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.3% ... Training loss: 0.063 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.3% ... Training loss: 0.064 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.3% ... Training loss: 0.065 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.3% ... Training loss: 0.065 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.3% ... Training loss: 0.065 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.3% ... Training loss: 0.062 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.4% ... Training loss: 0.071 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.4% ... Training loss: 0.071 ... Validation loss: 0.191(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.4% ... Training loss: 0.068 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.4% ... Training loss: 0.065 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.4% ... Training loss: 0.066 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.5% ... Training loss: 0.064 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.5% ... Training loss: 0.063 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.5% ... Training loss: 0.063 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.5% ... Training loss: 0.062 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.5% ... Training loss: 0.063 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.5% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.5% ... Training loss: 0.063 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.6% ... Training loss: 0.067 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.6% ... Training loss: 0.069 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.6% ... Training loss: 0.063 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.6% ... Training loss: 0.064 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.6% ... Training loss: 0.065 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.7% ... Training loss: 0.063 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.7% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.7% ... Training loss: 0.063 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.7% ... Training loss: 0.068 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.7% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.7% ... Training loss: 0.063 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.8% ... Training loss: 0.063 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.8% ... Training loss: 0.063 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.8% ... Training loss: 0.063 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.8% ... Training loss: 0.063 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.8% ... Training loss: 0.064 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.8% ... Training loss: 0.063 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.8% ... Training loss: 0.063 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.9% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.9% ... Training loss: 0.069 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.9% ... Training loss: 0.063 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.9% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 72.9% ... Training loss: 0.065 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 73.0% ... Training loss: 0.068 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.0% ... Training loss: 0.068 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.0% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.0% ... Training loss: 0.062 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.0% ... Training loss: 0.063 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.0% ... Training loss: 0.063 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.0% ... Training loss: 0.063 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.1% ... Training loss: 0.062 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.1% ... Training loss: 0.062 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.1% ... Training loss: 0.067 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.1% ... Training loss: 0.067 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.1% ... Training loss: 0.070 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.2% ... Training loss: 0.088 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.2% ... Training loss: 0.070 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.2% ... Training loss: 0.065 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.2% ... Training loss: 0.063 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.2% ... Training loss: 0.066 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.2% ... Training loss: 0.064 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.2% ... Training loss: 0.063 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.3% ... Training loss: 0.063 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.3% ... Training loss: 0.063 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.3% ... Training loss: 0.062 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.3% ... Training loss: 0.068 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.3% ... Training loss: 0.063 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.3% ... Training loss: 0.062 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.4% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.4% ... Training loss: 0.063 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.4% ... Training loss: 0.062 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.4% ... Training loss: 0.063 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.4% ... Training loss: 0.064 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.5% ... Training loss: 0.067 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.5% ... Training loss: 0.064 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.5% ... Training loss: 0.062 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.5% ... Training loss: 0.065 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.5% ... Training loss: 0.064 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.5% ... Training loss: 0.063 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.5% ... Training loss: 0.064 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.6% ... Training loss: 0.064 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.6% ... Training loss: 0.062 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.6% ... Training loss: 0.067 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.6% ... Training loss: 0.062 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.6% ... Training loss: 0.062 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.7% ... Training loss: 0.063 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.7% ... Training loss: 0.069 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.7% ... Training loss: 0.063 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.7% ... Training loss: 0.063 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.7% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.7% ... Training loss: 0.062 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.8% ... Training loss: 0.067 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.8% ... Training loss: 0.070 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.8% ... Training loss: 0.064 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.8% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.8% ... Training loss: 0.066 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.8% ... Training loss: 0.082 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.8% ... Training loss: 0.067 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.9% ... Training loss: 0.063 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.9% ... Training loss: 0.069 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.9% ... Training loss: 0.066 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.9% ... Training loss: 0.063 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 73.9% ... Training loss: 0.062 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.0% ... Training loss: 0.064 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.0% ... Training loss: 0.063 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.0% ... Training loss: 0.063 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.0% ... Training loss: 0.067 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.0% ... Training loss: 0.073 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.0% ... Training loss: 0.071 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.0% ... Training loss: 0.082 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.1% ... Training loss: 0.067 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.1% ... Training loss: 0.064 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.1% ... Training loss: 0.063 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.1% ... Training loss: 0.070 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.1% ... Training loss: 0.062 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.2% ... Training loss: 0.063 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.2% ... Training loss: 0.062 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.2% ... Training loss: 0.062 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.2% ... Training loss: 0.064 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.2% ... Training loss: 0.070 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.2% ... Training loss: 0.079 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.2% ... Training loss: 0.080 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.3% ... Training loss: 0.074 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.3% ... Training loss: 0.071 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.3% ... Training loss: 0.062 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.3% ... Training loss: 0.064 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.3% ... Training loss: 0.069 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.3% ... Training loss: 0.076 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.4% ... Training loss: 0.070 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.4% ... Training loss: 0.064 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.4% ... Training loss: 0.070 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.4% ... Training loss: 0.075 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.4% ... Training loss: 0.067 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.5% ... Training loss: 0.065 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.5% ... Training loss: 0.068 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.5% ... Training loss: 0.065 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 74.5% ... Training loss: 0.065 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.5% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.5% ... Training loss: 0.064 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.5% ... Training loss: 0.076 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.6% ... Training loss: 0.067 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.6% ... Training loss: 0.064 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.6% ... Training loss: 0.064 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.6% ... Training loss: 0.073 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.6% ... Training loss: 0.064 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.7% ... Training loss: 0.062 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.7% ... Training loss: 0.062 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.7% ... Training loss: 0.063 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.7% ... Training loss: 0.062 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.7% ... Training loss: 0.062 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.7% ... Training loss: 0.066 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.8% ... Training loss: 0.063 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.8% ... Training loss: 0.064 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.8% ... Training loss: 0.062 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.8% ... Training loss: 0.062 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.8% ... Training loss: 0.061 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.8% ... Training loss: 0.062 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.8% ... Training loss: 0.066 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.9% ... Training loss: 0.063 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.9% ... Training loss: 0.064 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.9% ... Training loss: 0.062 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.9% ... Training loss: 0.064 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 74.9% ... Training loss: 0.063 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.0% ... Training loss: 0.062 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.0% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.0% ... Training loss: 0.062 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.0% ... Training loss: 0.067 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.0% ... Training loss: 0.062 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.0% ... Training loss: 0.067 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.0% ... Training loss: 0.063 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.1% ... Training loss: 0.065 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.1% ... Training loss: 0.064 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.1% ... Training loss: 0.063 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.1% ... Training loss: 0.063 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.1% ... Training loss: 0.062 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.2% ... Training loss: 0.068 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.2% ... Training loss: 0.063 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.2% ... Training loss: 0.064 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.2% ... Training loss: 0.064 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.2% ... Training loss: 0.067 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.2% ... Training loss: 0.064 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.2% ... Training loss: 0.063 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.3% ... Training loss: 0.063 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.3% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.3% ... Training loss: 0.062 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.3% ... Training loss: 0.064 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.3% ... Training loss: 0.062 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.3% ... Training loss: 0.065 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.4% ... Training loss: 0.069 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.4% ... Training loss: 0.067 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.4% ... Training loss: 0.066 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.4% ... Training loss: 0.067 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.4% ... Training loss: 0.061 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.5% ... Training loss: 0.062 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.5% ... Training loss: 0.066 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.5% ... Training loss: 0.062 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.5% ... Training loss: 0.062 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.5% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.5% ... Training loss: 0.064 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.5% ... Training loss: 0.063 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.6% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.6% ... Training loss: 0.062 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.6% ... Training loss: 0.062 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.6% ... Training loss: 0.064 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.6% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.7% ... Training loss: 0.064 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.7% ... Training loss: 0.075 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.7% ... Training loss: 0.073 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.7% ... Training loss: 0.064 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.7% ... Training loss: 0.064 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.7% ... Training loss: 0.072 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.8% ... Training loss: 0.071 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.8% ... Training loss: 0.062 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.8% ... Training loss: 0.064 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.8% ... Training loss: 0.062 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.8% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.8% ... Training loss: 0.062 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.8% ... Training loss: 0.062 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.9% ... Training loss: 0.063 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.9% ... Training loss: 0.063 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.9% ... Training loss: 0.062 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.9% ... Training loss: 0.062 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 75.9% ... Training loss: 0.062 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.0% ... Training loss: 0.063 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.0% ... Training loss: 0.061 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.0% ... Training loss: 0.065 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.0% ... Training loss: 0.062 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.0% ... Training loss: 0.063 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.0% ... Training loss: 0.062 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 76.0% ... Training loss: 0.061 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.1% ... Training loss: 0.061 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.1% ... Training loss: 0.063 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.1% ... Training loss: 0.078 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.1% ... Training loss: 0.071 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.1% ... Training loss: 0.067 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.2% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.2% ... Training loss: 0.061 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.2% ... Training loss: 0.062 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.2% ... Training loss: 0.062 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.2% ... Training loss: 0.063 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.2% ... Training loss: 0.061 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.2% ... Training loss: 0.064 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.3% ... Training loss: 0.065 ... Validation loss: 0.189(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.3% ... Training loss: 0.063 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.3% ... Training loss: 0.065 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.3% ... Training loss: 0.068 ... Validation loss: 0.201(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.3% ... Training loss: 0.068 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.3% ... Training loss: 0.067 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.4% ... Training loss: 0.061 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.4% ... Training loss: 0.061 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.4% ... Training loss: 0.064 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.4% ... Training loss: 0.062 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.4% ... Training loss: 0.061 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.5% ... Training loss: 0.062 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.5% ... Training loss: 0.061 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.5% ... Training loss: 0.064 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.5% ... Training loss: 0.062 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.5% ... Training loss: 0.065 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.5% ... Training loss: 0.063 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.5% ... Training loss: 0.065 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.6% ... Training loss: 0.065 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.6% ... Training loss: 0.063 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.6% ... Training loss: 0.069 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.6% ... Training loss: 0.065 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.6% ... Training loss: 0.062 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.7% ... Training loss: 0.063 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.7% ... Training loss: 0.062 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.7% ... Training loss: 0.070 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.7% ... Training loss: 0.065 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.7% ... Training loss: 0.065 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.7% ... Training loss: 0.062 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.8% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.8% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.8% ... Training loss: 0.061 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.8% ... Training loss: 0.064 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.8% ... Training loss: 0.062 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.8% ... Training loss: 0.062 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.8% ... Training loss: 0.062 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.9% ... Training loss: 0.067 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.9% ... Training loss: 0.069 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.9% ... Training loss: 0.062 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.9% ... Training loss: 0.061 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 76.9% ... Training loss: 0.062 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.0% ... Training loss: 0.061 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.0% ... Training loss: 0.061 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.0% ... Training loss: 0.063 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.0% ... Training loss: 0.061 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.0% ... Training loss: 0.066 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.0% ... Training loss: 0.071 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.0% ... Training loss: 0.064 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.1% ... Training loss: 0.079 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.1% ... Training loss: 0.082 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.1% ... Training loss: 0.063 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.1% ... Training loss: 0.062 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.1% ... Training loss: 0.062 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.2% ... Training loss: 0.064 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.2% ... Training loss: 0.063 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.2% ... Training loss: 0.063 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.2% ... Training loss: 0.078 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.2% ... Training loss: 0.070 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.2% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.2% ... Training loss: 0.061 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.3% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.3% ... Training loss: 0.063 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.3% ... Training loss: 0.062 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.3% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.3% ... Training loss: 0.062 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.3% ... Training loss: 0.064 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.4% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.4% ... Training loss: 0.062 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.4% ... Training loss: 0.062 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.4% ... Training loss: 0.064 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.4% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.5% ... Training loss: 0.069 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.5% ... Training loss: 0.064 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.5% ... Training loss: 0.063 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.5% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.5% ... Training loss: 0.068 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.5% ... Training loss: 0.065 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.5% ... Training loss: 0.062 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.6% ... Training loss: 0.061 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.6% ... Training loss: 0.063 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 77.6% ... Training loss: 0.064 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.6% ... Training loss: 0.075 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.6% ... Training loss: 0.072 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.7% ... Training loss: 0.097 ... Validation loss: 0.193(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.7% ... Training loss: 0.091 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.7% ... Training loss: 0.081 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.7% ... Training loss: 0.066 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.7% ... Training loss: 0.064 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.7% ... Training loss: 0.062 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.8% ... Training loss: 0.063 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.8% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.8% ... Training loss: 0.066 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.8% ... Training loss: 0.062 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.8% ... Training loss: 0.064 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.8% ... Training loss: 0.062 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.8% ... Training loss: 0.067 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.9% ... Training loss: 0.074 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.9% ... Training loss: 0.068 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.9% ... Training loss: 0.066 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.9% ... Training loss: 0.061 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 77.9% ... Training loss: 0.062 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.0% ... Training loss: 0.061 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.0% ... Training loss: 0.062 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.0% ... Training loss: 0.061 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.0% ... Training loss: 0.061 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.0% ... Training loss: 0.061 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.0% ... Training loss: 0.063 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.0% ... Training loss: 0.069 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.1% ... Training loss: 0.081 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.1% ... Training loss: 0.097 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.1% ... Training loss: 0.078 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.1% ... Training loss: 0.066 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.1% ... Training loss: 0.066 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.2% ... Training loss: 0.088 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.2% ... Training loss: 0.073 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.2% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.2% ... Training loss: 0.062 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.2% ... Training loss: 0.065 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.2% ... Training loss: 0.064 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.2% ... Training loss: 0.061 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.3% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.3% ... Training loss: 0.061 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.3% ... Training loss: 0.066 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.3% ... Training loss: 0.061 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.3% ... Training loss: 0.062 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.3% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.4% ... Training loss: 0.062 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.4% ... Training loss: 0.062 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.4% ... Training loss: 0.062 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.4% ... Training loss: 0.066 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.4% ... Training loss: 0.069 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.5% ... Training loss: 0.068 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.5% ... Training loss: 0.061 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.5% ... Training loss: 0.067 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.5% ... Training loss: 0.061 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.5% ... Training loss: 0.064 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.5% ... Training loss: 0.065 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.5% ... Training loss: 0.068 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.6% ... Training loss: 0.063 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.6% ... Training loss: 0.067 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.6% ... Training loss: 0.063 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.6% ... Training loss: 0.061 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.6% ... Training loss: 0.062 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.7% ... Training loss: 0.063 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.7% ... Training loss: 0.063 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.7% ... Training loss: 0.062 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.7% ... Training loss: 0.062 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.7% ... Training loss: 0.062 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.7% ... Training loss: 0.062 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.8% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.8% ... Training loss: 0.064 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.8% ... Training loss: 0.063 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.8% ... Training loss: 0.062 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.8% ... Training loss: 0.068 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.8% ... Training loss: 0.062 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.8% ... Training loss: 0.062 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.9% ... Training loss: 0.065 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.9% ... Training loss: 0.067 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.9% ... Training loss: 0.068 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.9% ... Training loss: 0.066 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 78.9% ... Training loss: 0.061 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.0% ... Training loss: 0.063 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.0% ... Training loss: 0.061 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.0% ... Training loss: 0.061 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.0% ... Training loss: 0.061 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.0% ... Training loss: 0.061 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.0% ... Training loss: 0.061 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.0% ... Training loss: 0.061 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.1% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.1% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.1% ... Training loss: 0.064 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.1% ... Training loss: 0.063 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.1% ... Training loss: 0.062 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 79.2% ... Training loss: 0.061 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.2% ... Training loss: 0.075 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.2% ... Training loss: 0.088 ... Validation loss: 0.191(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.2% ... Training loss: 0.088 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.2% ... Training loss: 0.071 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.2% ... Training loss: 0.069 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.2% ... Training loss: 0.062 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.3% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.3% ... Training loss: 0.061 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.3% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.3% ... Training loss: 0.065 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.3% ... Training loss: 0.062 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.3% ... Training loss: 0.062 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.4% ... Training loss: 0.067 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.4% ... Training loss: 0.061 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.4% ... Training loss: 0.061 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.4% ... Training loss: 0.062 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.4% ... Training loss: 0.067 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.5% ... Training loss: 0.061 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.5% ... Training loss: 0.061 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.5% ... Training loss: 0.061 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.5% ... Training loss: 0.063 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.5% ... Training loss: 0.061 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.5% ... Training loss: 0.066 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.5% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.6% ... Training loss: 0.062 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.6% ... Training loss: 0.061 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.6% ... Training loss: 0.062 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.6% ... Training loss: 0.067 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.6% ... Training loss: 0.078 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.7% ... Training loss: 0.073 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.7% ... Training loss: 0.063 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.7% ... Training loss: 0.067 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.7% ... Training loss: 0.069 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.7% ... Training loss: 0.064 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.7% ... Training loss: 0.065 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.8% ... Training loss: 0.068 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.8% ... Training loss: 0.076 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.8% ... Training loss: 0.078 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.8% ... Training loss: 0.070 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.8% ... Training loss: 0.064 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.8% ... Training loss: 0.060 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.8% ... Training loss: 0.061 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.9% ... Training loss: 0.060 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.9% ... Training loss: 0.062 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.9% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.9% ... Training loss: 0.061 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 79.9% ... Training loss: 0.061 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.0% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.0% ... Training loss: 0.061 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.0% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.0% ... Training loss: 0.064 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.0% ... Training loss: 0.063 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.0% ... Training loss: 0.065 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.0% ... Training loss: 0.062 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.1% ... Training loss: 0.061 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.1% ... Training loss: 0.063 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.1% ... Training loss: 0.061 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.1% ... Training loss: 0.063 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.1% ... Training loss: 0.064 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.2% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.2% ... Training loss: 0.061 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.2% ... Training loss: 0.063 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.2% ... Training loss: 0.071 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.2% ... Training loss: 0.063 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.2% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.2% ... Training loss: 0.061 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.3% ... Training loss: 0.063 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.3% ... Training loss: 0.065 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.3% ... Training loss: 0.071 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.3% ... Training loss: 0.066 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.3% ... Training loss: 0.070 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.3% ... Training loss: 0.062 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.4% ... Training loss: 0.065 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.4% ... Training loss: 0.062 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.4% ... Training loss: 0.063 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.4% ... Training loss: 0.064 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.4% ... Training loss: 0.061 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.5% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.5% ... Training loss: 0.060 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.5% ... Training loss: 0.062 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.5% ... Training loss: 0.062 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.5% ... Training loss: 0.064 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.5% ... Training loss: 0.065 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.5% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.6% ... Training loss: 0.064 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.6% ... Training loss: 0.063 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.6% ... Training loss: 0.066 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.6% ... Training loss: 0.065 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.6% ... Training loss: 0.061 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.7% ... Training loss: 0.074 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.7% ... Training loss: 0.062 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.7% ... Training loss: 0.062 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 80.7% ... Training loss: 0.061 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.7% ... Training loss: 0.063 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.7% ... Training loss: 0.061 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.8% ... Training loss: 0.061 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.8% ... Training loss: 0.060 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.8% ... Training loss: 0.065 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.8% ... Training loss: 0.065 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.8% ... Training loss: 0.061 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.8% ... Training loss: 0.062 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.8% ... Training loss: 0.067 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.9% ... Training loss: 0.062 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.9% ... Training loss: 0.061 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.9% ... Training loss: 0.061 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.9% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 80.9% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.0% ... Training loss: 0.060 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.0% ... Training loss: 0.061 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.0% ... Training loss: 0.068 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.0% ... Training loss: 0.066 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.0% ... Training loss: 0.061 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.0% ... Training loss: 0.068 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.0% ... Training loss: 0.061 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.1% ... Training loss: 0.061 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.1% ... Training loss: 0.062 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.1% ... Training loss: 0.061 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.1% ... Training loss: 0.061 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.1% ... Training loss: 0.061 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.2% ... Training loss: 0.064 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.2% ... Training loss: 0.061 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.2% ... Training loss: 0.061 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.2% ... Training loss: 0.065 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.2% ... Training loss: 0.062 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.2% ... Training loss: 0.061 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.2% ... Training loss: 0.062 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.3% ... Training loss: 0.069 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.3% ... Training loss: 0.079 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.3% ... Training loss: 0.064 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.3% ... Training loss: 0.060 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.3% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.3% ... Training loss: 0.065 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.4% ... Training loss: 0.062 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.4% ... Training loss: 0.063 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.4% ... Training loss: 0.066 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.4% ... Training loss: 0.064 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.4% ... Training loss: 0.064 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.5% ... Training loss: 0.064 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.5% ... Training loss: 0.062 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.5% ... Training loss: 0.060 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.5% ... Training loss: 0.063 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.5% ... Training loss: 0.060 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.5% ... Training loss: 0.060 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.5% ... Training loss: 0.062 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.6% ... Training loss: 0.062 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.6% ... Training loss: 0.061 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.6% ... Training loss: 0.062 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.6% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.6% ... Training loss: 0.061 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.7% ... Training loss: 0.062 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.7% ... Training loss: 0.060 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.7% ... Training loss: 0.060 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.7% ... Training loss: 0.060 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.7% ... Training loss: 0.064 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.7% ... Training loss: 0.062 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.8% ... Training loss: 0.062 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.8% ... Training loss: 0.066 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.8% ... Training loss: 0.061 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.8% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.8% ... Training loss: 0.060 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.8% ... Training loss: 0.060 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.8% ... Training loss: 0.061 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.9% ... Training loss: 0.061 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.9% ... Training loss: 0.061 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.9% ... Training loss: 0.064 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.9% ... Training loss: 0.061 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 81.9% ... Training loss: 0.060 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.0% ... Training loss: 0.060 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.0% ... Training loss: 0.063 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.0% ... Training loss: 0.068 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.0% ... Training loss: 0.072 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.0% ... Training loss: 0.065 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.0% ... Training loss: 0.061 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.0% ... Training loss: 0.061 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.1% ... Training loss: 0.065 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.1% ... Training loss: 0.062 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.1% ... Training loss: 0.061 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.1% ... Training loss: 0.061 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.1% ... Training loss: 0.065 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.2% ... Training loss: 0.060 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.2% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.2% ... Training loss: 0.060 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.2% ... Training loss: 0.067 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.2% ... Training loss: 0.061 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.2% ... Training loss: 0.065 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 82.2% ... Training loss: 0.071 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.3% ... Training loss: 0.070 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.3% ... Training loss: 0.063 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.3% ... Training loss: 0.064 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.3% ... Training loss: 0.067 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.3% ... Training loss: 0.075 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.3% ... Training loss: 0.073 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.4% ... Training loss: 0.063 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.4% ... Training loss: 0.060 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.4% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.4% ... Training loss: 0.067 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.4% ... Training loss: 0.064 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.5% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.5% ... Training loss: 0.067 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.5% ... Training loss: 0.063 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.5% ... Training loss: 0.062 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.5% ... Training loss: 0.066 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.5% ... Training loss: 0.069 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.5% ... Training loss: 0.063 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.6% ... Training loss: 0.061 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.6% ... Training loss: 0.066 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.6% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.6% ... Training loss: 0.061 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.6% ... Training loss: 0.064 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.7% ... Training loss: 0.064 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.7% ... Training loss: 0.066 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.7% ... Training loss: 0.064 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.7% ... Training loss: 0.062 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.7% ... Training loss: 0.061 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.7% ... Training loss: 0.063 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.8% ... Training loss: 0.061 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.8% ... Training loss: 0.061 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.8% ... Training loss: 0.061 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.8% ... Training loss: 0.060 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.8% ... Training loss: 0.062 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.8% ... Training loss: 0.064 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.8% ... Training loss: 0.061 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.9% ... Training loss: 0.061 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.9% ... Training loss: 0.061 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.9% ... Training loss: 0.060 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.9% ... Training loss: 0.061 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 82.9% ... Training loss: 0.061 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.0% ... Training loss: 0.062 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.0% ... Training loss: 0.061 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.0% ... Training loss: 0.063 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.0% ... Training loss: 0.062 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.0% ... Training loss: 0.062 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.0% ... Training loss: 0.061 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.0% ... Training loss: 0.060 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.1% ... Training loss: 0.060 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.1% ... Training loss: 0.061 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.1% ... Training loss: 0.064 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.1% ... Training loss: 0.061 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.1% ... Training loss: 0.060 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.2% ... Training loss: 0.061 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.2% ... Training loss: 0.063 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.2% ... Training loss: 0.061 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.2% ... Training loss: 0.060 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.2% ... Training loss: 0.060 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.2% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.2% ... Training loss: 0.068 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.3% ... Training loss: 0.064 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.3% ... Training loss: 0.063 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.3% ... Training loss: 0.061 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.3% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.3% ... Training loss: 0.061 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.3% ... Training loss: 0.061 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.4% ... Training loss: 0.063 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.4% ... Training loss: 0.060 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.4% ... Training loss: 0.060 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.4% ... Training loss: 0.060 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.4% ... Training loss: 0.064 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.5% ... Training loss: 0.065 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.5% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.5% ... Training loss: 0.061 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.5% ... Training loss: 0.065 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.5% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.5% ... Training loss: 0.061 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.5% ... Training loss: 0.061 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.6% ... Training loss: 0.060 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.6% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.6% ... Training loss: 0.068 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.6% ... Training loss: 0.064 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.6% ... Training loss: 0.063 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.7% ... Training loss: 0.066 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.7% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.7% ... Training loss: 0.060 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.7% ... Training loss: 0.070 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.7% ... Training loss: 0.063 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.7% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.8% ... Training loss: 0.068 ... Validation loss: 0.194(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.8% ... Training loss: 0.067 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.8% ... Training loss: 0.064 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 83.8% ... Training loss: 0.061 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.8% ... Training loss: 0.062 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.8% ... Training loss: 0.067 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.8% ... Training loss: 0.066 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.9% ... Training loss: 0.065 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.9% ... Training loss: 0.062 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.9% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.9% ... Training loss: 0.060 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 83.9% ... Training loss: 0.061 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.0% ... Training loss: 0.060 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.0% ... Training loss: 0.066 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.0% ... Training loss: 0.062 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.0% ... Training loss: 0.060 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.0% ... Training loss: 0.061 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.0% ... Training loss: 0.060 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.0% ... Training loss: 0.060 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.1% ... Training loss: 0.062 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.1% ... Training loss: 0.061 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.1% ... Training loss: 0.059 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.1% ... Training loss: 0.060 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.1% ... Training loss: 0.065 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.2% ... Training loss: 0.061 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.2% ... Training loss: 0.060 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.2% ... Training loss: 0.060 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.2% ... Training loss: 0.060 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.2% ... Training loss: 0.060 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.2% ... Training loss: 0.060 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.2% ... Training loss: 0.060 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.3% ... Training loss: 0.061 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.3% ... Training loss: 0.071 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.3% ... Training loss: 0.063 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.3% ... Training loss: 0.060 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.3% ... Training loss: 0.060 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.3% ... Training loss: 0.060 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.4% ... Training loss: 0.060 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.4% ... Training loss: 0.060 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.4% ... Training loss: 0.061 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.4% ... Training loss: 0.060 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.4% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.5% ... Training loss: 0.060 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.5% ... Training loss: 0.060 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.5% ... Training loss: 0.061 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.5% ... Training loss: 0.060 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.5% ... Training loss: 0.060 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.5% ... Training loss: 0.060 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.5% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.6% ... Training loss: 0.060 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.6% ... Training loss: 0.060 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.6% ... Training loss: 0.059 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.6% ... Training loss: 0.060 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.6% ... Training loss: 0.060 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.7% ... Training loss: 0.061 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.7% ... Training loss: 0.066 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.7% ... Training loss: 0.063 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.7% ... Training loss: 0.062 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.7% ... Training loss: 0.068 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.7% ... Training loss: 0.061 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.8% ... Training loss: 0.064 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.8% ... Training loss: 0.066 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.8% ... Training loss: 0.071 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.8% ... Training loss: 0.075 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.8% ... Training loss: 0.079 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.8% ... Training loss: 0.060 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.8% ... Training loss: 0.060 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.9% ... Training loss: 0.060 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.9% ... Training loss: 0.060 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.9% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.9% ... Training loss: 0.061 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 84.9% ... Training loss: 0.066 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.0% ... Training loss: 0.067 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.0% ... Training loss: 0.068 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.0% ... Training loss: 0.073 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.0% ... Training loss: 0.073 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.0% ... Training loss: 0.074 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.0% ... Training loss: 0.064 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.0% ... Training loss: 0.064 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.1% ... Training loss: 0.060 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.1% ... Training loss: 0.059 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.1% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.1% ... Training loss: 0.060 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.1% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.2% ... Training loss: 0.061 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.2% ... Training loss: 0.060 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.2% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.2% ... Training loss: 0.059 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.2% ... Training loss: 0.061 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.2% ... Training loss: 0.064 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.2% ... Training loss: 0.069 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.3% ... Training loss: 0.069 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.3% ... Training loss: 0.069 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.3% ... Training loss: 0.067 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.3% ... Training loss: 0.061 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.3% ... Training loss: 0.061 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 85.3% ... Training loss: 0.061 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.4% ... Training loss: 0.063 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.4% ... Training loss: 0.062 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.4% ... Training loss: 0.062 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.4% ... Training loss: 0.060 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.4% ... Training loss: 0.061 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.5% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.5% ... Training loss: 0.061 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.5% ... Training loss: 0.065 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.5% ... Training loss: 0.061 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.5% ... Training loss: 0.060 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.5% ... Training loss: 0.063 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.5% ... Training loss: 0.063 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.6% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.6% ... Training loss: 0.059 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.6% ... Training loss: 0.059 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.6% ... Training loss: 0.061 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.6% ... Training loss: 0.061 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.7% ... Training loss: 0.066 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.7% ... Training loss: 0.061 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.7% ... Training loss: 0.062 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.7% ... Training loss: 0.061 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.7% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.7% ... Training loss: 0.063 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.8% ... Training loss: 0.060 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.8% ... Training loss: 0.060 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.8% ... Training loss: 0.061 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.8% ... Training loss: 0.077 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.8% ... Training loss: 0.070 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.8% ... Training loss: 0.068 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.8% ... Training loss: 0.064 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.9% ... Training loss: 0.090 ... Validation loss: 0.229(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.9% ... Training loss: 0.088 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.9% ... Training loss: 0.076 ... Validation loss: 0.218(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.9% ... Training loss: 0.065 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 85.9% ... Training loss: 0.060 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.0% ... Training loss: 0.060 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.0% ... Training loss: 0.059 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.0% ... Training loss: 0.060 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.0% ... Training loss: 0.060 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.0% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.0% ... Training loss: 0.062 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.0% ... Training loss: 0.059 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.1% ... Training loss: 0.061 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.1% ... Training loss: 0.059 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.1% ... Training loss: 0.064 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.1% ... Training loss: 0.063 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.1% ... Training loss: 0.066 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.2% ... Training loss: 0.092 ... Validation loss: 0.221(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.2% ... Training loss: 0.072 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.2% ... Training loss: 0.069 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.2% ... Training loss: 0.075 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.2% ... Training loss: 0.070 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.2% ... Training loss: 0.074 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.2% ... Training loss: 0.064 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.3% ... Training loss: 0.063 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.3% ... Training loss: 0.061 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.3% ... Training loss: 0.061 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.3% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.3% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.3% ... Training loss: 0.060 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.4% ... Training loss: 0.066 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.4% ... Training loss: 0.061 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.4% ... Training loss: 0.061 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.4% ... Training loss: 0.063 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.4% ... Training loss: 0.059 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.5% ... Training loss: 0.059 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.5% ... Training loss: 0.059 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.5% ... Training loss: 0.064 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.5% ... Training loss: 0.076 ... Validation loss: 0.188(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.5% ... Training loss: 0.082 ... Validation loss: 0.131(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.5% ... Training loss: 0.072 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.5% ... Training loss: 0.069 ... Validation loss: 0.131(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.6% ... Training loss: 0.062 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.6% ... Training loss: 0.059 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.6% ... Training loss: 0.060 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.6% ... Training loss: 0.060 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.6% ... Training loss: 0.060 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.7% ... Training loss: 0.061 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.7% ... Training loss: 0.060 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.7% ... Training loss: 0.062 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.7% ... Training loss: 0.061 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.7% ... Training loss: 0.063 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.7% ... Training loss: 0.060 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.8% ... Training loss: 0.061 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.8% ... Training loss: 0.060 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.8% ... Training loss: 0.064 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.8% ... Training loss: 0.060 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.8% ... Training loss: 0.071 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.8% ... Training loss: 0.063 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.8% ... Training loss: 0.080 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.9% ... Training loss: 0.078 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.9% ... Training loss: 0.062 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 86.9% ... Training loss: 0.064 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.9% ... Training loss: 0.084 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 86.9% ... Training loss: 0.061 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.0% ... Training loss: 0.062 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.0% ... Training loss: 0.065 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.0% ... Training loss: 0.061 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.0% ... Training loss: 0.060 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.0% ... Training loss: 0.059 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.0% ... Training loss: 0.061 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.0% ... Training loss: 0.063 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.1% ... Training loss: 0.060 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.1% ... Training loss: 0.059 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.1% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.1% ... Training loss: 0.064 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.1% ... Training loss: 0.066 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.2% ... Training loss: 0.063 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.2% ... Training loss: 0.068 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.2% ... Training loss: 0.075 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.2% ... Training loss: 0.075 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.2% ... Training loss: 0.066 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.2% ... Training loss: 0.064 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.2% ... Training loss: 0.062 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.3% ... Training loss: 0.061 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.3% ... Training loss: 0.060 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.3% ... Training loss: 0.060 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.3% ... Training loss: 0.060 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.3% ... Training loss: 0.062 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.3% ... Training loss: 0.061 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.4% ... Training loss: 0.060 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.4% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.4% ... Training loss: 0.068 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.4% ... Training loss: 0.072 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.4% ... Training loss: 0.080 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.5% ... Training loss: 0.074 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.5% ... Training loss: 0.080 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.5% ... Training loss: 0.069 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.5% ... Training loss: 0.060 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.5% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.5% ... Training loss: 0.061 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.5% ... Training loss: 0.062 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.6% ... Training loss: 0.059 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.6% ... Training loss: 0.065 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.6% ... Training loss: 0.062 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.6% ... Training loss: 0.064 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.6% ... Training loss: 0.094 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.7% ... Training loss: 0.080 ... Validation loss: 0.217(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.7% ... Training loss: 0.080 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.7% ... Training loss: 0.077 ... Validation loss: 0.207(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.7% ... Training loss: 0.076 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.7% ... Training loss: 0.072 ... Validation loss: 0.188(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.7% ... Training loss: 0.075 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.8% ... Training loss: 0.074 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.8% ... Training loss: 0.072 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.8% ... Training loss: 0.078 ... Validation loss: 0.189(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.8% ... Training loss: 0.074 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.8% ... Training loss: 0.062 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.8% ... Training loss: 0.060 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.8% ... Training loss: 0.061 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.9% ... Training loss: 0.064 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.9% ... Training loss: 0.067 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.9% ... Training loss: 0.069 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.9% ... Training loss: 0.064 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 87.9% ... Training loss: 0.064 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.0% ... Training loss: 0.064 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.0% ... Training loss: 0.060 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.0% ... Training loss: 0.062 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.0% ... Training loss: 0.063 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.0% ... Training loss: 0.061 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.0% ... Training loss: 0.060 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.0% ... Training loss: 0.061 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.1% ... Training loss: 0.061 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.1% ... Training loss: 0.064 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.1% ... Training loss: 0.062 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.1% ... Training loss: 0.064 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.1% ... Training loss: 0.060 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.2% ... Training loss: 0.061 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.2% ... Training loss: 0.075 ... Validation loss: 0.193(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.2% ... Training loss: 0.062 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.2% ... Training loss: 0.068 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.2% ... Training loss: 0.062 ... Validation loss: 0.133(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.2% ... Training loss: 0.062 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.2% ... Training loss: 0.061 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.3% ... Training loss: 0.059 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.3% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.3% ... Training loss: 0.060 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.3% ... Training loss: 0.060 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.3% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.3% ... Training loss: 0.060 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.4% ... Training loss: 0.064 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.4% ... Training loss: 0.067 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.4% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.4% ... Training loss: 0.061 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.4% ... Training loss: 0.064 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 88.5% ... Training loss: 0.070 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.5% ... Training loss: 0.067 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.5% ... Training loss: 0.066 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.5% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.5% ... Training loss: 0.060 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.5% ... Training loss: 0.065 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.5% ... Training loss: 0.064 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.6% ... Training loss: 0.063 ... Validation loss: 0.133(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.6% ... Training loss: 0.069 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.6% ... Training loss: 0.066 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.6% ... Training loss: 0.064 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.6% ... Training loss: 0.059 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.7% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.7% ... Training loss: 0.070 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.7% ... Training loss: 0.061 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.7% ... Training loss: 0.059 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.7% ... Training loss: 0.059 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.7% ... Training loss: 0.062 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.8% ... Training loss: 0.061 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.8% ... Training loss: 0.064 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.8% ... Training loss: 0.061 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.8% ... Training loss: 0.061 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.8% ... Training loss: 0.059 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.8% ... Training loss: 0.061 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.8% ... Training loss: 0.060 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.9% ... Training loss: 0.059 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.9% ... Training loss: 0.060 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.9% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.9% ... Training loss: 0.062 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 88.9% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.0% ... Training loss: 0.059 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.0% ... Training loss: 0.066 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.0% ... Training loss: 0.060 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.0% ... Training loss: 0.059 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.0% ... Training loss: 0.063 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.0% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.0% ... Training loss: 0.060 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.1% ... Training loss: 0.060 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.1% ... Training loss: 0.061 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.1% ... Training loss: 0.064 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.1% ... Training loss: 0.060 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.1% ... Training loss: 0.071 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.2% ... Training loss: 0.077 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.2% ... Training loss: 0.071 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.2% ... Training loss: 0.063 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.2% ... Training loss: 0.060 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.2% ... Training loss: 0.060 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.2% ... Training loss: 0.060 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.2% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.3% ... Training loss: 0.060 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.3% ... Training loss: 0.065 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.3% ... Training loss: 0.060 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.3% ... Training loss: 0.059 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.3% ... Training loss: 0.060 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.3% ... Training loss: 0.062 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.4% ... Training loss: 0.060 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.4% ... Training loss: 0.060 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.4% ... Training loss: 0.061 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.4% ... Training loss: 0.063 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.4% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.5% ... Training loss: 0.066 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.5% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.5% ... Training loss: 0.062 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.5% ... Training loss: 0.071 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.5% ... Training loss: 0.068 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.5% ... Training loss: 0.062 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.5% ... Training loss: 0.061 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.6% ... Training loss: 0.060 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.6% ... Training loss: 0.063 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.6% ... Training loss: 0.063 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.6% ... Training loss: 0.060 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.6% ... Training loss: 0.060 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.7% ... Training loss: 0.059 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.7% ... Training loss: 0.063 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.7% ... Training loss: 0.060 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.7% ... Training loss: 0.069 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.7% ... Training loss: 0.060 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.7% ... Training loss: 0.060 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.8% ... Training loss: 0.065 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.8% ... Training loss: 0.062 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.8% ... Training loss: 0.068 ... Validation loss: 0.193(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.8% ... Training loss: 0.060 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.8% ... Training loss: 0.062 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.8% ... Training loss: 0.066 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.8% ... Training loss: 0.060 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.9% ... Training loss: 0.062 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.9% ... Training loss: 0.059 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.9% ... Training loss: 0.060 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.9% ... Training loss: 0.059 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 89.9% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.0% ... Training loss: 0.062 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.0% ... Training loss: 0.060 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.0% ... Training loss: 0.060 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 90.0% ... Training loss: 0.059 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.0% ... Training loss: 0.059 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.0% ... Training loss: 0.062 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.0% ... Training loss: 0.061 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.1% ... Training loss: 0.060 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.1% ... Training loss: 0.059 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.1% ... Training loss: 0.059 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.1% ... Training loss: 0.059 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.1% ... Training loss: 0.064 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.2% ... Training loss: 0.070 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.2% ... Training loss: 0.068 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.2% ... Training loss: 0.071 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.2% ... Training loss: 0.060 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.2% ... Training loss: 0.059 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.2% ... Training loss: 0.061 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.2% ... Training loss: 0.062 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.3% ... Training loss: 0.061 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.3% ... Training loss: 0.061 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.3% ... Training loss: 0.077 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.3% ... Training loss: 0.063 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.3% ... Training loss: 0.062 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.3% ... Training loss: 0.063 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.4% ... Training loss: 0.059 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.4% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.4% ... Training loss: 0.062 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.4% ... Training loss: 0.074 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.4% ... Training loss: 0.074 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.5% ... Training loss: 0.070 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.5% ... Training loss: 0.069 ... Validation loss: 0.133(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.5% ... Training loss: 0.067 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.5% ... Training loss: 0.077 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.5% ... Training loss: 0.064 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.5% ... Training loss: 0.061 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.5% ... Training loss: 0.062 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.6% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.6% ... Training loss: 0.066 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.6% ... Training loss: 0.067 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.6% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.6% ... Training loss: 0.071 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.7% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.7% ... Training loss: 0.059 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.7% ... Training loss: 0.069 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.7% ... Training loss: 0.067 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.7% ... Training loss: 0.075 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.7% ... Training loss: 0.060 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.8% ... Training loss: 0.060 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.8% ... Training loss: 0.062 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.8% ... Training loss: 0.067 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.8% ... Training loss: 0.063 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.8% ... Training loss: 0.062 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.8% ... Training loss: 0.064 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.8% ... Training loss: 0.059 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.9% ... Training loss: 0.059 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.9% ... Training loss: 0.061 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.9% ... Training loss: 0.059 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.9% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 90.9% ... Training loss: 0.061 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.0% ... Training loss: 0.066 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.0% ... Training loss: 0.066 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.0% ... Training loss: 0.063 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.0% ... Training loss: 0.061 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.0% ... Training loss: 0.060 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.0% ... Training loss: 0.060 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.0% ... Training loss: 0.066 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.1% ... Training loss: 0.064 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.1% ... Training loss: 0.059 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.1% ... Training loss: 0.060 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.1% ... Training loss: 0.060 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.1% ... Training loss: 0.061 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.2% ... Training loss: 0.071 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.2% ... Training loss: 0.071 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.2% ... Training loss: 0.063 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.2% ... Training loss: 0.060 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.2% ... Training loss: 0.063 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.2% ... Training loss: 0.065 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.2% ... Training loss: 0.067 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.3% ... Training loss: 0.066 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.3% ... Training loss: 0.075 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.3% ... Training loss: 0.064 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.3% ... Training loss: 0.061 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.3% ... Training loss: 0.062 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.3% ... Training loss: 0.060 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.4% ... Training loss: 0.059 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.4% ... Training loss: 0.060 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.4% ... Training loss: 0.061 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.4% ... Training loss: 0.072 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.4% ... Training loss: 0.063 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.5% ... Training loss: 0.060 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.5% ... Training loss: 0.060 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.5% ... Training loss: 0.070 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.5% ... Training loss: 0.062 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.5% ... Training loss: 0.067 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.5% ... Training loss: 0.063 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 91.5% ... Training loss: 0.063 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.6% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.6% ... Training loss: 0.059 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.6% ... Training loss: 0.064 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.6% ... Training loss: 0.060 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.6% ... Training loss: 0.064 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.7% ... Training loss: 0.059 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.7% ... Training loss: 0.061 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.7% ... Training loss: 0.058 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.7% ... Training loss: 0.059 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.7% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.7% ... Training loss: 0.059 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.8% ... Training loss: 0.067 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.8% ... Training loss: 0.066 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.8% ... Training loss: 0.061 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.8% ... Training loss: 0.065 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.8% ... Training loss: 0.059 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.8% ... Training loss: 0.060 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.8% ... Training loss: 0.059 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.9% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.9% ... Training loss: 0.059 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.9% ... Training loss: 0.060 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.9% ... Training loss: 0.059 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 91.9% ... Training loss: 0.060 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.0% ... Training loss: 0.059 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.0% ... Training loss: 0.060 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.0% ... Training loss: 0.059 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.0% ... Training loss: 0.059 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.0% ... Training loss: 0.062 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.0% ... Training loss: 0.063 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.0% ... Training loss: 0.059 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.1% ... Training loss: 0.060 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.1% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.1% ... Training loss: 0.059 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.1% ... Training loss: 0.062 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.1% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.2% ... Training loss: 0.062 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.2% ... Training loss: 0.061 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.2% ... Training loss: 0.065 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.2% ... Training loss: 0.061 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.2% ... Training loss: 0.065 ... Validation loss: 0.187(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.2% ... Training loss: 0.064 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.2% ... Training loss: 0.059 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.3% ... Training loss: 0.065 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.3% ... Training loss: 0.060 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.3% ... Training loss: 0.059 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.3% ... Training loss: 0.059 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.3% ... Training loss: 0.062 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.3% ... Training loss: 0.061 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.4% ... Training loss: 0.059 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.4% ... Training loss: 0.059 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.4% ... Training loss: 0.059 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.4% ... Training loss: 0.063 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.4% ... Training loss: 0.060 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.5% ... Training loss: 0.059 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.5% ... Training loss: 0.060 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.5% ... Training loss: 0.062 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.5% ... Training loss: 0.060 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.5% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.5% ... Training loss: 0.059 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.5% ... Training loss: 0.059 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.6% ... Training loss: 0.059 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.6% ... Training loss: 0.059 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.6% ... Training loss: 0.061 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.6% ... Training loss: 0.059 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.6% ... Training loss: 0.058 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.7% ... Training loss: 0.058 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.7% ... Training loss: 0.062 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.7% ... Training loss: 0.061 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.7% ... Training loss: 0.059 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.7% ... Training loss: 0.059 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.7% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.8% ... Training loss: 0.061 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.8% ... Training loss: 0.062 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.8% ... Training loss: 0.070 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.8% ... Training loss: 0.069 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.8% ... Training loss: 0.064 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.8% ... Training loss: 0.062 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.8% ... Training loss: 0.060 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.9% ... Training loss: 0.058 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.9% ... Training loss: 0.059 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.9% ... Training loss: 0.060 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.9% ... Training loss: 0.067 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 92.9% ... Training loss: 0.061 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.0% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.0% ... Training loss: 0.059 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.0% ... Training loss: 0.058 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.0% ... Training loss: 0.060 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.0% ... Training loss: 0.059 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.0% ... Training loss: 0.059 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.0% ... Training loss: 0.058 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.1% ... Training loss: 0.064 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.1% ... Training loss: 0.060 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 93.1% ... Training loss: 0.060 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.1% ... Training loss: 0.063 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.1% ... Training loss: 0.061 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.2% ... Training loss: 0.060 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.2% ... Training loss: 0.059 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.2% ... Training loss: 0.072 ... Validation loss: 0.200(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.2% ... Training loss: 0.068 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.2% ... Training loss: 0.074 ... Validation loss: 0.210(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.2% ... Training loss: 0.066 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.2% ... Training loss: 0.066 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.3% ... Training loss: 0.067 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.3% ... Training loss: 0.080 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.3% ... Training loss: 0.064 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.3% ... Training loss: 0.062 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.3% ... Training loss: 0.061 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.3% ... Training loss: 0.058 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.4% ... Training loss: 0.066 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.4% ... Training loss: 0.072 ... Validation loss: 0.184(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.4% ... Training loss: 0.060 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.4% ... Training loss: 0.058 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.4% ... Training loss: 0.068 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.5% ... Training loss: 0.066 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.5% ... Training loss: 0.061 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.5% ... Training loss: 0.068 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.5% ... Training loss: 0.068 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.5% ... Training loss: 0.066 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.5% ... Training loss: 0.058 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.5% ... Training loss: 0.060 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.6% ... Training loss: 0.064 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.6% ... Training loss: 0.064 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.6% ... Training loss: 0.062 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.6% ... Training loss: 0.060 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.6% ... Training loss: 0.060 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.7% ... Training loss: 0.060 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.7% ... Training loss: 0.076 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.7% ... Training loss: 0.060 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.7% ... Training loss: 0.060 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.7% ... Training loss: 0.061 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.7% ... Training loss: 0.061 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.8% ... Training loss: 0.060 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.8% ... Training loss: 0.065 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.8% ... Training loss: 0.061 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.8% ... Training loss: 0.059 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.8% ... Training loss: 0.063 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.8% ... Training loss: 0.061 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.8% ... Training loss: 0.061 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.9% ... Training loss: 0.059 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.9% ... Training loss: 0.065 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.9% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.9% ... Training loss: 0.059 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 93.9% ... Training loss: 0.064 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.0% ... Training loss: 0.064 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.0% ... Training loss: 0.059 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.0% ... Training loss: 0.061 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.0% ... Training loss: 0.062 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.0% ... Training loss: 0.060 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.0% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.0% ... Training loss: 0.059 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.1% ... Training loss: 0.059 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.1% ... Training loss: 0.058 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.1% ... Training loss: 0.062 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.1% ... Training loss: 0.059 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.1% ... Training loss: 0.061 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.2% ... Training loss: 0.063 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.2% ... Training loss: 0.060 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.2% ... Training loss: 0.060 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.2% ... Training loss: 0.058 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.2% ... Training loss: 0.069 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.2% ... Training loss: 0.061 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.2% ... Training loss: 0.058 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.3% ... Training loss: 0.067 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.3% ... Training loss: 0.060 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.3% ... Training loss: 0.059 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.3% ... Training loss: 0.059 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.3% ... Training loss: 0.060 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.3% ... Training loss: 0.064 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.4% ... Training loss: 0.061 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.4% ... Training loss: 0.061 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.4% ... Training loss: 0.060 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.4% ... Training loss: 0.058 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.4% ... Training loss: 0.058 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.5% ... Training loss: 0.061 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.5% ... Training loss: 0.058 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.5% ... Training loss: 0.064 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.5% ... Training loss: 0.066 ... Validation loss: 0.199(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.5% ... Training loss: 0.058 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.5% ... Training loss: 0.059 ... Validation loss: 0.163(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.5% ... Training loss: 0.059 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.6% ... Training loss: 0.060 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.6% ... Training loss: 0.060 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.6% ... Training loss: 0.059 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.6% ... Training loss: 0.059 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.6% ... Training loss: 0.060 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 94.7% ... Training loss: 0.058 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.7% ... Training loss: 0.058 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.7% ... Training loss: 0.059 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.7% ... Training loss: 0.062 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.7% ... Training loss: 0.059 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.7% ... Training loss: 0.059 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.8% ... Training loss: 0.059 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.8% ... Training loss: 0.060 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.8% ... Training loss: 0.059 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.8% ... Training loss: 0.058 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.8% ... Training loss: 0.065 ... Validation loss: 0.183(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.8% ... Training loss: 0.064 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.8% ... Training loss: 0.060 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.9% ... Training loss: 0.058 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.9% ... Training loss: 0.059 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.9% ... Training loss: 0.059 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.9% ... Training loss: 0.061 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 94.9% ... Training loss: 0.062 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.0% ... Training loss: 0.060 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.0% ... Training loss: 0.059 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.0% ... Training loss: 0.064 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.0% ... Training loss: 0.062 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.0% ... Training loss: 0.059 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.0% ... Training loss: 0.059 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.0% ... Training loss: 0.060 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.1% ... Training loss: 0.063 ... Validation loss: 0.182(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.1% ... Training loss: 0.062 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.1% ... Training loss: 0.059 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.1% ... Training loss: 0.061 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.1% ... Training loss: 0.060 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.2% ... Training loss: 0.061 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.2% ... Training loss: 0.062 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.2% ... Training loss: 0.065 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.2% ... Training loss: 0.067 ... Validation loss: 0.189(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.2% ... Training loss: 0.066 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.2% ... Training loss: 0.059 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.2% ... Training loss: 0.062 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.3% ... Training loss: 0.067 ... Validation loss: 0.188(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.3% ... Training loss: 0.072 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.3% ... Training loss: 0.073 ... Validation loss: 0.204(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.3% ... Training loss: 0.066 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.3% ... Training loss: 0.068 ... Validation loss: 0.196(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.3% ... Training loss: 0.058 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.4% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.4% ... Training loss: 0.059 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.4% ... Training loss: 0.059 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.4% ... Training loss: 0.077 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.4% ... Training loss: 0.089 ... Validation loss: 0.219(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.5% ... Training loss: 0.082 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.5% ... Training loss: 0.065 ... Validation loss: 0.185(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.5% ... Training loss: 0.062 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.5% ... Training loss: 0.060 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.5% ... Training loss: 0.059 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.5% ... Training loss: 0.058 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.5% ... Training loss: 0.059 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.6% ... Training loss: 0.058 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.6% ... Training loss: 0.065 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.6% ... Training loss: 0.060 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.6% ... Training loss: 0.058 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.6% ... Training loss: 0.059 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.7% ... Training loss: 0.060 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.7% ... Training loss: 0.059 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.7% ... Training loss: 0.061 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.7% ... Training loss: 0.059 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.7% ... Training loss: 0.060 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.7% ... Training loss: 0.059 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.8% ... Training loss: 0.058 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.8% ... Training loss: 0.061 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.8% ... Training loss: 0.058 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.8% ... Training loss: 0.058 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.8% ... Training loss: 0.058 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.8% ... Training loss: 0.058 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.8% ... Training loss: 0.058 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.9% ... Training loss: 0.058 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.9% ... Training loss: 0.058 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.9% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.9% ... Training loss: 0.066 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 95.9% ... Training loss: 0.061 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.0% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.0% ... Training loss: 0.064 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.0% ... Training loss: 0.058 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.0% ... Training loss: 0.060 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.0% ... Training loss: 0.058 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.0% ... Training loss: 0.060 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.0% ... Training loss: 0.058 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.1% ... Training loss: 0.060 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.1% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.1% ... Training loss: 0.060 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.1% ... Training loss: 0.065 ... Validation loss: 0.177(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.1% ... Training loss: 0.061 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.2% ... Training loss: 0.066 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.2% ... Training loss: 0.058 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.2% ... Training loss: 0.058 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 96.2% ... Training loss: 0.059 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.2% ... Training loss: 0.061 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.2% ... Training loss: 0.061 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.2% ... Training loss: 0.059 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.3% ... Training loss: 0.061 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.3% ... Training loss: 0.059 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.3% ... Training loss: 0.060 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.3% ... Training loss: 0.060 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.3% ... Training loss: 0.059 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.3% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.4% ... Training loss: 0.058 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.4% ... Training loss: 0.059 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.4% ... Training loss: 0.058 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.4% ... Training loss: 0.059 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.4% ... Training loss: 0.063 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.5% ... Training loss: 0.061 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.5% ... Training loss: 0.063 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.5% ... Training loss: 0.069 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.5% ... Training loss: 0.059 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.5% ... Training loss: 0.061 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.5% ... Training loss: 0.060 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.5% ... Training loss: 0.062 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.6% ... Training loss: 0.059 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.6% ... Training loss: 0.059 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.6% ... Training loss: 0.064 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.6% ... Training loss: 0.063 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.6% ... Training loss: 0.060 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.7% ... Training loss: 0.058 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.7% ... Training loss: 0.060 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.7% ... Training loss: 0.062 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.7% ... Training loss: 0.065 ... Validation loss: 0.178(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.7% ... Training loss: 0.067 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.7% ... Training loss: 0.062 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.8% ... Training loss: 0.061 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.8% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.8% ... Training loss: 0.060 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.8% ... Training loss: 0.058 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.8% ... Training loss: 0.058 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.8% ... Training loss: 0.059 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.8% ... Training loss: 0.066 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.9% ... Training loss: 0.060 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.9% ... Training loss: 0.067 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.9% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.9% ... Training loss: 0.058 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 96.9% ... Training loss: 0.058 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.0% ... Training loss: 0.060 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.0% ... Training loss: 0.058 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.0% ... Training loss: 0.059 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.0% ... Training loss: 0.067 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.0% ... Training loss: 0.070 ... Validation loss: 0.175(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.0% ... Training loss: 0.080 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.0% ... Training loss: 0.110 ... Validation loss: 0.216(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.1% ... Training loss: 0.086 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.1% ... Training loss: 0.064 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.1% ... Training loss: 0.074 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.1% ... Training loss: 0.062 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.1% ... Training loss: 0.059 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.2% ... Training loss: 0.058 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.2% ... Training loss: 0.068 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.2% ... Training loss: 0.062 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.2% ... Training loss: 0.058 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.2% ... Training loss: 0.059 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.2% ... Training loss: 0.059 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.2% ... Training loss: 0.063 ... Validation loss: 0.168(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.3% ... Training loss: 0.062 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.3% ... Training loss: 0.060 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.3% ... Training loss: 0.058 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.3% ... Training loss: 0.059 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.3% ... Training loss: 0.059 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.3% ... Training loss: 0.058 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.4% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.4% ... Training loss: 0.070 ... Validation loss: 0.181(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.4% ... Training loss: 0.071 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.4% ... Training loss: 0.060 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.4% ... Training loss: 0.061 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.5% ... Training loss: 0.058 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.5% ... Training loss: 0.058 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.5% ... Training loss: 0.061 ... Validation loss: 0.172(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.5% ... Training loss: 0.072 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.5% ... Training loss: 0.061 ... Validation loss: 0.180(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.5% ... Training loss: 0.058 ... Validation loss: 0.167(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.5% ... Training loss: 0.068 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.6% ... Training loss: 0.065 ... Validation loss: 0.197(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.6% ... Training loss: 0.059 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.6% ... Training loss: 0.058 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.6% ... Training loss: 0.058 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.6% ... Training loss: 0.058 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.7% ... Training loss: 0.060 ... Validation loss: 0.179(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.7% ... Training loss: 0.058 ... Validation loss: 0.164(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.7% ... Training loss: 0.058 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.7% ... Training loss: 0.060 ... Validation loss: 0.176(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.7% ... Training loss: 0.064 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.7% ... Training loss: 0.068 ... Validation loss: 0.192(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 97.8% ... Training loss: 0.067 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.8% ... Training loss: 0.058 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.8% ... Training loss: 0.059 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.8% ... Training loss: 0.058 ... Validation loss: 0.158(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.8% ... Training loss: 0.058 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.8% ... Training loss: 0.058 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.8% ... Training loss: 0.058 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.9% ... Training loss: 0.058 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.9% ... Training loss: 0.063 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.9% ... Training loss: 0.059 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.9% ... Training loss: 0.058 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 97.9% ... Training loss: 0.060 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.0% ... Training loss: 0.059 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.0% ... Training loss: 0.058 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.0% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.0% ... Training loss: 0.070 ... Validation loss: 0.190(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.0% ... Training loss: 0.061 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.0% ... Training loss: 0.058 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.0% ... Training loss: 0.058 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.1% ... Training loss: 0.058 ... Validation loss: 0.155(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.1% ... Training loss: 0.068 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.1% ... Training loss: 0.059 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.1% ... Training loss: 0.060 ... Validation loss: 0.166(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.1% ... Training loss: 0.060 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.2% ... Training loss: 0.060 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.2% ... Training loss: 0.063 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.2% ... Training loss: 0.059 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.2% ... Training loss: 0.062 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.2% ... Training loss: 0.064 ... Validation loss: 0.186(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.2% ... Training loss: 0.066 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.2% ... Training loss: 0.062 ... Validation loss: 0.170(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.3% ... Training loss: 0.060 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.3% ... Training loss: 0.059 ... Validation loss: 0.165(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.3% ... Training loss: 0.059 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.3% ... Training loss: 0.068 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.3% ... Training loss: 0.065 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.3% ... Training loss: 0.060 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.4% ... Training loss: 0.058 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.4% ... Training loss: 0.058 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.4% ... Training loss: 0.067 ... Validation loss: 0.169(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.4% ... Training loss: 0.063 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.4% ... Training loss: 0.058 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.5% ... Training loss: 0.058 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.5% ... Training loss: 0.061 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.5% ... Training loss: 0.059 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.5% ... Training loss: 0.058 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.5% ... Training loss: 0.062 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.5% ... Training loss: 0.063 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.5% ... Training loss: 0.067 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.6% ... Training loss: 0.058 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.6% ... Training loss: 0.069 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.6% ... Training loss: 0.060 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.6% ... Training loss: 0.062 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.6% ... Training loss: 0.060 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.7% ... Training loss: 0.061 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.7% ... Training loss: 0.059 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.7% ... Training loss: 0.060 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.7% ... Training loss: 0.058 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.7% ... Training loss: 0.072 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.7% ... Training loss: 0.060 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.8% ... Training loss: 0.061 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.8% ... Training loss: 0.059 ... Validation loss: 0.154(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.8% ... Training loss: 0.058 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.8% ... Training loss: 0.058 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.8% ... Training loss: 0.058 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.8% ... Training loss: 0.061 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.8% ... Training loss: 0.060 ... Validation loss: 0.160(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.9% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.9% ... Training loss: 0.059 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.9% ... Training loss: 0.060 ... Validation loss: 0.143(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.9% ... Training loss: 0.059 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 98.9% ... Training loss: 0.058 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.0% ... Training loss: 0.058 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.0% ... Training loss: 0.058 ... Validation loss: 0.145(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.0% ... Training loss: 0.060 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.0% ... Training loss: 0.061 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.0% ... Training loss: 0.062 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.0% ... Training loss: 0.059 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.0% ... Training loss: 0.061 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.1% ... Training loss: 0.058 ... Validation loss: 0.148(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.1% ... Training loss: 0.058 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.1% ... Training loss: 0.059 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.1% ... Training loss: 0.059 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.1% ... Training loss: 0.058 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.2% ... Training loss: 0.059 ... Validation loss: 0.153(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.2% ... Training loss: 0.058 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.2% ... Training loss: 0.061 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.2% ... Training loss: 0.062 ... Validation loss: 0.132(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.2% ... Training loss: 0.058 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.2% ... Training loss: 0.059 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.2% ... Training loss: 0.062 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.3% ... Training loss: 0.058 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.3% ... Training loss: 0.063 ... Validation loss: 0.133(15435, 1)\n",
      "(1440, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 99.3% ... Training loss: 0.059 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.3% ... Training loss: 0.061 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.3% ... Training loss: 0.059 ... Validation loss: 0.134(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.3% ... Training loss: 0.059 ... Validation loss: 0.135(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.4% ... Training loss: 0.058 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.4% ... Training loss: 0.066 ... Validation loss: 0.131(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.4% ... Training loss: 0.067 ... Validation loss: 0.174(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.4% ... Training loss: 0.062 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.4% ... Training loss: 0.058 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.5% ... Training loss: 0.062 ... Validation loss: 0.139(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.5% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.5% ... Training loss: 0.058 ... Validation loss: 0.146(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.5% ... Training loss: 0.060 ... Validation loss: 0.140(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.5% ... Training loss: 0.058 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.5% ... Training loss: 0.058 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.5% ... Training loss: 0.061 ... Validation loss: 0.156(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.6% ... Training loss: 0.058 ... Validation loss: 0.142(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.6% ... Training loss: 0.060 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.6% ... Training loss: 0.059 ... Validation loss: 0.152(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.6% ... Training loss: 0.058 ... Validation loss: 0.149(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.6% ... Training loss: 0.065 ... Validation loss: 0.173(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.7% ... Training loss: 0.064 ... Validation loss: 0.136(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.7% ... Training loss: 0.059 ... Validation loss: 0.147(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.7% ... Training loss: 0.058 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.7% ... Training loss: 0.059 ... Validation loss: 0.151(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.7% ... Training loss: 0.061 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.7% ... Training loss: 0.059 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.8% ... Training loss: 0.063 ... Validation loss: 0.138(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.8% ... Training loss: 0.061 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.8% ... Training loss: 0.070 ... Validation loss: 0.133(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.8% ... Training loss: 0.064 ... Validation loss: 0.171(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.8% ... Training loss: 0.063 ... Validation loss: 0.137(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.8% ... Training loss: 0.059 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.8% ... Training loss: 0.059 ... Validation loss: 0.162(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.9% ... Training loss: 0.059 ... Validation loss: 0.150(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.9% ... Training loss: 0.058 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.9% ... Training loss: 0.058 ... Validation loss: 0.159(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.9% ... Training loss: 0.059 ... Validation loss: 0.161(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 99.9% ... Training loss: 0.059 ... Validation loss: 0.141(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 100.0% ... Training loss: 0.058 ... Validation loss: 0.144(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 100.0% ... Training loss: 0.060 ... Validation loss: 0.157(15435, 1)\n",
      "(1440, 1)\n",
      "Progress: 100.0% ... Training loss: 0.060 ... Validation loss: 0.137"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "### Set the hyperparameters here ###\n",
    "iterations = 6000\n",
    "learning_rate = 0.5\n",
    "hidden_nodes = 10\n",
    "output_nodes = 1\n",
    "\n",
    "N_i = train_features.shape[1]\n",
    "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "losses = {'train':[], 'validation':[]}\n",
    "for ii in range(iterations):\n",
    "    # Go through a random batch of 128 records from the training data set\n",
    "    batch = np.random.choice(train_features.index, size=128)\n",
    "    X, y = train_features.ix[batch].values, train_targets.ix[batch]['cnt']\n",
    "                             \n",
    "    network.train(X, y)\n",
    "    \n",
    "    # Printing out the training progress\n",
    "    train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)\n",
    "    val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\\n",
    "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAusAAAH0CAYAAACEkWPuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWQcAgKkAKkmhXjF2GuoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x77bae70>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 373
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查预测结果\n",
    "\n",
    "使用测试数据看看网络对数据建模的效果如何。如果完全错了，请确保网络中的每步都正确实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(504, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python36\\Lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+0AAAIgCAYAAADwRojNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsvXucHFWd/v+c6p6ZJDBJiAESrgHl\nqoBcBLnHAIp7AV0QWODHguL3J7uKLrgravwSxFXXVbmouIhIEBEQkJsSFJQAAYSQIAiBQCABEgi5\nTK4zmZnuqvP9o7tnqk6dmqnTdaq7uut5v155TV+qqk9fqnKe83wuQkoJQgghhBBCCCGEZA+n2QMg\nhBBCCCGEEEKIHop2QgghhBBCCCEko1C0E0IIIYQQQgghGYWinRBCCCGEEEIIySgU7YQQQgghhBBC\nSEahaCeEEEIIIYQQQjIKRTshhBBCCCGEEJJRKNoJIYQQQgghhJCMQtFOCCGEEEIIIYRkFIp2Qggh\nhBBCCCEko1C0E0IIIYQQQgghGYWinRBCCCGEEEIIySgU7YQQQgghhBBCSEahaCeEEEIIIYQQQjIK\nRTshhBBCCCGEEJJRis0eQFYQQiwFMB7AsiYPhRBCCCGEEEKIfaYB2Cil3K3ZAzGBon2Y8WPHjp20\nzz77TGr2QAghhBBCCCGE2OWll17Cli1bmj0MYyjah1m2zz77TFqwYEGzx0EIIYQQQgghxDIHH3ww\nFi5cuKzZ4zCFOe2EEEIIIYQQQkhGoWgnhBBCCCGEEEIyCkU7IYQQQgghhBCSUSjaCSGEEEIIIYSQ\njELRTgghhBBCCCGEZBSKdkIIIYQQQgghJKNQtBNCCCGEEEIIIRmFfdoJIYQQQgghATzPQ09PDzZt\n2oSBgQFIKZs9JEICCCHQ1dWF7u5uTJo0CY7Tvn40RTshhBBCCCFkCM/z8NZbb6Gvr6/ZQyEkEikl\n+vv70d/fj97eXuy8885tK9wp2gkhhBBCCCFD9PT0oK+vD8ViEVOmTMFWW23VtmKItC6e56G3txcr\nV65EX18fenp6MHny5GYPKxV49hFCCCGEEEKG2LRpEwBgypQp6O7upmAnmcRxHHR3d2PKlCkAhn+3\n7QjPQEIIIYQQQsgQAwMDAICtttqqySMhZHRqv9Pa77YdoWgnhBBCCCGEDFErOkeHnbQCQggAaOti\niTwTCSGEEEIIIYS0JDXR3s5QtBNCCCGEEEIIIRmFop0QQgghhBBCCMkoFO15pa8HePZmYMPyZo+E\nEEIIIYQQQkgEFO155a7PAff8KzD7HwDPa/ZoCCGEEEIIIQYsW7YMQgice+65gcfPPfdcCCGwbNmy\nVF537ty5EEJg1qxZqRyfhKFozyvLn678XbcU6Fvb3LEQQgghhBCSQYQQgX+FQgGTJ0/GjBkzcPPN\nNzd7eKkQtRhAmkex2QMgTUJ6+tuEEEIIIYSQAJdeeikAoFQqYfHixbj77rvx8MMPY8GCBfjhD3/Y\n5NEF+c53voNLLrkEO+64YyrHP/TQQ/HSSy9h8uTJqRyfhKFozyseRTshhBBCCCFxUEPB//SnP+GE\nE07AlVdeiQsvvBDTpk1ryrh0TJ06FVOnTk3t+OPGjcPee++d2vFJGIbH5xU67YQQQgghhNTFcccd\nh7333htSSsyfPx9AMKz8lVdewemnn47tttsOjuNg7ty5Q/v29PTgq1/9KvbZZx+MHTsWEyZMwHHH\nHYc//vGP2tfatGkTLrroIuy0004YM2YM9t57b/zwhz+EF1GXaqSc9qeffhqnn346dtxxR3R1dWHq\n1Kn46Ec/it/85jcAKosTu+22GwDgxhtvDKQGzJ49G8DIOe2vvvoqzjnnHOy4447o7OzEDjvsgHPO\nOQevvvpqaNtZs2ZBCIG5c+fijjvuwKGHHopx48Zh0qRJOOOMM7BixYqojz930GnPKxTthBBCCCGE\n1I2UEkAl793Pa6+9hsMOOwx77rknzjrrLGzZsgXjx48HALzxxhuYPn06li1bhqOPPhonnngient7\n8bvf/Q4nnngirr32Wnz2s58dOtbAwACOO+44zJ8/HwcccADOOussrF+/HpdffjkeeeQRo/Fed911\nuOCCC1AoFHDSSSdhjz32wKpVq/DMM8/gmmuuwWmnnYbp06dj/fr1uOqqq3DAAQfgE5/4xND+H/zg\nB0c8/vz583H88cdj06ZNOOmkk7Dvvvvi5Zdfxs0334x77rkHf/rTn3DIIYeE9rvmmmtw77334qST\nTsKxxx6Lp556Crfddhuee+45/PWvf0VXV5fR+2xHKNrzinR9tynaCSGEEEJIPKZd8vtmDyE2y777\n96kc96GHHsLixYshhMCHPvShwHPz5s3DV7/6VXz7298O7fcv//IveOONN3DLLbfgjDPOGHp8/fr1\nmD59Oi688EKcdNJJ2H777QEAP/jBDzB//nz80z/9E26//XY4TiVQ+pJLLsHBBx8ce7yLFi3Cv/7r\nv2L8+PF47LHH8P73vz/w/PLllTbQ06dPx7Rp03DVVVfhgx/8YOwK8VJKnHPOOdi4cSN+9atf4ayz\nzhp67rbbbsMZZ5yBs88+G4sWLRp6DzUeeOABzJ8/H/vtt9/QY2eeeSZuueUW3HPPPTjttNNiv892\nheHxeYVOOyGEEEIIIbGYNWsWZs2aha9//es49dRTceKJJ0JKiS996UvYddddA9tuv/32Q4Xr/Dz3\n3HN45JFHcMoppwQEOwBMnDgRl112Gfr7+3HnnXcOPX7DDTfAcRx873vfC4jd3XbbDRdeeGHs8f/0\npz9FuVzGN77xjZBgB4Cddtop9rF0PPHEE3j55Zdx+OGHBwQ7AJx++uk46qijsHjxYsybNy+074UX\nXhgQ7ACGog2efvrpRONqFxI77UKIcwHcMMpmnpSyoOx3BICZAD4MYAyAJQB+AeBHUvpt4MA+/wDg\nywAOBFAA8CKAa6SUNyZ5D7nEo9NOCCGEEEJIHC677DIAlVD4iRMn4uijj8ZnPvMZnH322aFtDzjg\nAG1I95NPPgkA2LBhg9bBXr16NQDgpZdeAlDJZV+yZAl23nlnvPe97w1tP3369KFxjcZf/vIXAMDH\nP/7xWNubsnDhQgDAjBkztM/PmDED8+bNw7PPPotjjjkm8JwuZH7nnXcGAKxbt87ySFsTG+HxfwUQ\n9Ws5GsAMAHP8DwohTgZwJ4B+ALcB6AHwjwCuAHAkgE+pBxJCfB7AjwCsBfArAIMATgUwWwixn5Ty\nyxbeSz6QEoBU7hNCCCGEEDI6aYWcZxlpMF+eMmWK9vG1a9cCAB588EE8+OCDkftv3rwZQEXcAxgK\nlY/7OjrWr18PAKm1gauNNapqfe3x2jj8TJw4MfRYsViRqa6r9XJzR2LRLqX8KyrCPYQQ4snqzZ/5\nHhsP4DoALoDpUspnqo9/A8CfAZwqhDhDSnmrb59pAL6Pirg/REq5rPr4NwHMB3CxEOJOKWXt9chI\nqBcdOu2EEEIIIYRYQS1MV2PChAkAgKuuuipWaHtt+3fffVf7/MqVK2OPqSaMV6xYkUq7ttpYo8b0\nzjvvBLYjZqSW0y6E+AAqoe8rAPirVZwKYFsAt9YEOwBIKftRCZcHgAuUw30aQBeAH9cEe3WfdQBq\nFR4+Z3P8bY2afaDPRiCEEEIIIYRY4sMf/jAA4LHHHou1fXd3N973vvdhxYoVeO2110LP+9vIxX3t\nOXPmjLIlUChUsppNXO4DDzxwxDHVHj/ooINiH5MMk2Yhuv+/+vd6JUe9lujwgGafRwH0AThCCOFP\nBBlpnznKNmQ0VGedTjshhBBCCCGpcsghh+Doo4/Gb3/7W/ziF7/QbvO3v/0Nq1atGrp/3nnnwfM8\nfOUrXwn0ZV+6dCmuvvrq2K99wQUXoFgs4vLLL8eiRYtCz9eqxwPANttsAyEE3nzzzdjHP/LII7HX\nXnth3rx5uOOOOwLP3XHHHXj00Uex55574qijjop9TDJMKi3fhBBjAZwNwAPwc+Xpvap/X1H3k1KW\nhRBLAbwfwO4AXoqxzztCiF4AOwkhxkkp+0YZ24KIp+zHiWQVinZCCCGEEEIazq9//WvMmDEDn/nM\nZ3D11VfjsMMOw8SJE7F8+XI8//zzeOGFF/Dkk09iu+22AwBcfPHFuPvuu3HnnXfioIMOwsc+9jFs\n2LABt912G4455hjce++9sV533333xTXXXIPPfe5zOPDAA3HyySdjjz32wNq1a/HMM8+gu7sbDz/8\nMABg6623xmGHHYbHHnsMZ511Fvbcc8+h3u7777+/9vhCCNx444044YQTcPrpp+Pkk0/G3nvvjcWL\nF+Puu+9Gd3c3fvnLX4bavZF4pNWn/TQAEwH8Xkr5lvJcLZFhQ8S+tcf9FQni7LNVdbsRRTtBsHI8\nQNFOCCGEEEJIA9hpp52wYMEC/OhHP8Kdd96Jm2++Ga7rYsqUKdh3333xhS98IdD+rKurCw899BBm\nzZqF2267DVdddRWmTZuGmTNn4pOf/GRs0Q5U2qh94AMfwPe//33MnTsXd999NyZPnoz9998f559/\nfmDbm266Cf/+7/+OBx54ALfccguklNhpp50iRTsAHHbYYZg/fz6+9a1v4aGHHsJ9992HyZMn45//\n+Z/xjW98A3vttVfkvmRkhEklxNgHFeJxAEcAOElKeZ/y3CsA9gCwh5RyiWbfJwAcDuBwKeVfqo8N\nAugA0CGlLGv2eRvAVABTpZTxKzIEj7HgoIMOOmjBgigjvo3o3wh8d+fh+/9nLrDDgc0aDSGEEEII\nyRC1lmP77LNPk0dCSDzi/mYPPvhgLFy4cKGU8uBGjMsW1uMThBD7oiLYlwO4X7NJzS2PKh04XtnO\nZJ+NMYeZb0KF6Oi0E0IIIYQQQkgWSSOpIKoAXY3F1b97qk8IIYoAdgNQBvB6zH2mohIav3y0fHZS\nJdTyjX3aCSGEEEIIISSLWBXtQogxAP4/VArQXR+x2Z+rf0/UPHcMgHEAnpBSDsTc5+PKNmQ0WqkQ\nnZRcVCCEEEIIIYTkFttO+6cAbAPgfk0Buhp3AFgD4AwhxCG1B6uC/1vVuz9V9rkBwACAzwshpvn2\n2QbA16p3/zfp4HNDqxSi2/gO8NMjgJ8cBqyP33KCEEIIIYQQQtoF26L9/1T//ixqAynlRgCfBVAA\nMFcI8XMhxPcA/BWVAnR3ALhN2WcpgP8AMAnAM0KInwghrgDwPID3AviBlPJJy++lfWkVp/2FO4FV\ni4A1i4Hnf9Ps0RBCCCGEEEJIw7HW8k0IsQ+AoxBdgG4IKeXdQohjAXwdwCkAxgBYAuAiAFdLTUl7\nKeWPhBDLAHwZwDmoLDgsAjBTSnmjrfeRC1SRrjrvWWFws+92b/PGQQghhBBCCCFNwppol1K+BEAY\nbP84gL8zfI37ANw36oZkZFqlerx/MSGrYySEEEIIIYSQFEmjejzJOq0SHi9bSLT39TR7BIQQQggh\nhJA2hKI9j7RKITr/uLI6RgC483zge7sBf5zZ7JEQQmzjucD8nwNP/AgobWn2aAghhBCSQyja80ir\n9GkPhMdndIyDvcDfbq/cXvjL5o6FEGKfV/8I/P7iyqLc87eNvj0hhBBCiGUo2vNIy4THt4DT7g76\nbpeaNw5CSDqsXjx8e82rzRsHIYQQQnILRXseaZVCdK0g2j3fuLJahZ8QUj+tVFuDEEIIIW0JRXse\nodNuD07oCWlvuDBHCCGEkCZD0Z5HWkW0t0LLt1YYIyGkflph8ZAQQgghbQ1Fex5pmerxLSCIAxN6\nunCEtB2tcB0ihBCSGkKIwL+uri5su+22OOigg3D++edjzpw5cF07c8DZs2dDCIHZs2dbOR5pH4rN\nHgBpAiGnPaNisxUcrlB9AAkI0ZyxEELs0wrXIUIIIalz6aWXAgBc18X69evx4osv4qabbsL111+P\nQw45BDfffDP23HPPJo+StCsU7XmE4fH20EUtiEJzxkIIsU/gOpTRBU5CCCGpM2vWrNBj7777Lr7w\nhS/g9ttvx/HHH49nnnkG2223XeMHR9oehsfnkZBoz2gPdP+4MjtG5bNkoSpC4vP8b4Af7gs8+H+b\nPZJoEobHr9rUjxseX4olqzZbHBQhhJAssP322+PWW2/F9OnT8dZbb+Hb3/524PkFCxbgi1/8Ig44\n4ABMmjQJY8aMwR577IGLL74Y69atC2w7ffp0nHfeeQCA8847LxCSv2zZMgDA22+/jW9+85s48sgj\nMWXKFHR2dmKHHXbAmWeeiZdeeqkh75k0BzrteaRVnPZWyCVtlc+SkCzy6P8AG1cAj18NHPklYNyk\nZo8oTCA83nzx8KLbnsO8JWswdcIYPPIfH0FnkWvlhBDSTjiOg5kzZ2Lu3Lm45ZZbcMUVV0BUUyWv\nu+463HXXXTj22GNx/PHHw3VdLFy4ED/84Q8xZ84cPPXUU+ju7gYAnHvuuZg4cSLuuecenHzyyfjg\nBz849BoTJ04EADz66KP47ne/i4985CM45ZRTsPXWW+PVV1/FHXfcgXvvvRePP/44DjjggMZ/CCR1\nKNrzSMsUomuBXNJW+SwJySL9G6o3JDC4OZuiPWHLt3lL1gAA3tnQj7+t2ICDd93G1sgIIaR5zJrQ\n7BHEZ9aG0bdJyFFHHYVisYhVq1Zh2bJl2G233QAAX/3qV/GTn/wEhUIwdfL666/H+eefj2uuuQZf\n+cpXAFREOwDcc889+MQnPjF038+MGTPw7rvvDgn9Gs899xyOPPJIXHLJJZgzZ479N0iaDpf880ir\nuMOtkNMeKkTH8HhCYiOTCeKGYHHxcO3mgYSDIYQQkkW6urrwnve8BwCwevXqocd33XXXkGAHgE9/\n+tMYP348/vCHPxi9znbbbRcS7ABwwAEHYMaMGXj44YdRKpUMR09aAYr2PBISmi0giLM6RjrthNRP\nwoW53oEy3t3Yb3FAGixeh9b2DiYcDCGEkKwiqylUwtdFqFQq4cc//jGOOuooTJo0CYVCAUIIOI6D\njRs3YsWKFcav8/vf/x7/+I//iKlTp6Kjo2Mo7/2+++7DwMAA1qxZY+09kezA8Pg80ipOeyuEx7fK\nAgghWSRBvnhP7yBm/GAu1veV8NOzDsLH95tqeXBVLEb80GknhLQNDQg5byX6+/vR09MDANh2222H\nHj/99NNx1113Yffdd8fJJ5+MKVOmoKurCwBw5ZVXYmDA7P+Fq6++Gl/84hexzTbb4IQTTsAuu+yC\ncePGQQiBu+++G88995zxMUlrQNGeR1pFtLdEeHyLVOInJIsERLtZePzsx5difV8lBPCCmxdi2Xf/\n3ubIhkkwRpU1m+m0E0JIOzJv3jyUy2Vsv/32mDZtGgDgmWeewV133YXjjz8e999/Pzo6Ooa29zwP\n3/ve94xeo1wu49JLL8WUKVOwcOFCTJ0aXKx+8sknE78Pkl0YHp9HvBYR7YGWbxkdo/pZZjUvl5As\nkmBhbmN/2fJgIrAYHr+GTjshhLQdnufhv/7rvwAAZ5555tDjS5YsAQCcdNJJAcEOAE8//TS2bNkS\nOlYt/911w/PJNWvWYP369TjiiCNCgn3z5s1YuHBhsjdCMg1Fex5pld7irZDTzvB4QuonQSG6nbYZ\nGzxUWlEuCVu++elhTjshhLQVq1atwhlnnIG5c+dil112wde+9rWh52qO+9y5c0P7/Nu//Zv2eLVi\ndm+++Wboue222w7jxo3DggULsHnz5qHHS6USvvjFLzKXvc1heHweaZWQbouT5dRgITpC6ifBwlz3\nmOB/X6s2DWD78WNsjCqIV39tDXUhgU47IYS0LrNmzQJQcdbXr1+PF198EfPmzcPg4CAOPfRQ3Hzz\nzZg8efLQ9h/60Idw5JFH4re//S2OOOIIHHXUUXj33XcxZ84c7LXXXthhhx1Cr3H44Ydj3LhxuPLK\nK9HT04Ptt98eAPCFL3wBEyZMwIUXXojvfve72G+//XDyySdjcHAQDz/8MHp6evCRj3wEDz/8cEM+\nC9J4KNrzSKu4wy2R086Wb4TUTeAcNzt3XOWS8GZPXzqi3T8uw2gAda1x9SaKdkIIaVUuu+wyAEBn\nZye6u7ux66674pxzzsEpp5yCj370o3CcYABzoVDAvffei5kzZ+L+++/H1VdfjR133BHnn38+Zs6c\niX333Tf0Gttssw3uvPNOXHbZZbjhhhvQ29sLADj77LMxYcIEXH755dh2223x85//HNdeey0mTJiA\nE044Ad/61rdw6aWXpv8hkKZB0Z5HWqUQXUtUj0/2WQ6WPfzqL2+g4Aic/eFdUXDE6DsR0i4kOMdd\nRRG/ubYPH5o2ycaoglgc47q+EgbLHjqLzEwjhJBWIUn61aRJk3DNNddon1u2bJn28RNPPBEnnnii\n9rlisYiLLroIF110Uei52bNnY/bs2fUOlWQcivY80ioh3a2Q057ws/zd82/jm79bBADoKDg487Bd\nbI2MkGwjJQDfREgt6jgKnhecRL3R02dhULoXqv865Hrhid7qzQPYceJYzdaEEEIIIXq43J9HWsVp\nT5BL2jASFvX7xt0vDN3+2l1/szEiQlqDhAteqiB+Ky3RnsBp9zTuzKqN/UlHRAghhJCcQdGeR9SJ\nZCsI4oyOUXpK2ynDEKrJ3V0WRzMCa14FfvYR4NdnACWKBpIBEi4eqoL4zdREe5K8e41oZ147IYQQ\nQgyhaM8jrVKIrgXC4zf0KRNww3FOnZBC4Swdz/wCeHsh8Moc4NU/NOY1CRmJhEUcVUGcmmgPRPyY\nLcppNDtFOyGEEEKMoWjPI4qw9AxzSRvFYHnYxe4fLDVxJNF4bjLhMXVCg3pNb1mnv01Is0gaHq+c\nK6s3DaBvsByxdQKShMfrctpTCo9/q6cPNz6xDO9s2JLK8QkhhBDSPCja84gy8VzXm81w6eVrNw/d\nXrp6UxNHEo1MKDwmjO0I3N+wJaXFiVZon0fyRcJ6EDpB/FZPCoI1Qcs3dWEBSMdpl1LivNnzcem9\nL+JzNy2wfnxCCCEky6RmemUIivY8okw8RUZFXG//4NDtTb7bWSKpaFfzcpevS8kl84/LUHgQkgoJ\n03TUPu0A8Mba3gQD0uP5zhfXwsJCGqK9b9DFklWVRc7nlm+wfnxCSP4QotKCNqvRmIT4qYn22u+2\nHaFozyPK5FggmxdkxzcuB9lcQROq8DB14rxGiXY67SRjqBPBhOHxALCuz/7i3sr1wwsB724wy5vX\n5bSvTkG066rUE0JIErq6KoVye3vtL4YSYpva77T2u21HKNrziFSd9mxO+AoB0Z5NoSkV4SGNnfbg\n/eXrGtC2ik47yQLKudJfMksN0bnYZZ1KTog/TWfVRrNFNd3CwkDZ/vmnvu08hAkSQtKlu7sbALBy\n5Ups2rQJnufx2kIyhZQSnudh06ZNWLlyJYDh3207Umz2AEgTUC66WXXahc9dz6rT7ikt31zXNTqp\nVOGxYn1KTrtXf9sqAED/BuDNp4DdjgE6GlTxnrQ3yu9w+dpevM9gd50gLrv2rxP+61DB8FqpXVhI\nYYxqxI7rSRQL7RsiSAhJn0mTJqG3txd9fX1Yvnx5s4dDyKiMGzcOkyZNavYwUoOiPYd4nhsIsVDd\n4qzgnyCLjIp2qVSPN/0sVeGRXni873VMw+OlBK7/GLD6JWDfTwCn3Wh3bCSfKBEfDpLni6fhtCdJ\n09H1aU9jjCHRLiX/cyeEJMJxHOy8887o6enBpk2bMDAwQKedZA4hBLq6utDd3Y1JkybBcdo3iJz/\nr+cQzy0H8yIymuMcdNqzOUZPER6q8z76/orT3oicdtPw+N7VFcEOAEsfsTcmkm9CtTVMe6DrXGz7\n1wmRQLQ3aoyqaOe8mhBiA8dxMHnyZEyePLnZQyEk97TvcgSJRK0EKusJl24AhRYoROe5QZFuWmU1\n7LQ3IKfd9Pv2L0S4KfTBJvkkVFsjefX4NFzsJBE/WtGewhjLynVH5/ATQgghpHWhaM8hqjssDSd4\nUkr86i9v4IoHX8HmgfREXCtUjw8tgJi2hFLe1sb+Mjb2p9Cr3T8u03QIv2j3UuojT/KHGh6fsF0i\nkE6+uH9cpjntjVpYUEU6q8kTQggh7QXD43OIp+ZhGzqvTy/twcy7XwAArO8bxGUnf8Da2Pw4IvuF\n6KQSDq9+tqOhy8tdsW4Lxk/tSDSuENacdop2YomEBTF1brKbQn2OJGk6ujGWUgiPVxcCMlqmhBBC\nCCF1Qqc9h4TdYbMZ3i+ffGPo9o2+27YJhMeLbM5C1UJ0xuHxmkn9llIK6QpJ+rSrlefp4hEbqItH\nFvq0l1IuRGcjPD6N0HVdITpCCCGEtA8U7TlEdYdNJ8s7bjPW4miiaYXweDUc3jRqoVFtqwJC27QQ\nnfp7odtOLKDWgxCG546+nZr9xb0k16FGhfCrx2R4PCGEENJeWBXtQoijhRB3CiHeEUIMVP/+UQjx\nd5ptjxBC3C+E6BFC9AkhnhdCfEkIURjh+P8ghJgrhNgghNgshHhKCPEvNt9DHvDUia2hiNthQrBP\ndyo52GgN0R6KWrAQHq8WlbJCkj7tqmg3rJBPiA43lKaTPEol7ZZv5jntumiA9KvH664rhBBCCGld\nrIl2IcRMAI8COAbAAwB+AOA+ANsAmK5se7Jv27sA/ARAJ4ArANwacfzPV4/3AQC/AnAdgB0AzBZC\nfN/W+8gDYXfYbBIphAjcf23V5sRj0uEX6pnt0y7Von7Ji2mlUvnZ/x0ndNrPvHYelqT0nRN7rN08\ngBdWbGj2MCJRRXvSzgtAOi52oHq8SO60S2lfVIeqx9NpJ4QQQtoKK4XohBCfAnA5gIcA/JOUcpPy\nfIfv9nhUBLcLYLqU8pnq498A8GcApwohzpBS3urbZxqA7wPoAXCIlHJZ9fFvApgP4GIhxJ1Syidt\nvJ92J1Q9PqHD9drqXhy4yzaJx6XSCi3fQjntxnm54cfSCY+vP6d9c98Atvbdf2nFWtz4xDJc/ol0\nChCS5PT0DuKo/34YW0ouvv3J/XDmYbs0e0ghXDXNwjCCQx+lYv/cSVKILmo4Jc9DlxMZVGZMuHq8\ntUMTQgghJAMkdtqFEA6A/wbQB+BMVbADgJTSPzs7FcC2AG6tCfbqNv0AZlbvXqAc4tMAugD8uCbY\nq/usA/Dt6t3PJXsn+SHkBtcHBhIJAAAgAElEQVTlDsuhas+vrU7faTedLDcKtT6ANOxj3ijhEawe\nb/ZZDpYGA/eLcNHTNxixNckCVz30ylBBw6/d9bcmj0ZPuCCm2e9ev+CVbk67jfB4wP7CXLh6PFU7\nIYQQ0k7YcNqPALAbgDsArBNC/D0qIez9AJ7WuN8zqn8f0BzrUVTE/xFCiC4p5UCMfeYo25BR8FSh\naSjinMHNuLdzJiaLDfjs4MVYsmp7m8Mbfp1WcNo9+9Xj02hbFezTblgsT3FEO+DCTSMagFhj/Zbs\nFwt0lQWuUIHMUdAJ0zRSSxzpAdWMIONCdFGi3fI42aedEEIIaW9siPYPVf++C2AhgP38TwohHgVw\nqpRydfWhvap/X1EPJKUsCyGWAng/gN0BvBRjn3eEEL0AdhJCjJNS9o00WCHEgoin9h5pv3Yi5LQb\nivb935iN/Z2lAICbOr+DU1ffbGtoAYJOezYnoepnqYr40dDm5abutJuNUY0eKAo3nWJ5xBqtYLR6\naiE6CwteqbR88+Wxm9bWiMottx0RUPYkPl2Yg3MLD+Bn7j/A9Y61enxCCCGENBcbhei2q/79HICx\nAI4H0I2K2/4HVIrN3e7bfkL1b1SFpNrjE+vYZ0LE88RH0j7t2216cej2JLEZb6ztw2DZvogLFoDK\nqEgMFaIzFMQNa/lWfyE6rxwU7R0op7OwQKyh+11lDbUQnY0+7WlEqQRra1gKj7d8/njlQXy5+Bvs\n4qzGxcXb6bQTQgghbYYNp71WTUeg4qg/V73/ohDik6i448cKIQ6PWSiuVprcZNYRex8p5cHaA1Qc\n+IMMXrN1UUWbaY6zE+zT7noSb/b04n3bdScdWYBWDI/PatuqJDntnhIeX4SbzsICsUYrfDthpz35\nglcphd9lkpz2KO1sPTy+XMI4Uckmm4BerG6FHwAhhBBCYmPDaV9X/fu6T7ADAKSUW1Bx2wHg0Orf\n0Vzx8cp2JvtsHHW0BJ4aHp1QtAOVCvK2SSrapZRYvHITSikUpxp6jaQ57Zq3lXpOuw3RzvD4bCOB\nIsqYgOy25lN/V1YWvFIuRGd6HYouRGd3nJ4vhcUREq7h8ftLLr4z5yV863eL0F8ybAlJCCGEkNSx\nIdoXV/+uj3i+JuprSq+2/Z7qhkKIIipF7coAXte8hm6fqQC2ArB8tHx2UkUNj7cg2nsHzIpIjYqU\nKPhySesR7V++/Xl87MpHcc71T6cWLpw0p11XqCoNtzBReLyrhse76fSSJ9bocPswt+sizO+6AB91\n5jd7OFpCC1zG9SDCj6URpeK/9tjKabd9jrtKCotabHQ07np2Ba595HX8fN5SXPFgqHQMIYQQQpqM\nDdH+KCoiew8hRKfm+Voz52XVv3+u/j1Rs+0xAMYBeMJXOX60fT6ubENGQRWWwnCyPKAR7dYny8pk\nV9TR8u3OhcsBAE++vhZLVqXkOKqV+K1Uj0+7T3uyQnQFuOksLBBr7Nm3EDuJNegULk4uPN7s4WhR\nc9pNzx1tu8QUfpeFBE57VPV42+e4p6bpqPUCRuHqP706dPvaR18fYUtCCCGENIPEol1KuQbAbaiE\nrv9f/3NCiBMAfAyV8PZau7Y7AKwBcIYQ4hDftmMAfKt696fKy9wAYADA54UQ03z7bAPga9W7/5v0\nveSFpNXjB50xocesT5YVYZk0p33ROyllTkg1asHQxW5G9XjjaADFaRd02rNOUQ4O3e6E5SgYS6iL\nQabnTqMWvJLktEcNx3bKjhoNY5pis9M24YVYQgghhGQHG047AFwEYAmArwshHhVCfF8IcTsqPdRd\nAJ+VUq4HACnlRgCfRaWA3VwhxM+FEN8D8FcAh6Mi6m/zH1xKuRTAfwCYBOAZIcRPhBBXAHgewHsB\n/CBmkTuC8OTYNDy+PFR7sEIXBu3nOCtjSiraX165KdH+kchkOe060Z5OTnv9hehCLd/gplongCRH\nyPqFZqNQf+fGUSq6QnQpnDvB8PjkYwRSqB6v1gcwDI/faZtxNodDCCGEEMvYqB4PKeUqIcRhAGYC\n+CSADwPYBOD3AL4jpfyLsv3dQohjAXwdwCkAxqAi+i8CcLXUJCBLKX8khFgG4MsAzkFlwWERgJlS\nyhttvI+8EMq7NhRxQtm+G1vsh0urIfwJhcfilER76LM0zcttRk67cXi8WoiuTKc94wjfd5zdzguK\nsDSNUmmQ055OeLxtp10NjzcT7dt1dwXu9w6UsVWXlekBIYQQQixg7X9lKWUPKqL7opjbPw7g7wxf\n4z4A95mPjgRQRbqhaFeFabfos1+1OWF4vLruk5ZoDxX1Mw0911aPTzmn3VAwqAKAheiyjyP9QjOj\nTrsqLG047am0fKu/IGbUeWJ7nOoCiGnEz1DT1Cpv9vRhn6nj9dsSQgghpOHYCo8nLUQ4p91sAikU\nQd2NvhQK0SULj1eHs2L9FmzoK+k3TkLCVIPm9Gk3dNo9TXg8W75lGv85mtXweFVYmp47+kJ0Kbd8\nE9LoeqlLfwHs1wBRnXY1x33U/ZXP8o21bMRCCCGEZAmK9jySMDxeFX0Vpz1b4fE6MZxKMbqE4fGN\ny2l39bdjoHXaWT0+04gEId2NQhWapotJOqc97fB4AEbXy0jRbvkcDxX1M07TCd5/s6c36ZAIIYQQ\nYhGK9hwScrQSivatsSWFQnTBya65094g0a5Wj89o26qg026aDqE67WWUGB6faQI57SKbTnsop924\nXWL4MesRIFJW3PXAY/FfI8r4t+60qy3fDAvRqddLOu2EEEJItqBozyOqo2VaiE6ZII4XffZzSaXq\ntCfPJV30dhqiPWF4fMNavtnr015ky7fMEwjpzmp4vKpobRSis34d0nx2Bi5246rHKznthn3a1fP5\nzR6KdkIIISRLULTnkKR92sM57VtSKEQXPJ5pXq5usrxsbQohn+pnlzAstfJYGqLdd0zT8Hi1Tztb\nvmWfFmj5pv6u6lnw2lm8i0PFS0PpANYjQHRjMhinphEKgPTD442vQ3TaCSGEkExD0Z5HlEmn2sJt\nNBpSiE436TSY6OpcuMGyffGiRh0Yh8drK2CnnNOeMDy+wOrxmccJtHyr//f07Jvr8J37X8KSVfa7\nL6h51+q5NBqT3VV4uPNi/Kbrcvxz4WEAKSx46cZkFB7fmEJ0oerxhk67uriwYv0WLswRQgghGYKi\nPY/YdtpFn/UJnjYn06gAVPixVCahak67aQXsBhXTCozTQiG6VEL4iTWCLd/q+66Wr+vDGT/7C659\n9HV8/tfP2hraEEmrx589cBuK1Xz9b3dcDyCFc1wXsl+naD+78CB+0nEl9hZvphAer1SPNy3qp4zH\n9STeXr8l8bgIIYQQYgdrfdpJC5Ewp10V/ZXweLuTUNf1wj/OhA5XGmJYXcAwD49vRsu3pIXo3FRa\naxGLWGj59vW7XsBANTrl5ZXpO+2mhegKMtzC0fo5rg2Pj3+O1xbljnaex7c6bgAAbI1+vOP+vZXh\nDQ0pYRcL3em8ckM/dn3PVglGRQghhBBb0GnPIwmrxzsyKOK6hf3weFfXZzhhq6XUxTAAaaOYVsYK\n0akCoAgXntSPnWSDpIXoHn1lNR55ZbXNIYUIRdMYXof6MDb0WNph55UHTfq0V/7+V/H6oceOLTxv\nPfc+aXh8wyJ+CCGEEFIXFO15JGFOu7p/Gi3fXN2kM6HT3ojweJMJPaAvmJf6OA1dONVp7xCV+wyR\nzy4iYXj87CeWhR6LKqpWL0kLYm6WYdFuu+WbqzsXDa9Du4u3sYsTXADRHjcBqtNuuniou17y9CaE\nEEKyA0V7DgnnjprNzlTnbrzoSyE8Prlo70YfTnIex3ZYN/SYbdTwePM+7eHH7If4ysBnFwqlHQ1N\neDxAJy7LOAnD49duHgg9ZrutY+h3aCg0N2ucdmk5AqRcThjx40mcX/h94LHXvSnWF7zCPe+TVY+P\neowQQgghzYE57TkklIdt3KddCY+H/UJ0XsKqzZ6UuKbjShxdeAGveDvixMH/tt9LHslz2hsSxq+8\nxub+QXSb7K8JjwdqbasKycZGUkEkDI+PKuTYWbS3zpvUad8iOwL3O1BGCUWUPA9djp3fpTZNx7BP\n+0cKzwUeK8Czfy1SnXbduEdAF0XB9BdCCCEkO9BpzyNJW74pIqBbbMlcTrvrSRxdeAEAsKezAnuK\n5dZD+IHwZ2cclqoT7SlXwNZGMYyEpk87YD9/mNgjafV47WJSyk57aAFsFNTr0AT0ArAbAeKWEy4e\nehJjEYxaKAgPruVrkZrDbtzzXhsez/ObEEIIyQoU7TkkJNKTFqJLw2lPGB6vTjh3FSvhpiEyE3yW\nUkptCrx9p11dpDEU7VINj2dOe9YRSBYer/tqB9POwzYUsup1aILYDMBuGH/ygpjhzz+LTrvuq2X6\nCyGEEJIdKNrziCriDJ04VfSPFYOQ5XD7pSToJ8vxx6lOQqeJd60XqQKSLYBETYqtT5aTOppRTnsK\nnyexg/93KYT570kXLm17YS7stJum6TTAaU+YpuNKOZROUqMI1/q5k7QQnc5Vp2YnhBBCsgNFex5J\n6LzqJtdd7uZEQ1JJ6rSr1Zl3Fe+mEs7tQM1pN5vQ60jbaTevYRB8jzXnkOHx2cUfHl+f09748HgY\njlM99yaIimi3mV5S1l6HDPq0ezJUU8CBZ/8cT1qIjuHxhBBCSKahaM8hycPjwxPCzrJd0Z7U4VIL\n2e0mVqLsSettq8I57Sbh8ZW83I8683Gcs2AoRzftnPbE4fFs+ZZ5/PneWQ2PD53PxuHxwd/xRFSu\nQTZ/l562erxJxE+E055yP3njLhZap53nNyGEEJIVKNrzSMLw+JC7DKDT7U00JJXEOe1KeP3uztsA\n7Ieeq6LdRBC7nsRHnQX4WecVuL7zBzjWeS6VMYYjKwzTIZQFkI6hlm8Mj88qfkGrFmyLg76rQcrh\n8dac9gzltHsSBSU9wYFMYWFOXQBJHh7PnHZCCCEkO1C055DELd904fGWnfbE4fGK87S9WI+t0Wfd\nHVaFhonT7kqJazuvGLr/vx1XAkjBwfbqX1iobK/v006nPbs4itNuGmGi27xUtr2YlOw6VFD2r+W0\n26xdob0OmQhiGRb9lZz2dMPjw6kHI8PweEIIISTbULTnEWUyZloAShceP8bLlmjXVU/eTay0Xkwr\nSaqB2ge5s1aV3XaueNIWfxFOO3Pas4t/YaYAz7iomE6wWQ+PT1AgUUqJguK0T6xWj7fpEOty2tXU\nm5HQXcccePbPnaSF6DRfLQNpCCGEkOxA0Z5DVHfYSni8tyXRmFQS57RrclF3F29bD/lUFzBUgTsS\nrifhSTF8rGoYrfWq7Alz2tXts9TyzfMk3l5v97fXFvirx0Ma/+71hegsh8cn7Lyg5uqPr4bH21yY\n04lu7bUpAjXXHKg47dY7WajntKnTrguPp9NOCCGEZAaK9hyStBCdzql13MEkQwqhm+x6BhNdT7P/\n7s471vsjJwmP9yRQ1pyCaee0w7jFX7iQFpBCwTxDpJQ4+/qncMR3/4zvzHmpqWPJGmGn3VC0a75a\n+73F648A0bVSm5hGyzdNxI5n8LtXU0sAoCBkqLtFYpTP0kZ4vO2inYQQQgipH4r2HBIqnmbqtGuc\n2qK03ac9WViq1Oz/XvGOdRc7UXi8lCijGHrcfr6rXae9Q2Qjp33ROxvxxGtrAQDXPvJ6U8eSNfy/\nS6cOp70RfdqT5LR7HkLh8ROGnHaL1eM11xwTQay7DgER6T8JCC0OWOjT3uQ1OUIIIYT4oGjPJcly\nnNW+wwBQlAOJRqSimxgb5ZJqnPYdxBrruaShz8KwHZSrOQXTzml3jHPa9YXoml1deuWG/sD9Zjv/\nWSIo2utw2nWF6FJ2h01+lxWnPbh9reWbVae9nOw6FBWm7rp2FzlD10vDRQGtaKfTTgghhGQGivYc\nok6OTVstqQ4XABQ8u5NQnRNlEpaqK0TXgbJ1d1gV7aYt37Si3Xp4vNpay2xCr0ZW1HLarYs4Q5av\nC+ayr+uz+xtsZULh8YZflU7EWQ+PT+C0u55EQUS1fLOY065dPDQ4foRo112fkqBed0wL0ek+MobH\nE0IIIdmBoj2HhHuLm1aPD29f8OzmtGsnyybVpbWi3bXuxoY+C+Pw+ELocev9zxN2CwjntFf2b7bT\n/sbavsD9tb12oz1aGf9CiwNp7Jrqvtq0CySqEUAj4Xlhp308egFIq4teuogdo3xxTU47YFbMLhbq\n8QzPcbWTBdD885sQQgghw1C055Bw9XhDEacLj0cDnPaEheg6ULbuFobD400L0YVFe+o57Ybfd9hp\nr9y37rwa8mZPb+B+z2a7C0etjP87doQ0c4ehd1kHy7ZFe7LweLV6fJcoYywGrC4u6CN+DFxyzXUI\nAKTl8PhQhI/hZ6Bb1KFmJ4QQQrIDRXsOESHn1Wx2pguP77DstGtz2g3yNHUFoDpRtu4ehQSwYYhv\nK+S0q6K9oxoe30gnzvMkLvjVAhz7Pw/j6aU9AMJO+5peivYaajSFkdBEg8LjVWFpVIgu3KcdACag\n1+o4tQUxTc6fCEfdfiE69bM07dOuEe1U7YQQQkhmoGjPIUmddp3o60DZ6iRPX7XZIKdd57SLsvX+\nyOGcdrPweFc2ouWb6rRLo4J5DvSF6KyHS4/AHxetxJwXVuKNtX047don4XkSb/YERXvPZobH11DP\nUdNw7EaEx6vusHnLt/D2E0Wv1fNHex0yqa0R4bTDck57uD4A+7QTQggh7QRFew4JC03Dlm+ayXIn\nSlbDuvXV4w0EcVQhOtvh8Qly2l1PoqRt+ZZuGDKASAdQRyg8vtbyrYHh8S+v3BS4v2rTAAaUcO21\ndNqHUIsNGlU8h95pTzs83mTx0B3RaU+5EJ3BOR61EGHaR300HPV4Fr5v044DhBBCCEkPivYcEu7T\nnrx6fKco280l1RzLaKIbkdNuWxCrebUmDldUeLwnLYem6j43g3GGw+Mb3/Jtm3GdgftvrO0NbUPR\nPoy6mCQ1rctGQqfXUu9qYNinvai5Dk0Um63+LnVpNjZy2k3TFUYlodOuuywyPJ4QQgjJDhTtOURA\njnh/NHROexcGreaS6sJKjarHa2ahXSk47aHP0sCdkhIa0V7Z36pA0okhA4EU2fKtgeHxYzuCBfte\nfHtjaJu1DI8fQl2IMw+P1+S0W3bakyweeppCdADQLfqsnuM6cW2SphNZiC4qbL5OwoXoTFu+sRAd\nIYQQkmUo2nOIOtk1LUymddpRttwfOXwssz7t4erMlfB4u2GpYeGQLC+3q1qF36qLnTQ8Hqpob7zT\nPqB8908tXRvaZi2rxwOoOKTqwpqdQnQph8eb5rSL8G+4Ek1jM6ddE/FjoRBdVCu4eklSHwCIyGmn\naieEEEIyA0V7DgkXojOsHq+ZEHY1IKfdZLIsNa68IyTcsr3JspRhcRRyvEbA9eRQJfYaY1Fxi62G\n8Wud9vjjLESExzey5dtAKTiGWgV5Pz0Mjwegb4dmJDShD5cu2e68kMRp9/ROexGu1XNHG/FjsAAi\nIsS5yQJkvNdJoXo8c9oJIYSQzEDRnkNCQtO0eryuEJ0oW3Xi9LmkJi3f9JNlt2xP2Lka4WASHu9J\niQ6hivbK+KyG8evcPitOe+PC49Wic+v6wpEUaxgeD0D/uzRtMdaY8HjVHY7/m69EqYTfUxGu1cUk\nfW0NE6c9YtvU+7Qnrx5P0U4IiYOUkjUwCGkAFO05RHXWdY7VSOjC47tQslsAKmHV5qhcUrdkT9iV\nXQ+OUN6zYfX4DuWzHCtqTnt2ctrVyIoO4QKQTXXadWzsL9uvcN6ClFMKj7dfiE5J09FcV6KIqh5f\nhGt1MUm3eKiL4okiKvLG5BjxXqf+VANA77TbzoYghLQfPb2D+PhVj+HY7z+MJas2N3s4hLQ1FO05\nRJvDbuCq6ER+J0qWC9Elc4ejKs1Lm067ZkJv2qddDY8fU3Xa7ea066rHGxSi04ijArzG5rRHiPFt\nu7sweevhyvLr+hgi77oap91QyOq+2kHLKi5J68lK9fjweDqsO+3JWk9GXrNst3xTriPmLd/Cj0k6\n7YSQUbj8d4vw8spNeKtnCz7/64XNHg4hbQ1Few7RhbfHFnFSoqC6y6gWorPpcGnGY5QHGuEs2hTt\nZc1rmOXlIlK0W+01rTuWwaQ+ytG07ryOQH+E0773lG5M2mpYtLMYXaUeQiin3aBaeZRYSz083sRp\nlyM57SnX1jBINYjKabdZPd7zJBz1OzMtRKd12inaSf387NHX8OFv/wm/mLe02UMhKfKX14eLwr68\nclMTR0JI+0PRnkMcXeG5mJO8qMlmJ0pW87D1hegMnPaIbT3XotOuK2pnWAE7Kjze5oTZ031ncT9L\nz9P+XjosdwsYjSinfZ+p4/GerbqG7q/tZV67qw2Pj3/uRP307IfHB48XEp4joMvbB4CisPu7TFoQ\nMyo8Xlh02l0pURDJwuO11ePptJM6cT2JKx58FSs39uOKB19p9nBIivAyQUjjsCLahRDLhBAy4t/K\niH2OEELcL4ToEUL0CSGeF0J8SQhR0G1f3ecfhBBzhRAbhBCbhRBPCSH+xcZ7yAuViuf1i/aoyX+X\nKNktRJew5VvUpNizGh6fTLR7rqepHl8tRGdVtCdINYhYpLHtaI5GlGjfe0o3JvnC41lBXl893iSk\n25/P3oVBvF8sg4CXQni86rQn79NehJd+yzcDwR2Z025TtOvy+y1Uj+dknNRLyfWwpRodtWmgzFSL\nNoYFKwlpHEWLx9oA4ErN46HKFEKIkwHcCaAfwG0AegD8I4ArABwJ4FOafT4P4EcA1gL4FYBBAKcC\nmC2E2E9K+WU7b6O9KbseOjTh7XHFpuuWoVtV6bTc8k036TRy2huS0x4WtNrUgwg8txwqZJdGTrsu\n9z6+064X7bZzh0djoKwf7z5Tx+O5t9YP3V/D8HiU3bDTbhKOXZuECXi4qfM7ONRZjLvcIzGn/E2r\n40zS8s319H3abadtaJ12gwWQSEfdYni8votF8urxDI8n9aL+djwJFESTBkNShZcJQhqHTdG+Xko5\na7SNhBDjAVwHwAUwXUr5TPXxbwD4M4BThRBnSClv9e0zDcD3URH3h0gpl1Uf/yaA+QAuFkLcKaV8\n0uL7aUtc10WH7onYTru+VVEnLLd8S9xqqQE57RohaRKWKr3wZ1kLj7ea064TDnFXx0d02hsXHt9f\n0r/We7fdGpP84fFs+5a45Vvtp3GgWIJDncUAgE8WHsd9KfdpNwmPj+rTXknbsNkuMfz7N3LJI8Sz\nA6+Si+4kVzK6bgEmol1Kqb0c0EEj9aIunFUiY6ja2xNeJwhpFM3IaT8VwLYAbq0JdgCQUvYDmFm9\ne4Gyz6cBdAH4cU2wV/dZB+Db1bufS2vA7YSueBqAusLjt8jhsOROyy3fdOHbVqo2W8xp17bRMpno\nasaShtOuFWxJw+NFuaGF6KKc9s6ig/cwPD5A2QvnOJsIzZpYO8J5MfC4zYUkQFM93sRpH6FPu82C\nmLo2kyY57U6EeC7ARcnSOLX5/UZdLKIe52Sc1If6/xejNtoXXiYIaRw2nfYuIcTZAHYB0AvgeQCP\nynBM84zq3wc0x3gUQB+AI4QQXVLKgRj7zFG2ISPgRoifepz2AXQO5WB3ClfrPNeLbmJs1Nu4AU67\nzsE26TUtyxqnHfb7tOud9riiXb9dB1y7juYoDGic9ppJOXHccOzIhi36SJA8oStEZxKlUvvpfdhZ\nFHjctmgX0oPffDNJLYnq016wnLah7dNuEh4fUT2+AA9lV6LLwv/AZc8Ltb8zifiJElQNDKQhbYa6\ncMYFoPaF3y0hjcOmaJ8C4CblsaVCiPOklI/4Htur+jdUUlRKWRZCLAXwfgC7A3gpxj7vCCF6Aewk\nhBgnpewbaZBCiAURT+090n7tQmKn3SfMXVFASXSgQ1aEklvqTzw+3wuFHjIKj48SpV7ahegMQnw1\nqQZjUP0srTrtmnFaKETX0JZvmgWh847cDQAwYeywaN/YT9Gua/nmGSx4eVKiEyUc7LwaPK7FRTkg\n7Kw7JqklMrpPu820De3ioVEhOv1YbBbM0y3SmITHR026WT2e1Aud9vzAr5aQxmErPP4GAMehIty3\nArAfgGsBTAMwRwhxgG/bCdW/GyKOVXt8Yh37TIh4nlTRFiUDYtsqrm/C6sJBWQyHJnsli/nEuj7t\nFsLjde52vbiaiuYmIb5CEx5fy2m32bbK1R0rYSE622HIo+F32ru7ijjyfe/Bl47fAwAwfgyddj9a\np90kp90DDnJexVgR/H1G1bOol3B4vGnLN014vLAcAaJbPDQQxFHi2YFn7Rwvu8kK0UU77ZyNk/pQ\nz0FGbbQv7AxASOOw4rRLKS9THnoBwOeEEJsBXAxgFoBPxjxcLWDS5EoQex8p5cHaA1Qc+IMMXrMl\nKUdN3usIj/dQQEl0Yix6K/ctOu1aV92k93BUf+SUc9qN3EJNqP6YFFq+aSuHxx1nRpx2f8u33114\nFHZ9z1ZD98f7nfYt9qpytyplXY6zYfX4I50XQo9L26I9FB5vIDSljHTarZ47uvPEKDxe/55snj+6\nBQyj8PiISTfDXkm9hKvH87fUrvCbJaRxpF2I7n+rf4/xPTaaKz5e2c5kn41Go8shSXPapU+ounDg\nimHBJK067clySSNFikXRrguPN3HadWOp5bRbDY/XfW5xP8uIiIUiXKvRAKPhL0TXVQw2HRw/Znjt\nkeHxEdXjDfu0HyJCmUhWRbvnyZCz7hhM/7wRctptRoDorjkmn2V0ITp74fHaRRqjLhb6xxtYsoK0\nGepvm6kWbQy/WkIaRtqifVX171a+xxZX/+6pbiyEKALYDUAZwOsx95laPf7y0fLZSUQeNhDfafeJ\nOA8Oys6waPfK6Trt2oJqUUT2R7ZZiE4j2k0mJ5qWb2OqIclW2+el1Ke9kXmK/pZvYzqCl62g017K\nfbhepU978DMwqx4PbAfXuzUAACAASURBVCW2hJ+wuOClE5pm4fEeiiJ8jhRtF0jUfW4Gn+WIot3S\nOe5qugWYVuLXQXeU1EvIaWeqRdvC6wQhjSNt0X549a9fgP+5+vdEzfbHABgH4Alf5fjR9vm4sg0Z\ngch+zfWEx4sCXF9OO1x7Trs2rNTAPYoKSxUW3UJdfQAzp11XPd5+yzc3gfDQ9ZIHmtvyTXXaOwoO\nxnVWHvMk0Dtot2Baq6Fz2k1Eu4wIPZdRC351oMu7N6kery2uCPvh8drFLaM0Hf22BWGvyn3Z8xIV\nomNOO7GNGu1Cp7194WWCkMaRWLQLId4vhJikeXxXAD+u3v2V76k7AKwBcIYQ4hDf9mMAfKt696fK\n4W4AMADg80KIab59tgHwterd/wUZlaROu/S5Qx4cuM6waLcZHq+v2py8EJ2IEKH1oFsAMcklHSk8\n3mpertZpj7tIM4I4alD8rJQykNPeWQxftvzF6DbmvBidrnq8acu3IjQF2CwueOmEZijEewSiFiGK\nsLyYpIv4MTjHCyO0fLO1MOd64UUWk9oakdXjORsndaL+38DfUvsiGR9PSMOw4bR/CsDbQog5Qohr\nhBD/LYS4A8DLAN4H4H4A369tLKXcCOCzAAoA5gohfi6E+B6Av6LizN8B4Db/C0gplwL4DwCTADwj\nhPiJEOIKVHrBvxfAD6SUT1p4L21PZEhmHU67KwpB0V5O12nXFoWK3F8/WbZaiE4jHJI67bXweJuT\nHO3nFtOJcyOq7Teyevyg6w110usoCBQcEdpm/NjhvPa8V5DXVo+POB90eFKTIw1of6/1oosGqLx4\nssWkorAXdg7oFweMWr5FXA8K8KylwJS1Ld8s9GnnXJzUibpwRqO9feF3S7LICys24JPXPI5L7ny+\nrVImbVSPfxiVPuoHoiK6twKwHsA8VPq23ySVT0xKebcQ4lgAXwdwCoAxAJYAuAjA1er21X1+JIRY\nBuDLAM5BZcFhEYCZUsobLbyPXJA4p93n2ko48HyiHRZFu84pN5osR+a023Taw8cyCfHVuf616vE2\nc9p1ufdxw+MjxVEDnXa/yz5GCY2vQad9GF2+uEnLN09KFHWV3A0q0I9G2ZNwhOb3Iz3EWktu1O9S\nc100EcSNKETnehKdKfRpZ64qqRf2ac8PvEyQLHLdY6/j2TfX49k31+NTh+yEg3cNBYS3JIlFu5Ty\nEQCP1LHf4wD+znCf+wDcZ/paZJikOe3+9mGeKtot5rTr/icwcdqj3o9jVbTrJvQG/4Npw+NTcNoT\nFKKLcto7bIchj4C/R3tXh17QBYrR9ee77ZvreiFBbHLuSIlQYbPKge067doFLukizn9L2oUo1MLj\nLUaAJOxi4Ug30NauRhEuXEvjrPRpD47TqD5AxKYU7aRemNOeH3idIFmkp3fQd7t9jJy0C9GRjKEt\nSgbEXi71i3ZXFOAVuoaftOm0J5ws+8PjB8XwGG2Kdl3/c5PweKHZf6ywn9Ourbof87OMKj5WbGD1\n+P5SdBG6GoG2b7l32jXft6HT3qHJabcZpaJtUwbU1XrSj/1CdJraGgYudpTj7cCzVohOl2pgVIiO\nOe3EMqwenx/4zZIs4o+4a6f/yyjac0ZUuHM9hckq4fHDDqfNfHHteEycdp9wKflEu81CdLoFEJNe\n09qc9qrTbjcvt/7P0o1wV4vCtRrCPxL+8Pgop31CwGnPt2jXpW2YFE/zpL4oXEHaW6hx3SinPW7E\nj/47rrRSS7vlm4HTrkszAFC0WIhOV3jQTk57+0x0SGNhn/b80E75wqR98P+/1k7/l1G054zIkMyY\nzoy/erwrCpB+pz3llm9mTrtPtDtjhm4XbDrtGvfSLKc9vMgxJNpthsfrhEfM79srN99pH6ndWw1/\neHzuC9GV6/++geic9iLsLdRUqsdrfj9xWxFGOu22w+OTLR76q7hL3wKnY7EQXeL2eUMTGgm/b9ag\nOpOkDXFd1Wlv0kBI6rSRiUnaiJLvokOnnbQsiQvR+UK6pShAFoZz2u067cly2v2ivVxIx2lP2vJN\nGx6PAQDSbk67bsZkoxBdw0S7z2nXtHsD1EJ0+c5p19YwMAiPlxHV421GV0RWj08YHm/9d6kNj6+z\nEF1x+Dpks2BeWdPyzdRp3xbrcW/nTPy+82uYirWVx9vInSCNRT0H28npIoRkHzrtpC2wWYhOwgnk\ntDspt3yLKzSBYPX4st9plymLdoPweJ3TXhASnZaLvGlz2mNHVkQVomtcyzd/TvuYyEJ0vpz2nIfH\n6xbmPCOnHdqc9g6UreVh69qUAYh9HQqc32J4waYo7FaP11+HYl4rpfIefQucjrBbPd5RCgea1Nbw\npMT/7fgl9neW4v3OG/jvjp8BSCnsVUpg+TPAmiX2j00yA6vHE0KaCXPaSVvgRYbHxxXtw5NYT3Ha\noRGh9aJzikwmkcIX3usWhkW71erx2px2A6c9QhCPwYBV4aENj0/stJeb0vItuhAdW77VSFJ4EIju\n015xh22GdEe1fBsdf9/5sq+Dhc0xRo0nbiE6TyKQZiBUp91W9XhN1IJjUsPAA2Y4zw7dP6bwNwAp\nTXQW3Q38/DjgxwcDq162f3ySCVg9Pp8ITacMQpqB//8vinbSsrgRTnvcfHF/WKonnKB7lHIhOqOw\nVP/iQsHvtNsLndZVj0/apx0AxqBkrR0UEJXTHu8ipitqBjQ4PN7f8i0iPJ6F6IbRho6bOO2uRIcI\nb98BF4PWctojwuPryGkvO6oYTjc8PnYRR/U9+kR7AdLaopfnhVu+iYgCeDpcKeEivBiWyprc0keH\nby97LIUXIFmA1ePziUPVTjKCP6e9ndYMKdpzhk5oAiM48CPsL1EITERtinZtWKpJqyXfZNktplOI\nTudomoXH67+LsWLAbiE6rfBI5rR3NKsQXUecQnT5zmnXpW1oF26i9o/Y1mYethtRiC52O7UI0d6I\nlm+xzx01YqHgF+32xql12g0WD11PoqQR7amEx/uveRHXP9L6hHPamzQQ0lAo2UlWCDjtbaTaKdpz\nRpTTHjVRV/EXufKEIto9i33aNZNOo+rx/sWFYjo57TohpHUPI4haQBiLQbuCOEF4fGTBr0a2fPM5\n7WNiFaLLt9OuO5dNRLuuFSFQEZq2vvNSRMs3N6JbgYpf3Luq027xd6lfPIx3brqeRMEfsVAcjkqq\ntKazlWoQbvlmFB4vJTzNVCCVRTn/NZyivW1hTns+odNOsgJz2klboK0sDQPR7ttOikLQPbKY066d\ndJrktPtbLRXHDt0uaAps1YvuszRz2qPC4wetFfwCkrV8i4rMaJ7TzkJ0o6GLjhAG585I37mt8Pio\n6vGx00J879Ev2m062EBEFfa49SBGcdpLFp12NWrB1Gkva5z2VE5v/3WHor1tURek2ql6MxkBanaS\nEfx1Ndrp+kPRnjOiJsWxXexAyzdHcdotiiVdTruBW+j4cjr9or1os3q8RviaOO0iYiyd1nPa02j5\nlq1CdN0+p33zQDnXOZSeRgyZnDtRHQNsVmYvux4cUX8hOvgL0RUaHR4ftz0mgq3YAjntHlyLCyBF\nJYfd1Gl3NVOBVM4hhsfnAvUcbCeni0TjULSTjMBCdKQtiGr5Ft9pD/Zp91dEtum0ax2uOvu0o8Pn\ntFssRBfZ+zqm4I5a5OgUdlu+6fKE4xb1ixJwBXiZavlWcAS6uypuu5TApoH8CgJtNI2FnPZKyzdL\nQjNiMShueLz/3POcTsiqxeMIGbnQVA9OgnPHlUqBuIISHm/LadekGjgGheg8DyhLXSG6NES7q79N\n2opQeHwbOV0kGkGrnWSEMkU7aQd0Llzl8TrD49MS7bqcdpPq8b7JtugYzmkvWhTtkZ9ZzHFGinaU\n7LrYGnEdV9j4c9oHMSw6OlCGJxtTFTiO0w4Ei9HlOa9d+7s0OHcQGV3hWkvbiBLtsfvJS//iYRFw\nhtMjCtJipIXV6vG+2hrw7H2WSQvRRTntqfRpZ3h8HlAXpFIpakgyB512khVc3/+vDI8nLUtUGLyM\n66Apfdodv2iX6ea0m+TlBpz6jnFDN4uwKOaSivaIBYRO2HXadfnrscOlfRPrQRHshw2EJ2dp4Bft\nE711wP3/CTw0K1QwrXvMsHDbkGPRrvtuY1dlB4CIxaSixUJ0XsRxYl+HfKJfOgWIwvCCjU0XWxfx\nI+J22lBz2v2F6IRrLQWm7HooiuCxTGpreFE57amEx1O054FwIbomDYQ0FMFCdCQj+Fu+tdP1pzj6\nJqSdiGrtFtfhUp32gIttMac9sdMOv9M+HB7fYdNpj/rMYn6WIzntaee015MOURKdqGmBWh/vRoQd\n1cLjd8RqfGLhfwJbllee2G5fYP/ThrYbz17tACKiKIxy2qML0aUdHh/3dxmsrVEEnOHvvogyyp6H\nzoRr0lJK7XVI19lChztiITp7Trsuesokp9319H3a0ylEx+rxeUCNFGun8FQSDSU7yQr+aw6ddtKy\nRDmscQvRBUW7A6cjHac9SdVmIBge73T6nfYG5LTXER7v7zXdJUqZcdoD4fHO8AJNR/VzLDUgr32g\n5KEDZfy6878wvibYAeDdFwPbBdu+5VcQaEW3gYiTkU572Z7QjDh3osR8CP/im1MACsPrz7aK0XkS\n2l7yZuHx+pZvRYs1IXRtPE3D48sNavlWKg3/tmzWHiDZQl10bqdJM4mGRjvJCsxpJ21BdCG6eJM8\noThc/vD4Dpvh8bpJZ5057YVOn9Nus+Vb1GdWR3h8ubjV0O1Oy5XZdQI9ttPu+p32YD9sIJg3lBYD\nZRcfdhZhV2dV8ImNbwfu+sPjN+e4EJ0+p90kPF6/bVFYdNoTLh5CKYjpz2mv9GpP/rssa/qfV16w\nzurxPqfdgWftHNct0pgUopNS77SnkYf89rreodsr1m6yfnySDVg9Ph+o1wiGx5Ms4Hoy0CG6na4/\nFO05w2YhOjgOnE5/kTc7IcmeJ/U5mQai3e9wCZ/T3gF7Raoi84RjjrPgd9oDot22064ZT1SUgIrX\nfKe9v+RhIjaHn9j0TuDuuM5h4bFlML+iXRtFYSDaozoGVBxsWzntya5Dwr+dKATC4zuEnXGW3Yjw\neIM+7Y4/17wYXPSy1fNed003aT3petA77SmI9kGf097XP2D9+CQbqJNkOu3tiTpPyXOrVZId1P//\n2+n6Q9GeMyIL0cV2uII57QVfeLwt0R7KBa29nlHLt+FtAzntKNsTmlGT95iTen/7Obc4vLDQibLV\nnHadaK8nd7jkE+2dVdHeiBXMgbKLrcWW8BMb3waevAa4bgaw+IGAaO8bzG87KZ1oNyniGJVrXICL\nUjnl6vExF5P8ol06xUB4fMFSlfuypio7EJG6o8GVSv90X8s3Bx4Gy7YWQJKHx3uhqYBMp3gPq8fn\ngpCYa6NJMxmGrf1IFgkXwmyf3yVFe86ICoOPXU1cBsNSha+NUYct0e7pHa64k2Ug6LRDEe22TuCo\nhY64qQZOQLSn6LRrxhM7p923XcmXd19z2q22potgoOxhK/SHn+h5DfjDV4EVC4BbTsfYzmHhRtGu\nYFKILkJMdVh1hxMuHgZy2tVCdK4VQaxrpVZ98Vj7e6GWb36nPV3RbuK06yKbulBKJTxeBER7fs/R\ndofV4/MBF2dIFgml57TR75KiPW9ETJTiCk0oYal+p91WTnuUw1W3067ktNsqphUVchzXxS74FjnK\nHT7RLuzmtGvHWYfTXvaHx4uqaG9Qy7dundOuMK5jOJ9uSym/gkDf8s1g1hwh2iu54paEZln/GlEL\nBipBpz2Y095hSbSXPU+fphPz3KlEDI3gtFv6LHXpDA682KLb9SQ6lBz4LgymMtEJpDVY7ORBskUo\nPLWNnC4yjPr/QQOy5QgZFXX+3E7XH4r2nBHVpqwe51U6BRQDgtiS0+5KFGzmtBeHx9gpXJTLlgRd\nhMCIWwHbHx7vBcLjS3bDeXTh8XE/S79oL4Rz2q2G8UfQX3KxNUYX7duI4cJWfbnOaQ+/d2FSiC5C\nUFutHp908dD/fpTw+CLKdkS7GxEeH7flm6dcx9ScdltOu+azLMCLfQ0JLS6g4rSnMdEJLKbSaW9b\nGDadD9rZ0SSti7po2E6RPhTtOcOL+PXGdeLUAlCFznCec1IqDley8Hh/n2Kn2IEShif1btlOREDS\n+gCB8PiOrYdud1b7TFtDJ9jitluKzGmvHNNa1MIIVMLjRxftE911Q7dzHR6vywu34LRbDY+Pug7F\ndtp92ymF6IqWXGzXk3CEbvEw3m9eSiVNx9+nXdgLj9dXj5ex+6xLGXbax4jBVPq0+xePBJ32tkX9\nf4Fh0+1JO+cOk9alnQthUrTnjKiK53ELQAX2dwoodvjdV3s57UnD4/0tj5xCMSDayyVLrekiw+Pj\njdNfuM/rUFq+ZSSn3R8KrHfam1CIzhdm7Mcv2rfkWLRrw7dNFoGiwuOFnVZqwAiF6OK2nvQLvkIR\nKPhFuyWn3ZOJWk+6nlKIztenvWAxPD7KaY87UXE9BMcJYIztaJ8q/sVYOu3tS2jSTDHXlujmKWnU\nwiDEBHWe0k6LSRTtOSOpO+yfaHmigGLXsJDrQim2CzUSlcly+Dhmheh8k8NCESUMT+rdkqaoWR1E\nLSLUEx7vdiiF6KzmtCcR7b4xakR7I3La+0seuv1O++S9tNt1l3uGbufaadd830bh8SM47bb6tEe1\nnqznOgRRDLV8s+O06/u0x/0sXamIfl/RzgI8DFhy2nXfl2MYHl9UoqS6MJiKO0GnPR+wT3s+cDXz\nlKx+1399az1m3fsinntrfbOHQlKmndM2KNrzRpRYiyuIZTAstVgsYFAOt9qCm9zFdiMdrvgnnt9p\nLxSKKAv7TnuUUxRXePhFu1QK0aWd0x4/HcIv2n31C2qF6BqQLDRQcrGV8C20bLundrutSmuHbtNp\nD2Ky4CU9fcRMEa61donRi4dxu1j4w86LgDN8DbKVL5508dDzJIr+61jB77Snm9PuGDjtoXECGJOa\naKfTngfUWicNyKIiTUD3/0EWBZKUEl+4ZSFmP7EMF976bLOHQ1JGvf60U6QPRXveiJhwxq8e75t0\nOQUUHQeDPhcb5YEko6scImKybFaIzpfTXuhA2Rce71kS7ZGpBnX0afc6/TntJas57TpnsB5x5A+P\n72yg0z5Q9oKF6CKc9nGDq4du95Xy6+Ilrx4/vL8LvxguW+vTHpWOE/fccaTitPvC420J4rIbsXgY\nt+WbBByhb/lWgGctagERLd/iXkJcT6Io1Jz2Uio57Q6d9lygRooxZLo90ZkLWawgX/Yk3uqpzCHe\nWNvXViKOhGnnSB+K9pwR3Vs83mTZP9GSThEFRwSLvJWSi/aosFQj0e6bHDoFJ+C02xgjAIiozzJm\nfQB/TrtMM6dd97nF/b6jnPYG9WmXUlZEuz+nffIe2m3HDKwZut03kGMXT7NIYxIeLwLFB30tHS2G\nx0cVnIu7mOQfoygo4fFwMRjzHBwJTw1vr71evTnthaBot1aIThMZURAy9kKspwmPH4PB9HPaTVI2\nSEvBAmX5QPf/fxaLfoVynDM4RmKPdv6+KdrzRtREKXYLML/DVfn5lH1uXNlCZfayJ4MOVQ2jQnRB\np90Vvpx2C9EAlfEka5/nryztdQRbvmUnp903Rm1Oe7rL6v2lyvEDTvu2e2u37dziE+25Do/XtXyr\nrxBdSfiFpmvt+3YjU0viLh4GC2IGW77ZC4/XtZ6MHR4vlYKaKRWii/rMoj5jFU9TPb6rWlTUtiPF\n8Ph80M45pWQY3WJMFr/rcAuw7I2R2EO9/rRTZAVFe86ICpOtK5e0mkdaDjjtySvIV8JS7RWic5Sc\ndmmp5Vt0qoG50w5/eLzlnHbt51bH9+0VG++09w6WAcigaJ+0u3bb4hZfeHyO+7Rr4xNNRLsb4bQL\nF4OWwuOjrjd1VY93CpVe7VU6LIl2z0L1+IBoLwQXQOw57VEdQeKdA66HUGTTGFSukbZdM/9CJcPj\n2xdWj88Hupz2LH7X6jwli9EAxB7tXFODoj1vRE3w6ujTLqtC2BPDPyMbTntUy7e4uaRA0GkvFIuK\n055yy7c4oblKSKr0ifauRuS0x/y+/b3kpa/6dadwAcjUc9r7Blx0oYRiLfKi0AX42gz6cXpXDd3e\nUsqvi6ertVBv9fhBn9NetOi0RwnNuIuHgdxoJxgeX4CLQQv/S0cVxBTaa1OYitOub/lmKxqg8kLJ\n6gNUnHYlPF5UrpG2XTP/AqLD8Pi2JexsNmkgJFW0TnsGRbu6uNCIWjykeZTURZo2+r4p2vNGlFiL\n7XD5J8thp71kochb2ZP6ibGBW+jPJS04Rbh+p91WTntUeHyccXruUDRBWToQxWCRN7v/qWiOVUd4\nvHTCucNp/wfdVyoH2711dVf+HnBm5e+YiUNPif71GCMq0QslV9or9NVi6MOO438Wfge07ARFuzV3\nOOq7qacQnaP0aRd2xumq4e016g2P9zntjpAWP8sEi4fQ5N5jODzetiHlBJx2ivZ2JeS009lsS3SR\ndlkMj2fkR75o5+sPRXveSBwe79u/Kto9nyAuDSYX7aHJbhWTVkuBnPZiMKfds5TTnqgQna94VAlF\niI7hCX0nSijZcuEQ5bSbiyOnUAy0repAOfWc9t4BF1uLvuEHuqoRCX/3P8A/XQd89s/A1lOGnt6p\ns3fodl7z2vVOe5057aFCdJaqxycOj/flRheKKYXHA0Kz4KWvKB/G9RBspVYMLoAM2FpUioz4iRke\n73rDkSxVauHxthflnIDTzvD4diWUU9pGk2YyjO76kMWvWl1coNPe3rB6PGkfosJS44bHB3JJa+Hx\nw4XobDjtbsKWb65UnCNRgOdziFPPaY8zTl8/+xIKkL7Q2U6UrBWpAmCterzqaHagbE3ERdE3WMZW\n8PVorzntXVsD+58GvOe9wNbbDT29Y3Hj0O289mrXCXQT0e7/zoNOe9ne7zJJlAo01yHHX4iubKV6\nfDmii4WIOSutpPn4q8crhejKnp1WWJEFMWOKYs12XSmFxwcWU+m0ty2sHp8PdIv2Wfyu1ag7Ou3t\nTahPexZXkuqEoj13RDntdThcVafdtey0R+WSxg3xdV0PBeE7SZ1CwGmXrh3RLpBgsuwGnXYnkC9e\nEcO2/mNxkoj2EZz2TpRTD0HvG3TR7W/31jU+vNHW2w/d3MEn2ptSjG7NEuD+/wQWP9D4166h+W7r\nd9qHf5dFuNYiQCIje+rJaS8o4fGW2ql5UqKQoIuF9BQHuxgsRAeEc+/qIuJ648Y9NzUt44bC4y2f\n3g5bvuWCdm65RIbRhsdnUBCHFpH+H3tvHi1LcpcHfhGZVXWXt/TrTb1JSEiywOxI4zMWm9EsBh8Y\n+wwMtuGY1fbYRuYAwvYxYBvPjGx8EGA2gyyMxAAD0iDAbo4NNOoRQiAJNWrUoqVWt1q9vd7fet9d\nqiozI+aPzKz8RWREZWRGZN16dfM7551Xt24teSuXiu/3fb/vNxyPGw39e3Udj8muGEj7SYONnDsH\n0ZmU9oq0hwqiM6fHu514gs4WlwxgrBel3UaEhMtCvEbaVXs8gIBqu2HkW4cgOsbr9vj+SXuKXdrT\nTgL7FiCk/TZ+lTz3GEjBr38b8CdvBX71bwL7LzQ/vg/4zmknj017mtPuOy5RDaIb1ZX2ED3tVnu8\nY684nbwApmxjqeAHOcc9Q/1gsNEv7PGD0j6gA3QFdlA2NxMmm/k6qpo6iet76s2A40W9SHNMG9ID\nBtJ+wmBLPu6ktBep8ZLY49MAI998A6CytNqGrJgh3w9pt1l8HRajRO1PEQFKT3u+iJ4lYQiSUWl3\n7WmnKlw8Ue3xLAw5WoaDWaaOeyvt8RSnK9J+KyHtx5Ig/9wD1e0nP7D69wfMSnuLIDqq3GacJJ6z\nLFwhyTPxXFXaI01pD7OdtikWzsVDQoYFIjNpD3H+WEdPdrfH9zXybSDtJwP1nvZj2pABvcKkYK7j\nvt7kYLIBdQxz2gdsDizk3DWYTCF7UUmIyZz2EEp75jlqKa22MSsKC6IPe7xVaXch7RUZnktVaS/H\nL80C9OUCsPS0u32Wsaw+KxZv1ZT2oL33BhzV7PHLlfZbcGVx+9iD6EJlJ7RE2J52ao8PV6SxBs65\nOkCg2eNrQXT+X9JCWrI1XAsglLQzvgjuBMKSdtu8c1elXRrs8eXIt9CLnYgG0Tk6FgZcfxh62k8G\nrpue9mHk24lCqq1L1/GY7IqBtJ8wWG2yznZpGvCWL5QltceH6Gm3LZa72OMLpV3yPnrazdvjtFjW\n0uM5GQc1YSkA2a/S7rigjwT5rOJxvac9ADlahgNbEB0FGft2ihD8o+PoaacIdJy1hmd6vM0eH0ME\ns8ebx9K1scdX+5brc9oDOQJS25x2x+sQHcUmmK60578LQtptrgXH9HhaQCxR9rT3aY+PBqV9Y6Hb\njwdlczNhspmv476uHY8bROIG1FFLj1/DY7IreiHtjLG/wxiTxb+/a3nM1zDG3ssYu8oY22eMfYgx\n9i0Nr/stjLE/KR5/tXj+1/TxN2wqbGnF7vZ4zZaKYn53gcywAGwLYbWlutrjqS21UNrJot60SO0C\nrzntmj2eR6xGiEOp2OaZ924L5hFV2kdbtfT4ECndy3A4zxQibgyiIy6Fck57+dzjxD0fexLPXj1q\nfmBgGJX2FqqmorRHahBdqGPS5uxxz1qg889HQKQr7f773nYdch35RouHEhHADEp7iPPHNsbT8bWZ\nSWlf2OO7b5YJI0aCLQfSvrEYlPaTAdN+Xcd9rTsCBqV9s1Frh9ig/R2ctDPGXgrgpwDsL3nMGwHc\nDeBzAfwygLcBuAPAOxhjb7E85y0A3gHg9uLxvwzg8wDcXbzeABfYFnjOo5ZoenyxUCakPQ0y8k2A\nM9NJ5qq013vaJSGboWzLoezxCWJEnAGRGkYXSmk3k3a3144kWdBHE0MQXf8j3041BdGR5P0trA9p\n/6NPPoOv+w9/4TE1WAAAIABJREFUjCuHK1bcjUq7+36iduuMq20bwZwVtiKhY/GQEj7OI0VpjwPN\nabdNsXDOB1Ds8ZFqj2cSgMQsSE+7Xz6Auac9KV4j3PmtXxcHe/zmYpjTfjIQIojutx94Bm/40ffi\nJ9/zSKjNqqE2zWCDSNyAOgal3RGMMQbg7QAuAvg5y2NeDuAtAC4BeJ2U8jullN8D4PMBPArgTYyx\nv6w95/UA3lT8/vOllN8jpfxOAK8tXuctxesOaIBV3eiitBdknVrPQyjtmWWx6aq008VhVhziUlHa\nw5Ao26LTLYiOkvYIEWO5/bzAGClmAdRCwNbj7Ki0C6q060F0YcjRMhy6BNFpM+5LrHxOu/bFMEaC\nZ65O8c/e/UCYedyOMJ3jrcZrkfNPtcevUXo8Ofd4pM9pDxREZ2nTcQ6io+nxLAIYA1j1lRtBBCl6\nMUvgnHN6/JI57SHJVpKq3w2DPX5zUZuT3O/XxIBjgr6fgfb7+o3/z/349IsH+LF7HsaTFw8DbZmK\n2jSDDSJxA+rQe9oHpd2O7wLwBgDfBuDA8phvBzAB8NNSysfLO6WUlwH8m+LHf6A9p/z5zcXjyuc8\nDuBnitf7Ns9tPyGoDuaM7H7XIDrVHl88nyyYRYD0eJtS7UzayeJQLJT2itiZ7KCd4NNqkM0WN+dy\nBG5Q2kMRYqOd12V/C4G4CMUTkoFH9Z72voPoDuapZo83kXYy4/44lXatYLXL8n38uw8+j9/6s6dX\ntx1Ge7z7lxanmRDUHs9EENs5ALvjxzVrgR6/XJ/THqaYJAIq7RL1a2Wo7bRdF6VjejwzjnwretoD\nLnYy7X0GpX1zsclK14AKpqKjz75+8JmrzQ/qAF1pH+zxm436yLfN2d/BSDtj7LMB/DCAn5BSvm/J\nQ99Q/P87ht/9N+0xPs8ZYIKkpL1aQDrbUumCtVyAkn5SEUDFtpJ2Z3t8PT2+F6XdRy3U57TrSjtL\nw1hnYVEG2xYWECOOed0e37fSPs+0IDqTPZ4UO0gP/mGy4iA68nkBUBwCv/3RZ/O57ff9AnDlyd42\nwUY0jWGEFlB7vOSxklkhA+VB2M4R18KcorTHBqU9wHGZ2nraHbeR/o2i7Gcnfe0cIhBptwXRuRZi\nTUF04XvaE+18jJE5h4sOuL6wyT2lAyqE7ml/+ko/GTCbPAJsQB21ouEGOX3i5oc0gzEWA/glAE8C\n+P6Gh7+m+P9h/RdSymcZYwcA7mKM7UgpDxljuwDuBLAvpXzW8HplI8xfcNzWP7X86rNcnn+9gy6K\nMxZVbeLOPe1EaS9T46k9PvUnSlaFqJPSXtSlqNIeiHjYAqmclPZUJcQR04Po+lXanezSaUWWZxjl\nhYVaEF3/pP10YxBdpQbT4LyV2+O14+rlpwVQ+IL+9MnLkL/+r8Ee/0PgxlcCb7wP4OFzQG1Es10Q\nXfVYyYpk9uKc7HtcYpcgOs5H9baNALbzfORbd6VdUqW9JOtKcUEECaKzknZXn6pBaZ8UgY4hbaSZ\n6W+VQilkDNgMDEr7yYBJsfZpBTt/uR/Srrd1DUr7ZmOwxzfjXwL4IgDfKqVsOuvOFv/bfDBXtce5\nPv4Gy+8HUJDFriCj2twXy/X0eEaV9hBz2i2LTefFMlW4Cns8Jyp2H8RDgJH3b5ceP0ec8zc69g0B\nlfauQXQp3cYRYm5Q2vu2x89Sh552YuGWx2iPT1Wl/QtvjXDDTk4mrxwmOWEHgEuPAvvP97IJ1vC0\nVkF01HoegRFCHMms9oXYCb72eDqnvaa0p0Fs/Plnaehpd201EFoQHaAUavpW2k296ubnm+zx+bkf\n1B6fGoqljts44PrCkB5/MmBaq/ns675I++D8OFnY5KKht9LOGPtLyNX1H5VSfsB/kxbsp+2n7PR4\nKeVrjW+aK/Bf3PI9rzsoSjuIwtGFtPO60i4CqNjSMl/YOQBKT21GP6SdK/kAMXjRByoNi+AaMpUQ\nR1wPokuCBdFxKUBqCjmcSLuqtOcJ99U2TliCw57t8UdJhl3WlB5PwtKOVWnX7PFsite+7Bze89AL\nGEE7Jng/6mIqhGVMWRulnThVWKxsax5GJxH7br6NnHexx/ORQcEOkx4fM5+RbwZ7vGbjD5Me71cA\nMQXZVSPf+utpz98gRR5JM2BTIKWskaQNWjMPIDAp1j4E6fzlnoLo9PT44YDcaGxykcZLaSe2+IcB\n/AvHp+lKuo7S/7rn+PgmJX4AAVVdJbElus9pJ7bUQmnnitLur5zYbJ2uSruaHp9vYzQiC8NQI98o\naW/7WRLSnsiip10JokuD2eNNyqCbPb4ioTM5Qhzp9vgwNuRlOHBKj6+U9khU23w4X7GKpx1XcbKP\nL/6McwCAXWgKQqAWDR1CwKsPO3+sFvKmj1PrUWm3knkNNIiOxX0F0Zlfw/mzFMvt8aGUdtv2uPa0\n82WkPWBNLjUVIQelfeNwvczuHuAPnQwDfteMvnrak2FO+4nCJivtvvb4U8h7yT8bwJQxJst/AP5V\n8Zi3Fff9++LnTxb/13rQGWO3A9gFcF5KeQgAUsoDAE8DOFX8Xseri/9rPfIDDFB62mPj/ctAe2MZ\nL+3x1YJZBkhmFxal3VXhotsgiyA6StpDpMdLKZXFMv0sO/W060o7S4LZ442fm1NYHt3GsqddtccH\nSxO34HCe4BQNojMp7WSbIlIMWbU9Xmj2eJ7s47UFaT/FptqD+yHtmbUPu5s9XvKoF0JsHfnmeB2i\n9vgoGmmFhTAFL6MyjDb2eJoNUA+iC+UIMNnb9fdfCgNxjplAhCyw0m6yxw8J8puG0OrrgPWF777W\n+9+vTVNcPQr/3bjJyuuAOvQWvk0qGvra42cA/pPld1+MvM/9/ciJemmdvxfAlwD4KnJfia8mj6G4\nF8DfKZ7zdsfnDDCAWqWp0u5K2iP6uEJhZzEh7aa+xZawLdy7zWk3KO1BZsmrgV80id9NaTekx69w\n5JvTZ0mVdowQ8zppDzFn2gYhJLJkBj7J30NGYyU/YQGitHNSaDhKVksI5rMjbJGf2Xwfn3/XWUSc\nYVeP+rAQQl/Y7fHd0uNzpZ1YulmgWe1Wpb09aedRXHeABDh3rAn3zsVDqrTXR75FLFRPu6c93kL6\ntzAPSraMIaWD0r5xMBG5gSRtJoxz2ltcM0xk6smLh/i8u2zG2m7Q1ymD0r7Z2ORpAV5Ku5TySEr5\nd03/APyX4mG/WNz3zuLntyMn+29kjL28fC3G2DlUyfM/p71V+fMPFI8rn/NyAN9ZvJ5O5geYQMPT\nePsgOqb0kubP52TBHGIRJiykwFXhMint8Zgq7f72+FRIRKTfVSgFkObFskx1FRtaT3sapKddWpRX\nuBAPjbTnPe3Vvg5p4TdhmmYYkWA5RgoGCqIRykoUk+mC0K1aaT860oj57Bp2xjH+4u1nVIs/EGzs\noA6bPd614AWodmldaQ8VPsisPe2uQXSkTSdW++6jQBZ+23XIuQCipMeX+R/VV24UqLjgNXoSdufR\nFuZBFzsDaT8ZyALP7h6wvvAt0Jie/8SlA69tMkEvLmwSiRtQxzCnPSCklI8B+CcAbgRwH2PsZxhj\nPw7gAQCvhCHQTkr5xwB+rPj9A4yxH2eM/QyA+4rX+T4p5eMr/DOuW1Di20VpV0YtRXXSHmSOs2Uh\n50o8TPORR6OK8PEQFn4plc+S2uOFw2KZhuEliMAMSnsIe7w1AdvJwk+C6OQIu5N4penxB7MMYxrg\nZiPtjClq+7gIBFx1EN18plngZ/uAlPi8u84euz2+ndJOPjdm6GkPoQ7btqeDPT4PolNHviWZ9F6Y\nSQsZNp1P5hcg1yFe72mPek6Pt47OdHz+BEnQOe32ILoBm4TUqL4ew4YM6B2mnvY2VmSTKv/kpfBh\ndIPSfrKg7+9hTrsnpJQ/xRh7HMD3Afhm5MWDjwP4QSnlL1qe8ybG2AMA3gjg7yOXCj8C4EeklL+9\nkg3fAKj9qtXutypfGrjS016QdqIQh1iEZb5KO7HHl3PaR+OK1IXoaa/Z41v2tMukUrFTVhAOkoI+\nZmFUbK+53Vrf/e4kUkk763dO+9E8WxBwAMrnU0M8AdJczZ4gwRG2Vh5EN5saiHk6w2fevIsrNaW9\nn23LMvP+bkPauX6NiNTE8xD73OrscXGpSKn2tMf1vnsgDx+aeKT0S0uQm6s9Xr0W1nvaowA97UJz\n/FC4houagugAYIvNw458M73PQNo3DqZjZlA2NxNGpb2Fqml6/pMXw5N2vbgQMqtjwPqh5qzYoP3d\nG2mXUv4QgB9a8vu7Adzd8jV/EYCR1A9wg6q0d5nTXk+Pj2JyGIUY+WbpsXTuJc20kVUARpOK8AVR\n2jUbMp157+IIoEp7ioJwRLo9PozSbk6Pd+m7V+3xpyZ673C/9viDeYoRc1DaAU1pz5+zanv8fGZI\nvp3v45W3nsInV6m0G0hcG3t8vadd7RcPkWPAPUa+CZmHuJVgXA+iy197ngpMPGbTZUIt/pWFD1el\nnZJm2ZPSbnNW5O/vRoj5kp52PSjKB5kxPX4Iots0GMPJBtK+kTD3tLs/31TMeaIH0q5v53A8bjZq\n6fEbtL9Xbo8fcHzQ+5tpT7vrINVaABTCK+3SprQ7bqNUUpvzQ3w8pmPBAijt2meZMTKr3sUeT1Ts\nxKS0B7LHW5V2B0Uzm1ckdI4xtkdRrbDQpz3+cJ6q9vgmpb3AFssLIrNUrPRiPZ9P63fO9vDKm0+t\nrKddd4CUaKe00552fZxaoEINzdYokzEBJxKX/436WLqKnI8IafeC0mZDRrV5jXxTe9p9z3Hb/s7f\n1DGnZElPe8heQONUkEFp3zgYR75tkNI1oIJvgcb0/Av7M8Mj/ZBo79PFHr83TXBt2k+xfUBYbPK0\ngIG0nyDk/c1E/ekw8k1Z/PNSaQ9M2i2E0l1pr7Zh0dNOlPbINiKpBfSU7qyla4Eq7QvCX1Pa/VUo\nISwjwBy2kdq9BR8XfferU9oP5xkm1B4fLSPtVVHm7KjaplUmyCd6TzsAzPZx57ltnOXa73qa0x6G\ntGstMLV+8RD73OxScVPa5UJNB5Bfh8hxWRJ6f+s57Umn89XdFgBMD/QD6kq77zZKO2l3T483P27M\n0rD2+IG0nwgM6fEnB8ZWCM/0+D5a7vQRYG2Px0+9sI/X/9t78dr/8/fxsfNXQ27agB6gt0NsUtFw\nIO0nCJm2wJO8A2knj4uK51N7PAuRHm9ZbLoulunzZdFLqijtMow9XnEtkF5Vp3yAlATRlaQlDj/y\nLfUg7fNZZVMTJWFWgujCWKVtqAfRjewPJoWjM4S0H85WRwqSudkeH3GGO7a17eiJrOiFuRLO4WnQ\ne9pHioodqqedEsUupF35e7TCQhxIaZcWpd25eKgE+pnmtGdIPLfR5qQB3Em7raedQ7gasJwgTBkB\nA2nfOPiOARtw/cA3iM702FnSA2n3VNp/8j2PYH+WZ/h8089/MOSmDegBehjmJl1/BtJ+glAnmjTS\nwFVpJ/b4gqzHhDCxAIRYWSyDkGHnACja054/fzKpSHuMAGF5WgFEENLgpLSnJqVdDaLrNT3ewR6f\n0B7tkqzrQXS9Ku1piyA6qrRXf9sq+9rTucHWN7sGALhtSzvmelXa6/u7c3o8r4e8hbbHZy1JeyZ0\npV0NyxsV57c/aTe7kjop7ayutPNQQXS2fetK2i3Oowhh20vMSvvQ075pMFqmN2fNPIDAd1LA6pR2\nvyC6py5XAsbedCg0rjvq9vhj2pAeMJD2E4Tc0k2C6KjS7po0rARAFUo7GafmmkK/DEr6O1WwO6TH\nl4vlEZnTPpL+vdj6YrltejwNeavS46k9PlRPuzCTdgcSp9i9S8Ksz2nPRNCwKorDeYZxhyC6M3G1\n//dXqLSniZ203zzWSHqPQXS+9vhoSRBdjDD2eOrYadumk4dA6j3tJOGe5b/zPX9okJtQXr+90r64\n1lLXAgsQRKf391MEIO0hFQo52ONPBEzq62CP30z4TgowkvYehAC9uNC2GPnKW04pPw/H83qjNvJt\nUNoHXI/QF7tUHXayx2uEICrUrXhUvY5tAdgGlPTSxbJrEJ0aAFUc4ppCPPXsddZtyKrF1+G1aXq8\nSWlHinmAnvYsE+CsW3p8SuzebGSyx+efc18W+cN5ungPAM5BdKfj6jmrVNrFEtJ+Q7S6nnbbnHbX\n4gqXdhU7FGmnx58SiOlANFMhlPT4emEh/53vdiqBlnQb4VaYUwqYC6W9Iu08UHq81R7vch2Ctr/p\n/YFJ+9DTfjJgDKIbSM5Gwuyq8Aui64W01+Z2tzsez+2orXnnLxta4QasDfT9u0nXn4G0nyDUe9qr\nC5HTSCiyCM0kQxTlqc+KPT6Agigt/a6uaqFC+svna2TTN6BM7yVV7PEuc9qN9vjqNSaBlPbMNmva\nibRXRJOVSraRtPfjPcp72mkQnZvSfpoo7QcrnNUuEkMQ3XwfAHB6DYLoXL+4aqRdG/kWPD2+Zb/4\nPE0XhSgBlieyawn3+ePC2eMBhkxWKffCibQbguhY2HwAveWJwnlO+1J7fOdNM2zPQNpPAsyW6c1Z\nNA+o4NvTbjou+nDv+Y4A04WJTzy3571NA/qD8Rq0IcR9IO0nCPqCXlWPHA5ospjPwPM0cQDxuCJT\nNtWmDdSRbR3s8cpiuVTa1dRz37ATIaWiYIu2M+9N6fGx2tMeghwZ1S24kaOMkFA+Kkm7+jkC/ZH2\noyTDBK72+Oqz241IT/tsPZT2HalW5sXKlXbprIDQ3ArGR/WRbwGcFcxSPHRp05nPybmDeq94HCg9\nXr8OCfJ1aSuGKaB/i6Wn3bcwp7c8qe/vRoht0zTCK+31z0ykA2nfNBjt8QNp30iYyFEbwm06VoDw\nfe36GqVtEJ3+/Ieevea9TQP6wyaPnRxI+wmC0GaLy5a2VEVpR4SoJO0xGbckM/+KljBb+J37cpXF\ndl1pHwdQ2pcVQFz6+hkl7dxkj+9XaXcprggjaSefIwujaNpwMEsxZu2D6ChpX6XSLk2z12e50h4l\nB8rdh0f92OtsI8CiFkngypz2KFKt58w/8RxQw+7aTrFI5tUxkZVfYZwG0YVPjwePIEGVdofjSskG\nqNvj4wD2eCGqHn6AFDEA5552WyhlHLyn3UDae8p2GHB8GOzxJwe++9r22BBrn2Xv03aNWiPtg9K+\n1jC1bG7KNWgg7ScIdaJJbO0uhJgsVDNwRDxfxLKaEhcuAEqx8DsH0ZHFsqmnHWF62hV7PGuXHm9W\n2sPPaaekndp7Xfa3JKQ9Gm+XNxb3LVK6e1LaD+f6yDdHpZ2TnvYVBtHJ1KC0z4uKfEHeSxxNDY8N\ngFRIRIagtK72eFZLZg9h6VYnGijZGi72+KQ6dxbTJUxz2n1JOz2PGasKBLCPpaRQineLIDptTnuA\nnnZazMyUnBK364dNaQ8dRCcMrp9Bad88GOe0b8Z6eYAG30kBNvUztBCgk7j2Srtmj392IO3rDGNA\n4qC0D7jekGkLelXhamePF+DgBWlXlTjhP2fTEgDFIZ2sV8aeeF7ZWyMmMZ35KTy6a6F1qJ9C2stx\nauHntFPSnqBdqJ8kdu9F+r7BHt+X0t5q5Bv57HYIaT9YYRAdUpPSfk39v7x7Zuh/DwARwh5PyV5k\nSI8PHJ6mulRcSHt9pCPdxjHLAEj/YpI2so0q7S72eGlS2ln1lRsFKIDoxcOMhSPtPHBPu6nQIQel\nfeMwKO0nB76TAjLL9T70msJ3brd+nX7i0iEOVigIDGgHYzFpQ65BA2k/Qajb41sSTbLoSsEX9ni9\nn3Tm0u/p+D6q0i7cKvbCoLQDSEnfuS9p0oPo2lp8jfb4Hka+UXWLknbrmCgKMpYunpiU9vw1+kuP\nz9T0eEelfZtXROBwhfZ4Jiz2eCEAzR7fF2m3BdFFEE6neP7Y5T3tvhkGet9925Fv6bw6LrMFaeca\nIfbvF5da+rsgry8cPgNBroO8/AwDK+15OwRx09BATMeF6VKlPeBCxzTyzZa5MeD6xSYvmAeo8E2P\nt11GQ5N23zTxVNtQKYFHXti3PHrAcUPfX8DmzGofSPsJQm1Br4xTa5keD44y401d1Gf+Srs0k3YO\n6VYhVUh/1eOZonqt+dyPNAltPnLrmfdNPe2hgujItmT66d6wnYzYvUdG0t6v0j5LhWqPd+xp3yZ9\n8AcrDKJjxp72a4sEeQoaphYSejGpBIdoobTTnvZY6cMeIcMsCGmvtkUvzDUhJW0bKVWWteKh/8g3\nWlhQg+ikgz2eklRu6GkPYo8XUgmiU5R25znt5DpG3AScBbbHm5T269gen2RhixqbApN6GjoNfMB6\nwLSv21wzTEF2QP9BdL7p8QCwdzS4hNYVQxDdgI2AHlIlo7ZKe7XAEuDgC6U97BgjaVHaOaTbxVaz\ntZbIyKJ+7ql0Lu9pbxdEt7DWx30E0amhXbSvvck+y4jSPl6QdmJD7rmnPc2EGkTnqLRvsWNS2k2k\nfb5vIe399LTrbpoS7ueOUAg1Z9xgj/f78ltqj3dR2mdViF/KyDGhbacvIVYC2jjPx8sVyFwIMSXt\nkaWnPbA9XnQg7UrxkRS/ohaFHhcYg+h6mqLQNx59cR+v/+F78Zd/+D04f/nwuDdnreCrvg64fmDM\nL2gz8u06UdpNBeC+puYM8Mcmu30G0n6CULd0twx5SyuiO5MjYo8Pq7Sr843VEUltlXYQ6y1d0Cae\nSvuymfdt7fEL0q4k3GfIhDTafNqA2ngF1DCtpkU9J3bvydZObRtHPafHJ5lsMfKtIhsTdjw97dzU\nnzvbq/WzA0DSE2lfao93Oneqz24uI3B9BjrzV7D1vnvRMjwtI1kLSvBapCrtYdPjY0gaROfQAkQT\n5nk5YYMUESPmv416q0HbzxLQ7PHkHItc25EcYVTaW9rj733oefzjX70ff/LYpVCb1Qk/fs/DePHa\nDM/vzfDPf+Njx7ot6wZzT/sxbMiA3uG7r21Ke4gQXgrfIDrTdXog7euLIYhuwEagbo9vqbQT0n6E\nSRVEp9njvZVXJQhLLSy49bTTkW9koU3+3vnMjzQJAVjH57mQdkLwFkF0mtIO+KvYtGdUgCsW36bt\njET1GU227Pb4vr68kky4B9GR303Ic1aVHi+lRCQNSvv0ai05HgCSpB97vHVOO5PW0B8FyoSICJwh\nuIJdn2JBznGHcycj9vjMorSPAmynmo2hj3xzGZFJlfZ+Rr5lUi7m0gNaEcOxiS8ihTFJziMeuKcd\nhqKWdJwlD+TXg+9550dx90efwTe89QPHarl+zydeWNz+w0cuHNt2tMFDz+3hG9/2QbzpXR/1LgYv\nwzCn/eTAtK/b9bSvZuSbbxCdaY0z7ynLZ4A/TPtrUNoHXHfQiSZVppx62slieYrxYuRbLYjOc5ya\nLYiuiz2ebhsl7UniR9pTIbxaDdQgujI9Xg2iA+DtWlCVdq4q7Q1KXEQW2dvbu8Wd9fT4Pkm7exBd\npbRTor+qnvZZqm1rielVYHqldnfWI2k3Ke2AW+I5PXdSRGCM1RRs7yA6qU2xiNqRdkFcCoKMrdSP\nTf/0eDWITh351vzaVKmPFkF0FWnnhT3eh3z6uhYAIFKKpKo9vvc57S162g9mKa6SPtJPXzhY8uh+\ncccNW8rPfbmNQuG5q1P8jZ/5I/zxoxfx7o+cx+99/Pne3mtIjz85MCnlba5ntuMieHp85muPrz/e\nd4rKgP6wydeggbSfIOiWbkVpd5nTnlS9e1OMzenxLKzSXguicznxlF7UapFMXyv1tCcLIRAxEqbV\nJgFbSsV6vvj8YjWIDvBX2oWinPJW9viYbOPWdl1pH/ccRJdkskUQXfW7EVG8D1bU034wSxWFX8He\n07W70mMg7S6WbmjHS8RYXcEOnB7f9jpElXaFtI+2Fze32Nz/uNR62qk93kRAayDnV6W0qwUQwO8c\nX9bTzjr0tNPzKCpadEJBGIoI7ZR2dVv+9InL3tvUFfrn8uAzV49pS5bj0Rf38e9+5yH87bd9EFNS\nAH7k+f6Sr81z2jdjwTxAhS85WhlpH3raTxQ2+Ro0kPYThPpiud3cbmqPn8pxf+nxwmyPd+9pp0F0\nVGmvFvipp9Keab3iUNLjGxbLJHwp7xsuFvRR3R7vrbTTBGywVvb4GBWx3NkplXZDenyf9vgOQXSU\ntB+uqKe9Np6O4spTtbsy00z3AMgsQXSAW+K5rrRzjtrItxD2+BjmwpzLdUiQqQaCHhMxIe2Y+1ss\nhXqtlKxlEB35LCNTT3uxn3xGJmZCdS1497TH/SntMBQ6ZIsgOn2RfP+Tx0PapZR4bk/NRDnOAoIN\n+7MU3/BzH8DPvvdRPKa5Ep7qMTzPmCi+ISrXABXeI98sjw29ptDbQQbSvtkwtf8MSvuA6w56ejwl\nQcxlgZdUqc1TjEl6vBYA5Xkx44rCpfa0O30haArZAoR8ZN5Ku9orTt+ncWwVscYniDGOi+eSOe2T\nRU+7H+mkc9plC6VdSomxrBbUC9LOI6Do7Y1YThD7VdpdSXtFNqhD4GBFPe0H81RzBRD77NU6aZd9\nkfZl9vgOpJ0xtSA1CmCPr+VBKPZ4h1FqNtJOlPZtzLyvQ0zqPe0tR75Re3xct8eXCrfP+aO7p0Tb\nnBJAKaCoSrtjhogjTJ9Z2552iuMiyntHqaJaA8D9T9ZbYI4b//WBZ3HxwHydefJSf6TdVIQa0uM3\nE8b8go1U2uuPH3ra1xeD0j5gI6CrMnrIWyM00m6yx4+Qeid/Kou7iKbHS7icd8ySHi+jcEq71MPu\niILWGABFSPscI0wWpJ32ZeeLWX1x2BZC2046tmrZon6aCMXuPRoX28aYqmoj9VIKlyEPomtvj6cB\neqtS2g9mmVpgOPWS6rZBae9r1FUmJDgz7w+npG5daWes5qTx3d95eJrtOuTQ007OXbnEHu/dc6ik\nx6tz2l2C6OgUjGgx8o2S9vxz9CLtmmtB8HYFEADgCmnfIveLoOqEkbS3SI/XSfsjL+wrPe6rgq6y\nA8B9T1xjAf9BAAAgAElEQVRau1nk77qvuu586+tfjnf+/f9+8fP5Hkm7Mbl5ECU3Eqae9jaXjJUF\n0ek97QGC6AalfX2xyRMsBtJ+giD01GaqUrkEQBHSfiTHxvT4KIDyShebdBudF5EWpZ2R1xKeSqea\nyh6BcfeAN11pn4yK5/IYpYo9Yhk4hPeXV5aqSrtwDKLbn6VacjtRjrW+9nng8SwlkkwtHND2gRqo\nrVeoPe2rWEwfztPFCDwAwOnbqttXn6w/IUt62a6lPe0uK2dSTMgkr6fH9zCmjBJvlyA6qrTDorRv\nYR52igWPFHu8cHhtSf6WeFQq7XROu7/SLqQeRFd9HtKRKcXEUcBGVGn3C8nTYSLtohVpV7dFSuDP\nnlq9wv28gbQ/vzfDM1f9xoiGxKMv7uO+wokQc4Y3vuFV+KKXnUN5CD+7Nw0+VqvEMKf95MC3p902\nei280i60nwOQ9iGIbi0hpRzmtA/YDOhWStZS4ZKEtM+VUUuqfdaXaFLSzghhiJhbejyzpMcr9nhf\npT3TlXZ6KjVsIyEdc8SYxIX6xlgtBd171jQhDpJxJCCfxxK19+BoirhwZWTgiuNBT+nuS2lPM4mR\nogK69bSzbLZwL0jp71ZwQa60k+Pu1K3VbYPSzmWKI98pCwbUWmDo71qGpy2Uds1J4x2OqI0pQ8v0\neHr+SKs9PkAQndLTrtnjG7ZTSmlW2g097T4tMJmA4loQbVueoNnjSWEsYiKsOmHaHg97PAB85Bgs\n8ialHQAefHp9wuj+3/vOL27/D599K24+NcE45rjjbH6OSAk8c6WfIoOxp30g7RsJXxvycdnj22Ys\nGNPjN0W63TDYjqlNuQYNpP0EIfUMohPzylI3A1E9uRpU1VcQHQBIJ6Wdqnjkb4zDKe1Cm3/ezh5P\ng+jiyh4P1OaN+6ohQisupJJup33BfHBY7esE6j7Qw+j6CKKTUmJeC6JzU9qRznBqUu33VSTIH85T\nuz3eUMQZIcOVw/DW3to5TiBcCFKtpx0Ge7zf/k6zZdkaLm4Acu7SlgkSRLfNZt4LP04VaB4rrSVN\nCvE8E0oqO1/Y44nSzvLf+xQ5MyE010KHIDqynWyk2eN7Vtp9etoB4OHnr3ltUxc8b1HUP/nc6rfF\nhvd/6sXF7a9/7UsXt+86V50jT/VkkTcROSnbjQIbsP4QwtyuGCQ9PngQXfj0+KGnfT1hc1EMSvuA\n6w66Pb6t0k572meK0k7UIyYxbzF71wSuKO3qfGSXMC09QGpxkyzwhefILbWnPQKnNvxGe3ylFCaw\nk/YQSrve0+6qtE8Pq7ThhOmknSjtzD9N3ITywqsG0Y0sj4ZK3tIpdibVfj9cwaz2fOQbVdpvsz8Y\nebGjj35c3R6fgow8bBlEly162qtzPYa/PV53Ayj2eJfrELXHx6uxxzPOc0dNgaZWg3kqFj3r+QuU\nI9+q11go7V6kHVpPe7sCiNB64pmWHh+UaJkIegt7/Dytb8uqMisoqNL+WbedXtx+6BgKCDbsT6vP\n9ZW37C5uv/TGncXtvsLobIvjTVk0D8iRWK6B69jTrpPuIT1+c2G9/mxI0XAg7ScI+oKexVSVaT6g\nFXs8VdoZQ0bIcTL3TMbWSLskClfmZPFVba2Lm2SBL7NwpD1X2mmveMNnqQTRkfR4QFXaWeL95aUq\n7ZFC4rDkMziaVQu6lGu2dK2nvY8vr7Iy7hxER1X4dIbd8WqV9oNZqo58o/Z4A2LWj9KuF+ZSUqRx\ns8dX25SC14LoQuxvPRBTKR66fLGS45ZFxGExqghJ8JFvWnp8UxDdPBWIGe2JryvtcaD0eHs+QPP+\nzqS9BSV0EJ1pe3yV9j5aTJpAe9r/ymuq83ydlHbaErQ1qq75LyOkva+xb1ala0MWzQNyWG3Ia5ge\nr79Pm2NRSjnY468j2K4/mzJ2ciDtJwh6aJE6A91hhFFSLVYUpR2AIGpskvqREXXkm2pLdemBVJR2\nC2mnancX0CA6ybjyPo1qYaoF0cWESBOlK7fH+yrt9PkMCSXtYonSflQVaDKmkWXdHt+D0l6qpGPn\nIDpNaR8TpX0FpH06nS1S2wUiYOempY8fIetFadft8Sk5L93s8dW5lyvtCJ5hkGdrWHraHa5DjJL2\nEVXaq3NnG/72eLotLIpbjXzT7fGL64NhQoTPOa4r5XR8nku4aCakWlygoxOR9T7yzbenfXoMpJ0q\n7V/26psX4W6PXTjoLdytLaZkOyhpf+mN/dvj7WSul7cbcEwIUZyxK+3hziNTMFmbYqTt+24g7esJ\n04x2YHOcPgNpP0HIBLRe0orocKc57aSnnW0pv5JktFrqaT2vK+3UHt98oaS2UEZ7SEkysvQsLKBm\njydkuGmxbBv5Bij7ZBIiiE6oBQwXe7yUEkfEHi/0+eg1Ehf+y6t8zREzq4A1aD3tu7SnfQX2+OmM\nFDn4CNg+t/TxMVJcPQo/q123nlMHjFOlmRwvSTmnnRZpWBqgkGSfYuEyepKTghujhRx95JvvnHZt\ndCRNj29KZp+nlnGF5DjdQr7//Xra1RF/smUQXS0UMFbT44OG95g+M5eWjQJGpf0Y7PHP71XH3ytu\n3l2o15mQ+NQL+yvfHhPo57JtU9ovHaEPmGZ3A5sTBDUgR2bbz22UdnJM0OM0pBDgm3Bv+x4xtesM\nOH7Yg+hWvCE9YSDtJwi6lZIpC7R2SnuiKe2SkNbMM+RNId1RrNjjXcZWMSVAivTbK0q7ZxAdtccz\nDpAFfZue9rkkI9+A4EF0CrlgXLPH10n7LM3wN9/6Qfzcez5RPazBHt9HEF258HMf+aYp7eQzPZit\nQGk/qs4NEY0bSfuqguhSEiLoZEWmPe2yVNpVZ4V3EJ22jXQUI3dQh5mgSjsl7RUh2Q5gj1evQ+3m\ntM9SsSDlAKqQPOqkKUIWfSzemUa6247PE1JLjydhfqHt8ZD146+NPd4U/DRdsbKdZAIX9vPrN2PA\nLacneM1Lqr72dbDICyGVY58WhV96rn97vK2wPtjjNwu2nvauSjt1x4Uk7b7jv2xFqEFpX09Y7fEb\ncv0ZSPsJwtLQIQdVhikj31QCRVPa0yScPZ5xdbHsEqZFFTJpUdqXhbC5QFJ7PLhSHGjuaSfp8cvs\n8SxAEJ2WHp9Isz3+8kGuTP7ug8/jTx6/hAklHDpZVkhc1kvFOTHa45cE0fFImWJwhtQZDlagxh1N\nqwWw5BMzad+6YXGzryA6XcWmSrt06WnPlve0jwKkx+fXIXO2hksQHScFt4hY4nUV29c6zbTrEM2t\ncFHalYJTWVQa1ZV2n+3Uc0raKu2Z0JX26vkrUdpbBNGZbI9H89UunF+8Nltc3m/anWAUcSWMbh1I\nu07YOa8KyrecnixI/JXDBNem/RQOTdiUntIBOUIompQQb1PSHpAQm76v2pB227YMpH09YSuybIo9\nPm5+yIBNQZYJRMRKGY2JquJij08r0p5o6iu1x2ee1nO62ORRYUstNtsliC4mSpykPZqEtHPhOadd\ns8ezNvb4dFl6PFWx/XvalSIHi6DsmYKg/ec/exrf+66P4s4btvGKm/Ok4TEjM6bHaitELT2+hy+v\nqqfdMYgOyEnbPP+bzoyqv3sVPe1HR0S1ikbA9g31B934CuCZ+wHk6uaVntLjY0ZJOwmic0qP13va\nWfAMg0xPj2858o2T85tblPYtFpq0t+tpn6UCW4wUvkZ1pX1hj/cm7bRISUP9HOzxQqpTD2J15FtQ\notXDnPZV97TTfvbbzubH3mtuO7O47xNrQNrpZ0L72QGAMYabdsd4phhbtzdNcXprSTG0A4b0+JMB\naxtER3t8X0q70R7fohhpI+cDaV9PpBvu9BmU9hMEQRZNAgx8VH1ZR2hePNH0+CzSetqJop0lfoRY\nD6JTFssOi/pYkvcn28lH1IabWgMrXKCPUqM97c32eKq0j7T0eD2IznNOO9lOxriSJl5uxy/80ePI\nhMSTlw7xBw+/uHjvEjeerZQkAHW7dB8j3xbp8Y72eEAh9Wfi6u9eRU/7jPS0s3iSk7RYK3ace/ni\nZsz6CaLTi1rCY+RbspjTru7vVEgvMqcTTWpxd2nTiYhDRCXtahCd7zgwXWkXrF16vFFp185vQE36\nbgs9w0BxozgsUvQCitrTLoP2AZqui8xgmbfBZI9fdXr8i9eq75ZbT+f78jVEaX94DUg7/Uy2NdIO\nqES+j6LHkB5/MhCiOENbKbbJxJeQpN0UJBekp32Y076WCDHVYJ0xkPYTBGqPleAY0cWyUxAd6dsl\nvY/5C1SLxayF5dEERWnXRr65qIWRorRXfyPTerGnPl8MehAdtfg2LURrc9qpPT50EB0tgHBjevxH\nn7pSe96t29UFLh7r+7r/kW9Jkb69cIYwDkQNxiBCiE5H1TatQmmfTatzY5EVoVvkCWkfIcPVHnra\nJS3MMZVoNlm68ycZ5rSTglwZDGjrZ3SBTjQZuZbEDiQuktX5HY/sI9+OksxrzrgaaKna47OGY36W\nZhppL7ZTC8sDwtrjaWHLSWnPhBr2SJ4fMRGWaBmOGadCUoEkFfjr/P34ldGb8ZU8d6xkQq5U8VJJ\ne/5ZvfymHZQO9Of2pseSaE+hKu31JR4l7X0E+dmK4QNn3yyESY+vbm+TYzXknHaT8hqCtPchVgzw\nh/W4HEj7gOsNlEwLFiGKSb+3g8JF0+OlprTThb3wTI9nSkiVOvJNOCzQYgtp1xVDn8WV3iseEdLO\nmyyfND1extb0+PD2eG5Mj6c9mSW+6I6KANVs6Vp6fF/2eGXueZPKDiitBadG1XNXobTPZxVpXzg6\naqT9FYubI6S99JPqWQuSFGmESzFNn9POUSvSAJ6zxbUpFnxcHWsujh+qtMe0dYMQ4m02h5R+iz9F\naY9U0t70Wc5rQXTFdpJzqfy9VxBdraedFg8dRr5pGQb0Oh7aHm9U2lvY4+X8AD8x/g/4kuhBvH38\nI4v7V6m2U9J+S0Ha44jjtjPVcfjs1WnteavE0RJ7PKD2Dq9Uad+QRfOq8cK1qVfxsS/Y9mebbaVK\n+05PSrvJxt+qp92S2TPY49cTmz69YiDtJwhSs3THZAEZOyyWWVotRqSutFPS7hnyxms97SSZ3kGZ\nGSn2eDtp91EZqKIpWQRGFsuRbPj7UzrybUl6fIggOqGqhab0eNOXz+ffRsjQkiC6cQA3gAlpJlVr\n/LJxb4vHUKW9Op77VtqllEiIPX5h2V6itMfIvOedmyA0B4gypszFTePQ0w7YZ9e6IBNCI+3dlXYl\nb4Fck7aQXwN8SAklvZyPNKW9eU57mQ6fb1tJ2qttDGGP11sN6PXDZea9JNfqDFE1Tx49BNF59rSP\nj17QXxAAMF3h2LcX9+ukHQDuPFft16cv9zNKzRX0eDKSdqq090Dah572cPjJ9zyCv/Tm9+Ab3/ah\ntSPu1t7hVvb46jYtJs0CEmLv9HjL3zmQ9vWE/bhc8Yb0hIG0nyBQdUiAIx61U9pZRkm7PZws9Q6i\nU4mmYo9vGURH+1x1hdirX5xuB4uU3lreaI/X57Tb7fHBlXZDerxOGrZHEV59EyHJNaWdkPaeguiS\nTKgBWfqseBPIdu4Q0t53evzBPFOKXrw8N3TSfqOqtNu+XLxAlXYeKQWvtvb4FOXIt+q8KZPGfRYs\nmQAiYsmm7Rcjh+JhTIpiqj2eknb/cWqcbEvdHt9SaR8ZlPbSHu9xHcpHvpl72p1GvqVqOwT9GyNk\nQRc6xrDTFvZ4PttTfi5dH8emtJ8ipP2G6th75spxk/Zjtsdv+MilVeLH7nkYAPCBT1/Eg8/sNTx6\ntbAWZ1rsZkVp72lOu6ldo2sQHT2fhp729YT9uNyM/TWQ9hMEPfGc9rTHDiocJ+nx0PqceUTT4/3s\n8ZFiS40hWyhcABATJY6OtVPJZuY1LkgPoouJEtxsj6c97ZGWHq/b48PNaWc8Mtrjy0Xe7biI77/p\nD/DWr73JHKJVQiFH814qzvNMtAuhAxSlfZeT9Pie57TvHSVqyn1JnLimcp26bXEzRma1cflAd4BI\n1m5cokLaZQSmKe1B7PFaT3s8qezxMd3nV88DH/qPwOXHleePKGmfWOzxhdLuQ0r0Oe1ScRM1k/am\nnvZKafdr0+FkIgi93jVehwAIcq1OESuOqQgirLpnKCK0CaIbzy8rP5duiuO2xwOq0n5+jUi7OYiu\nuiZ45bpYMCjt/WC/5++xtggx2o++hjLyzXPdY3uPEl3t8bvEwj8o7esJmwtwU4LohpFvJwiSLOIE\nixCT9PiYiTwphthpdXAyqoyNVNJOQ94yzznt1NapB9GlDqR9RJT2eEK2U+9p9/lioItNLYiu0bVA\n0+PlSLPHq+nSPtZZAJBkOxkz2+PLRe/Pj9+Czzl4AvjQvcAXfiPZJo0wBw7LMyHNpDJ2zs0eX23X\nNk8B5PvkoGd7/N40MRcY5ofqAyP1fEs9AxtNUHramTp5Qbgo7Ypd2jSnPQUgvdwV+iz5iJJ2Wjz8\ntW8Cnv0z4MM/D/yjDyJvsC+U9uKSMLL0tJcqdtcEeSklOM3W4DEEGZ8nG1qAZklqscfT87sc+db9\ns5RkPwio16FGxw/UVqaM1e3xIdUJYzBeC6V9PLuk/LyDGfZwyvsa2QZW0n5DdQxfT/b4PloL7Ep7\n8Lc6UbCvzI4H9jnt7juaPravOe2+Pe2UnO9MIlw8qN8/YH2w6UXDQWk/QZBaeNooipBKosQtW4iK\nDLxQuIRk4BqRo4vF1ENpl1KC017SKG7VSwoAI6K0j6gjQCMfXj3tmtIekXFyUdNiWZvTPo7MQXQT\nlnjbFxVLNNeC6EQCKSWOkgwTzPE5/In8/ouPAM89YNwmALXCQh+92Ymn0r7DVxdEt3eUavPki2Nh\nfqA+kDFljjZ6IO1UKc/t8dz4O/vzq89qYY8n1nDOcsLtZ4/XlPbxNoTMl6QxE/k2pLOcsAPAhU8C\nB/kowlQ7LqxBdPBLZte3kXFVaZcNLUDZvGolStm4KoYa5rT7KMVC0kIsB4/VsZZNqPW0k3YKDhF0\noWMe+eb+t0/m6pSL7aIw04fF2wQppdLTfvMpS0/7Fa1Yt2I0jXyj5KifnnbztWGwx/uB8/Wi7SHI\nESXUO6Rd06eQqcM06aQzaSfbOKTHryeGOe0OYIz9O8bYexhjTzHGjhhjlxhj9zPG/hVj7CbLc17P\nGPuvxWMPGWMPMMa+mzFW/5apnvM1jLH3MsauMsb2GWMfYox9S4i/4SRAsc4iAudMUV6zZQtRMqN9\nijEm2mKA2uOFR097JiQipipclHg0Ku1SYkx6SUdbS5R2nwULJe08QhS1IO1koTxDrH6Wij0+9V9U\nCdW1oIx8y+aYpQJSAjdCmy/82PvINmn5BZR4sHkvSnuip8e3VNopsfNtMWjC3lFiTrq/84ur+3Zv\nyf+nxC/zayMxoaa0Kz3t7ezxC6UdqJ07funxKiEejUbKcSmzOXCkWqFx9TyAsm3CMlXAEETX9fyp\nzS/nEQQpuDTtO0HGY6bcnA8xZhk4hNd1iJJuySJEcYvrEFSlPbfHq0p7yHUON9njWwTRTRL1mNgJ\nEDbYBnvT6rjfGUfYnVTnMu1pf3qN7PH69zTQfxCdre1nU5Su48J6UXZ1f8akoNCmOJMpSjvtFw+3\npjAdd+1IOyksTKgbYDie1xHDnHY3fA+AXQD3APgJAL8CIAXwQwAeYIy9lD6YMfbXAbwPwJcD+E0A\nPwNgDODHAfya6Q0YY28EcDeAzwXwywDeBuAOAO9gjL0l0N+x0dCVdqBQVwoky0a1EdJ+hLEangYo\nCo/I0s5f0KbFMrX4NirtZDE9kzEmpAVA7831WrAoAW8RojHNB2g7p91mj597L0jVnnaeL8wX21EV\nLm5kWsjN4cXq9u7N6u80pb2fIDqpkTMH0j7arbYrrYoQIWe+mnD1KMGYWqHLbf2yN+Vj3rbPAX/7\nnQDUkVxyFUo7DXFsGUSXIK66ZbRzx0tpl2rieRTFigMkS2YG0v4UgFyBUT7rWCfE+QaPWYYIWWcV\nVmhj6cAiQCHtTUp7db3MOCksMBb0HFfs8SxSrsMupF3SIDqmKu1RYKXdmB7fQmnfTlSlfdU97TZr\nPKCS9mevTI+VoDb1tE96DqLbdHvqcYEtaV08DtA2iDFZw7RLj6ekvZ+Rb6bvqjaFBfr8oad9/WFz\nfm6K0h6qp/2MlLI2nJQx9mYA3w/gnwP4R8V9Z5AT7gzAX5FS3lfc/y8A3Avg6xljf0tK+WvkdV4O\n4C0ALgF4nZTy8eL+/wPAhwG8iTH2binlBwL9PRsJ2tNeKnBUaU/SBNu1ZxVINaU9Vus9jKvW82mS\nKUqEK/LFMllI8Ejps28k7WQs3Qwj5cuklh7vYcESGmmPlZ72tunxlLRXi+4JEv8FKV24R6NaevzR\ngrRrSjvFq/8n9Welp72fILpEH5nlYo8nxYU8tOpOAGGtdibkPe0GV8DOjcA//khOhMv7yHnChF/2\ngwlMCaKLlTntzEtpV88d2+xaF+hKO3isFJOSeYL4UO1fdlLaGQNGO0CStyVsYd75/EmFUHrawWPV\nHt9A2gUpcmam9pLiOuWdW6Fd0+M2bTpQHQP5yLfqWsQDj3xTPs/yvhY97Tpp32EzQK5OabclxwO5\n5fym3TEuHsyRCokXrk1x+1nrt2mvaEqPV3rae3AhDenxYVAnv+v1+dE2iFHEgWLd1qY2Q//GvtLj\nTUUE2zFqgmKPJ60lA2lvxlOXDvFT9z6C133GjfiG/+6lzU8IgEFpd4CJsBd4V/H/q8l9Xw/gFgC/\nVhJ28ho/WPz4D7XX+XYAEwA/XRL24jmXAfyb4sd/0GnjTxCESWmnM9CXKu3VLp7KOmmntsoYWecA\nqFSb4QwtATtrulCSfvGZToipxZd5BtHRxSbniEgSf2MQHen5T2SsjXyrVLgx8+u7z7eTjq2KtfT4\n+eL1b4RlnMydrwXO3qXepwR+9RNEl+jkzMUef+rWxc3x9MLidh8LU4paTzslaZwr284I+UUPpF1R\n7zlX0+Od5rTrI9/q9vgYmV8QncFNk5CQtySZAkc6ac+V9nmqZR3oxwUZAbftoWLXlHYeQVK3R4M9\nXiYWpR2oTV/wSo/XSHtbpZ2m4Gcs1pR2GYxoSSmNI+japMfvplpP+4qV9guWGe0l1mVW+5FC2k32\neJIe37PSTsXhDVkzrww6KVw3jkjbIEYkl6eVPZ4cFPRYTYUMRrJCBtFRIWog7c34nnf+Gd5133n8\n03c/gHsfen4l72mf074ZF6C+g+i+tvifpFrhDcX/v2N4/PsAHAJ4PWOMfisue85/0x4zwAJ9HBQA\nReFaTtqrcB2T0q7Ocu7ep1lfLOtBdA2LPLJYnuk2fq0v14sQa+PzlCT+hoUoVbfmLMYoIisbTcX2\nXZBSdZVFsZYenzYr7Z/9tfX7NHtvP0q7bB9EV/aNA4iPKnv/apR2uq0j+4PpHO0GtbYTFBKnnjtt\n7fG50l78QJV2lnqF8NSUdhYp16F0brLH50r7LBWY2AokQK60F9hi887neD7/nLppuJJHgIZ9J0mR\nU9SUdnVWu5dSnKmkfUSVdoeZ9zQ0VDCtp51lwYhWJqRRaW8TRLebXVV+DjHWrw2W2eOB9elrp86N\n4wiiU2zTUTfb9IC6GmwjI8cFSs4nHe3x9G+MOFOckaHa7kzrk1Yj3wjp31aUdhl2JOaG4dEX93Hf\nE9X3+L/8zw/2IvDosGZqbMiuCjryjTH2fQBOATgL4HUAvhQ5Yf9h8rDXFP8/rD9fSpkyxh4D8DkA\nPhPAJxye8yxj7ADAXYyxHSnl0uhWxtifWn71WcuetxEwKO2C1G2SZQFyxHY+xVi1nQOK7TdG2llp\nN/W0o83YKqq0yxF2LPb4MTI/WypdbOoz7xuUdpHMFtRZ8rHaq0bT45F4Wz+ZprTPoVp8p02k/bOW\nk/atIj1eCBk03bZTEB2xx/OjSmmfpVmu9PXUE7h3lOB2ZVuXFBgidTRg6M9NOS55DJB2iKbZ4gCU\n83wuR9VnFrKnncwWl2C1rIU0mQE1e3zZ066PUlumYs9w6JEez/XrkOKSaCoeVp+jrAU5qrPa9zwW\nMnohlhPS3pitASCbVud9wie99bSneqGmgFPLRoFTmTk9fmX2+H27PR4A7iCk/fzaKO11XWZLCaLr\nY0579ZrjiC8yRQZ7fDvohdF1K3qE6GmnanrMGSYRXxC7WSKMTpG28A6iS9XjeRSxRd90kkmM4/XK\nGjhuTJMML16b4bfuf1q5//zlI/zSB5/Ad3zpK3p9/023x4ee0/59AF5Cfv4dAN8qpXyR3He2+F8t\nm1co77+h5XN2i8cd77yVNQbtaS8XZymLF61Sy5V20tMu60F0VIWKWebVS1oLgCJkqzE9npKOZfb4\nkEF0nCtK+6hB4RIpIe16wJqWHp9kEmkmEEcdTTF0Tns0UoIHZTbH0Tz/rGvp8UBOLm5+leH++qzp\nRAhMuP8XbIkkXdK7bANR2vnBBcSc5TY7mS8wFEdDQOxNE3yGY/89U4pbWfDPTRkjxyOFtDulx5Oi\n1xTjSmnXMit8VBBq4ReMI4J2HZrX0+PFlfPgAJJ5tX0ZOCL9s4s1e3zXILpa8TDWxvU1JP9nS5T2\nkTr2zS+ITutpj1u06QCQR1VbzDQ6rVzHo4A97amQxmKmsz1eZNgV6jWqSo9fjQJ5/Sjty+3xWz0H\n0VGlaxxzFLtp7UjnukMfVdamD3sVyCyOijaXDKPSXhwvsywDsMS15ojEk7SnSu8+wyjiSIp1aJKJ\nuoB1gjFNMvyPP/YH1qLlf/rDT/dO2m3nyaYE0QU92qSUt0kpGYDbAPyvyNXy+xljX7z8mQrKZWKb\nT9j5OVLK15r+AXioxftdl6CqjChIu6A97Y4j345MSjsZ+RZD4HDeLRlbCNQVLmrxbSTty3ra62F5\nXSE1pT3SlPZlVT1JtrGWik7t8QURnHooccpIJR7nFthyOzIaRKf1tE/OAv/b280vatjG0LanREhz\nIhmWcckAACAASURBVPsyENKOgxeVfd9ngvzeUeruCtCOQZuVqyv0IDq1tcTheNdGO9qD6HxCHAlp\nL+rGGateP01ntZ52fnQRd3/4ESRk/nnCDAs6ao/3CqKT4EwtHtI8gsZRZURpR7RkZGLhpulss9RG\nT1J7fOxgjxfTqg4+j0/1FkSXZWZ7PHe1xx9dAde+3tcpPR4Abj+7ZXzsqtFE2pUgur7t8TF1yW3G\nonlV0FOwszXz9yo97URtbkOO6PWlZo8P9J2dmuzxrdLj1d592r8/9LWr+MiTl2uEnYb3PXN12vsI\n3mzoaW8PKeXzUsrfBPA/A7gJwP9Nfl2uEs7WnpjjjPa4Ns+xpGkNAKDY40ulPaM97Uvt8cvT43V7\nfNfFQK2XlGtBdA32eDpqaYaRqlDrQXReSrtaWGDKa2eLSqzxqaSPtE7a6yq2jxrCNLt0SoiOTOfm\nnvZv/i/AP3sceM1Xm190RO29hdIeeEHRKYhuh4ymO7yALbKQmPW4sN+bJpjAscBAlMw+SLti247U\nNPDGghegtsHIkTGIbozUrwii5EGUgZjayDfdHg/g3//G/4eUkPbUpMLQIDo260zohJCIoZ/jVGlf\n3tPOyFhHObKT9gmbQ8ju/Zv6RJBIscc3/+1sWn1lzuLTvdnjEyEQGWrqzj3thxdqd+2w9SLtZ3eq\n4+PqYQ95FY5o09PeB2nPbKR9M9bMK4NONtdZaVeC6Nqo2JlK2id9kHbD9kjpvp10O3TS3se42+sZ\nVwzXvb/xRXfiVnK9vLDf4FLzhG0tuilFw159HVLKJwB8HMDnMMbKFfUni///gv54xlgM4BXIZ7x/\nmvxq2XNuR26NP9/Uz37SIYkVUXLDYjl1S4+fYYyJ3iunkJHu6fFCSERMtaWyFkp7MqtIe8I08qQR\nD68Fi6a0gzGkklRgk2UFkGrxx3QyqgTR5fvLZzuVnvZIVdohkoV9WLHH796skL0aYrXvHkDw6mmS\nCo0IO9jj4zGwVdT1pMAtcXU56FVpnyY4x/arO7bP2R8cMIXdBEUBZjEYIWHCxR6vKe2LU09pLcn8\nEs+12eKATtrnwNGV2vPuYBeRzghp54biiK60z7t9vvWwPK58Bk3j+hh10+h997Fqjwe6W7ylHog5\ndm/TAQA2r0h7OjqlBNHlSnunzaoh050L5Xs4k/aLtbu2y89uVUF0DenxN2xXx8fVo+Mk7e5Kez9B\ndGoPcIlNsaeuCvX0+PX6/Oj+7Lqfs2VKe6DvRnswmdt20v0wjjnGpM0utFhxvePyocohPvPmXXzn\nV74Kt56prpcv7NmGjYWB7TzZlOvPKpox7ij+L78d7i3+/yrDY78cwA6AP5ZSUn/Zsud8tfaYATZQ\nddhgj09dlXY5xjjSFgMR7YXMOqvDtcAiptrjswbikcyXkXbdHh9GLSwXuin9LBO7PVKZ8ayT0Yj2\ntOeP81lYcamS9kyzx5fj0M5RpX3npuUvqrgBCgt/4L7SVMj2SjugWORfElV/U9/2eGVkHlX8dSgp\n7Fn4RGA9iM5HabfY42PPUYSSEF65IO3V6+ekva6038EuKudVZrTHhxmnlkk9iC4GI9e4JtLOSU87\nDZ7Lt7F+/nR1gjAtp2REXjtG2mi75zNC2g1Keyh1IsmEMYiOu/a0H9SV9m3kn3HfIx2BfGTdpYNq\nQXrjbv16dANR2q8c9asmLQP9vtgeNwXR9au0d1VgB9QJ4bqlx4cIoqOP7c0e72mXpqR9FDGMyDb6\nTFHZRFCl/X//8s/Ee970Fbjzhm3cerr6Xnqh59YhmyNlU64/3qSdMfZZjLHbDPdzxtibAdyKnISX\nyUK/DuACgL/FGHsdefwWgP+r+PFntZd7O/J4ijcyxl5OnnMOwPcXP/6c79+y8dBUGQCK8ipa9LTX\n7fG0p727EldTuLSe9qa+XGqPT/X5yAGD6OjMYWYYn5eGUNqLnm4vezwd+cZV0o40H4nFIHAO3Uj7\nVk8JznM9Pd5FaQeA3WpW+0s4CdnqyUIrhMTeNMHNNBNgdwlp186TXu3xPAIj7yea+rABQ0978UNP\nIY7SpLSn9SA6ALiTvYiUnt96UQ5QCPI2697TLvTgNB4px2AzaSfneM0eTwoLzE9pF1pPO6fFUyYb\ngzsjqrRPzqgj3yCCqRO163oBd3u8QWln/u1DrpgmVavAJOb1IFYAZ7cJaV8Te7xpO7d6ntMegswN\nuA6U9szsqPAKoiOvE6rQblXanUk7TbgfetqX4QpR2m/YqSYj0WkbvZN2yz7ZFKU9RHr8VwH4EcbY\n+wA8CuAi8gT5r0AeRPccgL9XPlhKuccY+3vIyft7GWO/BuASgP8F+Wi3XwfwTvoGUsrHGGP/BMBP\nAriPMfZOAHMAXw/gLgA/KqX8QIC/ZaOhhqflFx5J7bNLSbuqwNXt8eqc9s72+JrCpQXRNVSbly7q\nNeIRLIguMs28t1uA6IKfLbHOTgIo7XRRzKIRBI9RfrxS5EF0Z3GAqBjBhcnZ5XPGe9hGE5JUtA+i\nAxTCfDMh7X0p7fvzFFJqQX7LSHukZj+E/tKndmPWsrUEgNbTblbaxx4jHfPtoH3YxehJcv0Q6dTY\n034nu4ir1w4WP2e8WWnvGoiZ6iPfWAQeU6V9+etSpb1O2uvtJV3VYiV9vWjTmcsIY5a/XjqfYxTb\nv+bjpCrWiZGqtIe0xye+QXSmnvYVBtFdm1XXotNb5s9zZxwtxkHNUoFpkgUZWdUWx22Pz/T0+PL+\nDVk0rwr6d8O69bT3MfJtpUp7B3v8KFZJe58OvusRtFh5jjiPqD2+75BO5biM+KLNYlPqKyFI++8D\n+I8AvgTAFyAf1XaAfKb6LwH4SSmlsgKTUv4WY+wrAPwAgK8DsAXgUwC+t3h87WySUv4UY+xx5GPl\nvhm5S+DjAH5QSvmLAf6OzYfB0k0VLrEsXKlmj9fT4zX7rIfSrihcTAvTarDHZ7NlSjshHizDtOOC\nHtDmC5uU9vmSz5KocDR1HoBiAw9D2jV7PD3ls3w/3UQJ586NzS9qIh2BF38h7PE3E8t6X0F0e0cJ\nAKlmAiy1x6u5CsEXYtq0AEaUU6eRb1p2xWLaotJa4tfTTrejdPpQxw+bXlPOkRK34yIevFZlBwhj\nTztR2jHrPIfa5Phh3L2nPRKV4hCNNdJuCHLsrBZrSjuQX4fGxTU0SWfYxo7xqQAwIqRdbp1VrrVx\nQHu8TWnnDmPpABiLONsL0t7/amx/Wp1XpybmZRNjDGe3x7hQ9L5fPUqOnbTT0DnTfX2My6PXNOrI\n6zwh4YSilh6/ZqTd1gbRpjhDjxXOGMbEGRKKtPsGkyk97RHTeto3hAkGwhWS5UHbhWgQ3YvXVtfT\nPo4r0r4p1x9v0i6l/HMA39nheX8E4K+1fM7dAO5u+14DChjmtMuOI98m+mKEkIMRMlzpuAitL5Zj\nRf2RDanNgpCOTF/UMwbJR4sFd7psLn0TFEWzLIBEi6GDaRnq95FfAh75XeBLvxe4M598yDKqtC9L\nj/frdwU05TUqZk2XH2+Wz7E+p4fQNUFTM/NtDPvlNc/EoqcfQAt7fEXab2TVAIq+KuJ7RynO4ACj\nQtnE+LTSs1yDZo8P/aVP2zYQRWAZcdI4Ke3UHm9Oj/e3x9PrUF1pj45eMD7tJraHq21IOws5pz0C\nI8nskVx+HYpFdR3iI400k6LXlm/Ry1Q8ZDHKYcfpbLmqMUqrz1NOzqhKOws38s3e097dHl+2FvQ5\nGaLE/oyQdovSDuQL1ZK0XzlM8JIzS64FPYGem1uGGdJbsaq0SykXNtYQsM3vHvhNO1yvSnubQp+i\ntEf92ONtxQ7Xz3P5yLf12ifHDWqPP0uCOW+hPe17K1TaY15+Fa5d0asrVhFEN2BNIA2qjCQkQjqS\n9hkMSjtddEN4Ku3qYlly91FLgmynMBA9SRTDZWFxjZCqdRbQx+fNgYOLwG9/N/CJu4Ff+KuL33Gq\nwulKe6T3tMtgQXQ8ipWxVeWc9pvahNABRSEl3/8xE3nwYB/2eEranZX2quhwTq6AtLfpZwc0e3z4\nnnbFWcFjMBoY6UKQEps9Xh2X6GOPlxlV2svrEFHyDyvSPo2rKZ83sj1cO6js8cLUMqGp2D7XIT0Q\nk7eY006Vdq4r7aae9o7Hp+L4Ka7lKWghdnlhcpJW5z7bOqsUlUKOfMv0doMCHFINSLXBEES3Snu8\ni9IO6H3txxNG16S0c220Vuhro5IeP9jjO0P/bsjWrOohLEp7m0IfPVYixpS2y3Dp8ebXcR75pgTR\nDT3ty6DY43erayGdtrHKnvbJBl5/BtJ+gsD0MWXQguiW2uMbetq1ZPaui/paajOLlGT6RqWdzHE2\nKnHkvsyDtKsBb5axVXtPV6piNgcuPppvQlYt5mKdtHOuFEDGSDuPrQL0HucRWKyTdqElxzso7Ywp\nxGOCpB97PCOv2UFpPysoae9nYX9tqiXHN5F2rtrMQycCs1p6PD13HNpBUlsQXTh7PPQ+bKhK++jo\nxcXty9svW9w+h30cHhBl2HR+x7o93kNpZ2rxkJPCUVPqeawU5prT47t+nkq2Bq8XD5vcRFtZVQTh\n22drQXShlPZUCLXticIlIPHqU7W7Fvb4FQTRXaNK+8Se+XEDIe3HMfZNSqlY3rcMQXSAliAf8PMT\nQio5CEN6fHfopHWdlfZJ15528tCIM0yiPnraPZV2fU57D2PpfPGJZ/fw7e/4MH763keOdTsuE9JO\nR2DeqpD2fu3xIRwg64yBtJ8gSKEuQgFVaV9K2pNq5vWRnDSmxwdTuHikEG3WRNpJcUFEBmsiWXin\nDSrUMjDTYpm2GiSzuivgkXuA5AiRyBebcxmBjQ39pppFPlQQHY9HCvFAlqfHq+PKHHraAc3i213R\ntCHR7fF6YJ8NCmmvZn2Htu+XmOqZAOT9jSAFqJhlwe11VAFmmrOCiebjXWqBk8xij/cKoiPbWDl+\nqu2cENK+P7oJezInvTETiI4qxVU2KO1bSDrb4zOBWvGQtrJEDaR9JKqCYDzRSLtxTrv/yDe5sMc7\ntjwB2MqqIgjbvqG3ILrUEkQHoJm0J1Pg4qdqd2+z41HabUF0AHBWGfu2etJOVfNxzMG52fbeVxgd\nVbNizhCR998Ue+qqUFPa1+zz03uHS7TZTEVp14LoQhXabeTc9fNMFUeB1tO+JkF03/GOD+Peh17A\nW37vYXzs/NXmJ/QAKSWuHtH0eLPSfmF/3iuBtrbnDEr7gOsNJqVdVeJc0+NHysVVfx2fOe2ZkIiZ\nRohpmnlDABQlHcJA9BR7ePCedjJrOksU1RIA8MjvKTbPSzijBK8sEKuz2n1UTb2nXSXteYL+jaxl\nTztQKyyEDjSap0INomtKtC9BSPPpjJD2nr5cZ6lo116gkd8+e9oZj9XU/SZyJKVyzM7p9AW9p91n\nDCGxx0uDPX4yq86R/egMLskzi59vZ1UgmZm0V0WwbdZdaU+FqGVrRFRpb/gsR5IG0dlJu3eQo8Ee\nn4G0AM2XuImSKUbF+89lhPFku660B1pcpZYgOgDNx+WLDy3akS7LU4u7t3vK0zBB6Wl3tMdfPYax\nb/S8NPWzl1DD6AKSdm2EFye98qFcGycF11NP+yjqVpyhkwb6mtNu+451PR6Tpfb49dgnz1yt1r1/\n8ng9tHMVOJxXIsTWiCtunq1RtLg2ZkLiUo+tQ9b2nPWor3hjIO0nCYb0eEVpT5csnjTbbG3+q2af\n7bqoF4ZRS1Ih7Q0LPKK0Gy3VComZd65eq+So3mqQpXOl0AEAePz9wJUnFj9ekmfqbQZALZ3dhyCp\nPe0jhXigGPl2rm1PO6BYfLfYvKf0+A5BdKcq0n4qrWZ992WPn6UZbgKpbDcp7VxtIwnd086VnvYI\nLKIulYYvyixZHNeJjCDJ8UyLcmPfcYmGQEx6/dgipP2An8YlnF78fBsoaTccE+S43MYcqZCdFn9C\n5MXHBXiEaFRt4zJ7fJoJjLGEtI/CzWlHk+NnmZtoWh2317CDrXGVVQGUSns40s6Z5bWarukvfHxx\n837xqsXtsqd9nglr32ooOAfREUvocdjj6ejAZcn1Wz0p7ZTg5Ep79buBtLfD2s9pp+SIZKe02c9U\n/dTntIci7b5BdHPdHr/mPe3LnEB94jKd0b5dL6grFvkew+hSy8jJwR4/4PqDEp5WzGl3VtpJenyT\nPZ5lOOxqj5d6EJ1q8eVNxCMlF4O4bo9noWa1G5R2JR8gTdQCApCPsfrz31j8eEGeqRc/AFVpZ372\neEo8uKa0sywp7PGEtG+72uN1pT28PX7SJYhu64bFsbglDhYjtfoYbVS+biunghZEF15pV50VNMOg\nKTyt3s9OrLUB7fG0D7uyx5NRhyQc7Ro7g4uK0k5SxE3HhGmcWodjM9PT4xkHJ+dlvCQ9fq4du8vm\ntPvb4w1Ku148tGFWtXXsyZ38mq4p7aEshaklPR6AWkw24fkHFzcfkJ8JIfPjcsKSRYG3a5CfK645\nBtHdoNjjVx9ER69zphC6ElukWLwqpX0N+c1aQ1dx101pp/tzFHdzVOjHSz9Ku//Itwnm+NHRz+I1\n7/8unJXV99M69LTTgiKguh5WCRpCR6+DJW5ZUV/7YI8fsDkwKe1UTVtiPZeN6fFqT3vXXtJUV9p5\n1M7iS0i7MJB2fU5258Vyg9Iu0nmdtAPAg7+5uHkRZ+rFD0BNkPfsaaf2eB6NEFPSLnKyfYoRG//W\nWThBcQOEV9rnqVC3a3LG/mAKxoAzdy5+fBV7BkC/SvvNrI3SXh0jeRBdaKWdnuMjpUjFG1pL9BYY\nZQqUlh7vlbNgGFNma3+4xk4r9vjbiD2emXIONHs80I2UCCERgewbHiMiBRAO+3VonoqFgg5ACcfT\nf67s8V2VduqsyK8lzqRdV9pHkdLTntvju22WDi97PCHtnxAvw5S0bawqjG5/Vp07zqT92O3xdtKu\n9LR7BJ3qoNezOFJ76jdl0bwq6CGlWeDQUl+oSnu3IDr62JhzRcToOz2+zci3747fja+L/hA3Pvbb\neMOVd5HfHf8+eX5PXWf6FNR90ETab11RgvwQRDdgY8A06ywAJRhLLEmXpqQ95Vv1gBtNQTxMHBKB\nDRCGOe2MbCNrIB6MkHbjol5Lue9KPhQbclQmYGtKe3JUex6OKtJxSVpIu2aP9yHEkRZEF40pac+L\nFqdALvqTU3BCrAV+BVayUyFxGlX4oXMxAQDu+KLFzS/keYBVXz3t00Tgpjbp8drx16fSrjsreMNs\ncaq0z2pKu3be+CjthtGTNtK+x06p9nhC2o3hhLFqjwe6EbraiDIeISJz2mPNHv/83hQ/+nufxPsf\nuYB5qrtElijtrLsbANADMYueQZKtIZbldhDSvid3cvW1r/R4LYhuJmmx2J20PyRfhiPU93HooqEO\n15FvZ445PV6xxy9R2nsLotOU04j2tG/IonlV0JXmdVPareRI5qFkbV8jYnoQXb/2eNfiQpIJ/MP4\n7sXPX/nCL1W/W4MgOt1qvoppGiZQZ5HRHn+mum6/2CtppyPfSKvYhhQNB9J+kmCwUlLlb+kM9HmV\nMpzEhsRzPT2+Y/U+T4+nShwHI8pzk1rIs4qA1mypgJbc3J1sKko7q5N2aVPaCS7KM5iY+g7JNnoH\n0Wn2eEo8WNHTfgqkuDB2Je10nnw/6fFnGCHtrko7ANz1usXNL2D5mL2+wqpmqRbk1zQyjyjWMQs/\np10PHmQtZotTpX0mR+A2pR2ZXx+xsafd3P5wTW4p9vgzxH3BjD3ttJjUnRCnQqojylik5EHo6fHf\n9av346fu/RS+/Rc/jKcuHy3eG0C9uDAyKe0BguhMYzyXpccTe/w17OQLnJ7mtOvBfnM4hovuvwgc\nvAAAOJQTPClvxQzV57nN8mO2d9Lu3NN+zKTdMYhuq6cgOkVp19Ljh572dtBJerYmoWcldJWc1nhd\ndzUt5EQRw3YPbRtJANJOsR/fSH53/PtEt5qvg9JOZ7SXoEp7r6Sd7JOuowjXGQNpP0mQmu0cUIKx\n5LJF/awi7WlkIu2q0n4076i0672kPGqntGfVxYCb7PHj3cXNbTb1sMdTclQulonClc3NSjvBRZxR\n5pIuQMjBhHUPopNSqkp7NMKIEAguc6fBrmJDPw0njPqd054kWXel/c7XLm5WSntP9vhE4CYve3zY\nYoJC2nkMPqr2d9RCaa/3tKtKO+Ch0ClKe/F5WDILDsUIl6T5mGQjw3MMyexdtlNIk9JOetoJof/k\nc9fwocdyB8A8FXj/IxdUpV2f024Y+db1+FRaDQyOH1d7/J7czUMxaRAdk+HS4zP1uj4FnWKx5Lh8\noVLZH5Z3QYJjygxuip5JO+1pP73UHl/9Xcdhj6dK+7Ke9t6Udi0NnF5CNmXRvCroKu46K+2xnl/Q\nUWnfIefWwSzQyDdLcdl1G3VifjCuwnrXoaddV9qPj7RX3zVnDUr7LSsi7fZRhOt1/nTFQNpPEBSi\nWagyjDvYFIUAI3Pahd6jCajzp4s57a4WKYpc4dLs8RrRXAZOSft4OWk/BR/Sro3Wgp7En6iheAZc\ntKbHh5nTLqRKLnSlncvcHr+r2OMdSbsWphWatHMxw4Tl+1pGYyUVvBG3f8FCdXwVewancNibPX6e\nJGqQX+PIN1rcSoNW6qWUWoaBZo9vobTnM9rJ73oi7eV+Yhal/UCOcQlml4WxKEfPHVaQ9o72eL14\nGJP0eGqPf+eHn1Kee/9TlxfvnT9YU9qV89svKJGBFmnqOSViGSGeUqV9u+hpZ5CEuCv2ew/o7QaH\nknwmy66Tz1fJ8Q+JlwIA5qx67s7KetrdlHY68u1Kj2ONbKAOt2U97X0F0dEiZKzb4zdk0bwqrHt6\nvFjSCuG6rXo7xQ4pNB12FH502Iod1oKkyIBH7gGe+3MAwHaqzj1P4sqNuI497V0FM1809bTT1qG9\naX8FTeqsGJT2Adc3DOOBKNmGzUqZHIIVoUyHcqIsXqvXo6RdQMhuPUm1XlIWqenxDUp7JAhpNxE9\nYv/eYdMw9njD+DyZpfU57RouydONPe1jpDjquI2pENrM+xiTUYREVl+MO6jIMVhkTNw3QiNHoXva\nJ1nl7JCTFio7kBdmbv2LAHK18PP4Y72Rdj69gqgYZTUfnWlOudccKSFHVQkJRGR/s2ikhqe1Udrl\nWLG26kF0ADDtGmClXIfy459HZiK0n40VezwFdREsoAUkAt0InZASEVOvQ7GitBefQZLhN+4/rzz3\nTx+/rNnj9ZFv9cJCiPR4tkiPJ+6pNkp7eS0iYXSyKdndEYkQSgHxiFjcsWwiyLVnFjefkLcBAGac\nKO1l2GDPvaWuc9rPEEJ/bZaufKFIjyNnpT1gwaPW086v3/T4T72wj2/6+Q/iB37zY8fSj6/butda\naY9YeSkH4F6g0Ue+7Y6J0h7ouGwdRPfhnwd+5euBt34ZcPFR3JGp1/exqL4n14G066Fux6W0X6b2\neBNpJ9fGvWl/hYVsmNM+YFNgIpqURMCmYs8PFjcPMDGPKaNkhFUL2rao2+PbqYUKadfnIwNK0Nop\nTLsHQFEFe0HayWeZGea0a7iAs+bPMtKS2TtehGtqYTTCJI6QonrPG1CRY0xOQZVXl6DnkW/bGd0u\nR/Wf4i5ikWePYtaTfXY8r4LR5pNzzU8gBahxYKU9q7lUIkQtXCq60m4b+TYuCGvXsEmlt74gmsbQ\nSAD7Irba47e3DW06gezxRqWd9rQX5/89H3++ZoO+Nku1IDq70u7Tdw/oQXSGQMwlhFjQ9Hi2XSVA\nkzA6ep3zgV6MVezxy7I/Di8vbpaBhAlR2leWHj91U9rjiC/mJEsJXOtRUTKBHkdbJhdXgb7s8ZQI\njbT0+OtNaX/rHzyKP/rURfzKh57EPZ94fuXvr9vj1y89vtqfnLVX2oWQRe+7xKvZeXCZKkp7KMW4\ntdL+6ffm/0sBPPY+3Jk9rfx6klXr4bXsae+5VciGq0fL7fGnt6q1T5/XRWVOezTY4wdczzCMfKP9\n4tbeQhJCdyi3zJZuxT6bv0+nAKg0A2d01BIHb6EWRqK6cERGpb2yx++EssdHdaU9t8d3VdpVJa7r\noqoWpsVjTEYcCartPMcIOR63IMca8QjdU7olDsgPLZV2oNbX3pcSN55VpD3darDGA/U57QEXYvUQ\nxwicWrpbzWkfgVl72ovzuytRkuo25q9vdijsZyNctNjjbzprCE2kxyVLAMjOQXSR5vihrSVjpBBC\n4iNPXjY8W2pKu54eXyftnYteWoYBAMX1JFL7PheHVxa3p/xUtb+J0t44Q90RidbTfuRqjyfTNq7I\n/No951UxdmXp8TPa026edFDiOMe+0c/BWBAuoAbRhb0Glbje0+Ofuly1BH74sUtLHtkPdLK5fkq7\n2grBlVaI5ueXKvu/jt+Beyb/FOztX41dclyG62mnxQVyv20jDy4ot18mVNI+JqQ91Cx5H+g97V1F\nHl9caVDaT1MXUq9Ku7mnfbDHD7juYOp/pNbz2gLtY78O/P4PAZcfW9x1gO36jHZAWXSXttQuNh0a\nhpcVijCPaZjW8pM9JqQ9NintxB6/yzyUdmWxnH+GktN58gkOD6sv/X2pLtxncoR9bFt62nV7fEel\nPdNIHI8wiTkSorSfZR0V7YBj6XRIKbFDSDvrQtpv+/zFzc9gz/entCdVX3DmYuNXHClh0+MzWc+D\noOFpERoIRE1pJ7+jqfdlT3vXxYFhigW3tBVcy8aYYqL2QBdgplYOzrVrUbdjU9RGvsVK331ZcLlq\nIGUjVIXHFJHaggRY0uO7Lf7U4MG60r7Mei5IT/s0IgUQTnvaA41dyrJFGwkAHClBdEvs8UdVUeRK\n4bhIOOlpL9Lj+7SEzlOxaK+JOFuqYANaX/uKE+S72OP7TI+n15DrbeQSJSH3P3VlySP7gR5ytm6k\nQy/QKK4Kh20tn/8t8T35Hec/jLP7n1r8PlxPe8sRYIeUtL+Al+MZ5dejlCrta0Da18YeT0a+tL6s\n2wAAIABJREFU7Ryf0q72tFf7e1OUdrvPa8DGgYvlpF1JZn/uY8C7vyO//fDvLe4+wMRMNAmxOl2k\nkXdZ1Museo5gESKgldJOSXs0aSDtmHYmc1wKYCFMla0G6mJ5nh2gNPA+Lm/D57LHF7++iNMAGMaR\nYWGl9eV2XVTlJE5T2jV7/DndHu8KSjwC97Rn2ox2ttVi3FuJ7cqqfpr1F0RHe4aNJFKHFugWsqe9\nPls81saUNRxHdE67tKfHj32D6AyWbnqOU1xLIwApLuH0InSs2iZLfkC8tSCCE3SbvpAZpljoIYLz\nTCpjvV516yl86oV9RWVP2bj+JcvjPKVdCsRMIEYayPFTBmJWn4tcEkQniT1+Rkk7VdoDBdGl9LoO\nhhkd+bZMaT+sFM7LyLcxicKM9XPFgdbPzhpaiOiM4lWPfaPX4eVBdH31tFfv34XIrRMuHVTn8cee\nvop5KhTlrm/o3w3rprQrI98iLb/AgSCZihA7JBQ3WE87JXEjvrhWWI/Hg4uLm3K/TtpjMcMIKRLE\nx07aD2ap4gICjtMevzyIbnccgbPchTFNBJJMYGQSAD1h72lfr/OnKwal/QSBkUVoaaVUlHa6QPvg\nz1a3ydidA7lttt0R0n4WeSWy06glOg6qSDFWx1Ytf82RbFLaK3u8l9JOPkte2uPJZymzFGJekaDH\nixClEmW4VpPS7qNi13uc40Jpr2jEDYo9vgVpD1RYMCEVEqdZx3Fvi42qXAOncdQfac8oaTf3ZStQ\nAhsz6wzZLshq7RBaeFpTEN0ypZ2HaX8B9PC0uptmgXgLs2K3GcPobJ+3dv50uw7prQZc+QzGLEOa\nZspC5UtfdfPiPUtQVbh6LaaE002QdD4+TUo7PcZcg+jmMXHZkJ52kaZByJYgpF2Cq3Pal9rjqdKe\nX59SEkRXpcf3Z7d0DaErcVaxx682QZ6qbNvjY+hpz6jSztU+5+tI6ZJSKsrhPBX4+LN7S54RHnq/\n9LrNaVfGtXGu2uMdrhn589XHbUXVz4ezQEo7+dxoIctYBEnnwKy6Lsq9Z/EZrJ5nsIt8bXfcPe26\nyg4cT3q8lGoBm7qNSjDGlOtnXxZ5pad9IO0DrmsYlXbLDPQZGWNFcICJ2R4/OYNSej7NjhAh65ba\nTBLsRWmPpwrXMuIhsoV1V0iGsWnk20RV2kPYUnlZAOFqPoAgc9of00j7pZK0myr3NIiO5WFlXSq6\nqU48+CjvaSfp8bUgOlf0GEQ3zwTOwLOnfVKRvFM4wjzpR/FiKZ1W4EDaaQp7D0p7pDkraB92hIYx\njNqcdmYJoivT47va8Jihp51bZq6XPYPGMLplSnuBCeuotBuC6MA5UvKVmSSJslD5sleXpL1a7Muo\nubCwhXlntZORdqGyAEsDMeWSaRuMXOOTUfX50usYhwhC6pTrOuOYSRraaSHtUio97VeRF1xTorSX\n6fF9WkKVGe1LQuhKnFlR76YJ1FK8u6TAsLWCIDpdaV8DJ7EzDuZZjZDdb8yv6A/6d/46K+0RU1sh\nnHrahVy4tkpsyepacNhxbLAO+jkqc7tNG3l4UfmRPfORaroOwanCTXrcc9r1cW/A8djjZ6lYnC+j\niJnXtViNRV5xVgxz2gdcz+CG2eKqPZ6c7CR8jsIaRMc5QGzMp3HYsae9rrRTtXBpAjZJIZ5hhPHI\n4AjQgui697QTpX0xPo/mA6QAIe2PC5W0X0BJ2pvs8d1HQmVZXXnN7fE0iI4UZzyD6EJ8wQJ5am7Z\nYpG/QQfSHsUQo7w5gTMJniwPBewMYj82qsW17VIV637T42Pl/B4hXb7wo0q7HCsjfPRiw//P3pvH\nWZKVZcLPieWuWblVVnVX9UIvdENvSkOziAKNgAwKwggKjgsOyieMiijyuc8on+PCp7aOMiqDoIKK\nIoqigAgoIAgN3c3SNPROd1XXvuV6l1jO/BE34rznxIm4cWK5WUXl+/vVr25m3nszMm4s53mf530e\noB7ztHje29bNtLs9Adp1ZnSZTLucg17mOpQyoptcL+m54/sjaWb52v3zeOXTLsfutnhdr5/RCHNl\niffQr0Eez9KGmAiyr5c2YZR8AtqpPN5GiM0amJuAXtdhY0yHBrKY9tF6dB1FBNRHkzl4CbSjedBO\nmfY8IJxskyue07RBnlpFVQFdyYiuoZl2RTJd1/1hFnV6M62QuOPh2c61p3Paz66uh9qgKSOP7ygj\nT7a3kQAtzusxSQwyQJz2Xkjn2aEQWaTmJ+N7qsP/rEvPtM8etMuNTTdzhGgWZnQ7RnQ79bVTVEo5\nkXTbEmgnJ9FID9o30M3soqGzmDycZ1ulFgMUtIcsPe/qIsg++cjibwRXv50EmPZZefd4S5LHx67N\ndF+Ope15iO+VXn+SR0B0qnt8hZlNPwzlrGmtPJ4w2kZGdDKbGfL6pGJ+yGWmvV1iph0AJ5+16+uP\n56rFAppWMCWjHUjJ4/063eM5l3LaYTkK2A7yFRs+lce7U2faSzPtGsWPdt+5XYwmYPY4X0z/PJPF\npg0lrxToDLniDzC5FlHQHnjjlCTw57/tWvzDq24Sm6Ib0VG2MfKEKLcvbemaPtk2u4DJmzeE7UUN\nO4/b8FvkHCPyeJuF2KrBxTmUvEoUeXzWNhJp/NgVjbvASbvH12VapauNkfiMi8jjaWzVrFkvOn/f\na+WA9oaM6FIz7dSI7hxaNJ/WjDXccWC2TLtqUno2M+2OzYzl8UHIk/M3qdG61Biro2FI9xtVmGiZ\n180T6e9pai6Rx28vaL/nSFoRux1Me9Fm4bzEtDckj5eMBwloP7tOn9K1A9rPo5Jn2tOAmEot6bwj\nrS20s81YCCM6j81SF4+QMENcEwflwM++UCpMu1YRQGfaS8rjubKgT2ZJpcWyD0bkxu3+Eo5zsX9i\nua92XzpytBQADMfm2xmEPJk/BgBYDlqqe3xpI7p0Y6EsW6jWOMW0lwPtFOy3Av24R9VixPjQ0kUM\nqkVHPWp2jw81TLs8iz4lF96T5fFZOe1uVfd4rjQWoFcpcKeTyCzfG34DuNS8YcDCxfr3V/wWNkvM\nRgYhUpn3gEi0AIDNrWGiBHBtJoAQzR3PMidUlCpDLywFaqREEFuj+Mkyots8ljw8iXm0CDsMJq5J\nVk1Mu69c1+WZ9oycdiKNHxHQHtpkpn3G8vi8jPa4utsK2sXv67fzjOjEZ1znNsoz7TKQO5dm2k9p\nmPYDpwY4rmE2m6pzzT1eYtqLgHbOk/GWpMYbctOrhoYhXS9KTLvuXqjI47Mqlsdv50z7V46s4c0f\nfyD1/YEXzNz0cWNYDLTLTHsz8vggK6f9LDt/ytYOaD+PSmdaRGfaJen5muyYGVemER0gm9GxzXLS\n85Au7iaHpyWzfJkdZwrauaufvU+B9urzrllO/BaZ1bzhsr04zJeTr09iHpev9KUbVFIKCweUY9oj\nB2zVPd6S3eNLG9HJbCZQXz6oV8dMOwDWEUx7i8S01FkWYQkdw5n21jQQbVhpDwMr5VZfnGlvQVK4\nKfnyQHmGTncd0jHtIZGQ32ddAfbjnwf+8x8BN70CePFbgPl9+l+gnD+lmodhmMS2TTYUAOAzcb08\nsyHOnYUukQQWAe2u7AkBIOUCXKRkb43JZ0SN6LJm2jeOJw+P8wWJgZKYdoT1gDoFtMvu8Rqm3R9L\nTPswk2mP9nWTklA5o92MaZ+1KRRtsOQtnunPyhx3WZUH5M6lRfMZTZQjMNu59nOKaVc/6yLy+EDP\ntNPzp57RHNk9Pvm+bhuLgnZs70x7GHK89p2fS5rG1180L6la6iJQitY6VSPlNDZnIY/3sj7vs+z8\nKVs7oP08KnkOO54lJXFqMWAergLjLCO6TjbT3iXyeGyVWrCEkjw+lnvKgCHTvEuSx7fQ1s20t6k8\nflAOtKvS2Ri0O/JMux2IxfuV+/fgXn5R8vVXwwvxhhdep5/9UVg4oKQ8Xp1pt6PINyqPXwL5nEvn\ntE+Y9ppi3yL3eMK0l5TH03z3brhZ+6KRcw5GjBGLGdHJsWF1yut0M+1GzD5l2lORb/UZ0UED2l3N\nvgvJ7HLLsYDeMvD1LwOefwtww0uy31/xhCjDtIekeRjCRtzBoKB9dUPsr3nqlku8AZClviDAs8Oi\n86cMeJIj3zTNw6yZ9g3hiHxCBe1kpt1CWGr/qRWooJ3TuXuFbfvXXwV+dT/wN69IvjVwxLnM6b6b\nNDwanWkvyCLFdbbI4/Pm72mOsm5+u2zJOe2W8Zzz2VI6ph2YbV57eqb97Np/9LO2GJOavIVAO+eJ\nJ0VSozVprKOOsZe4MX4pO4pv33w3Lp24wWvXAzny+IctsX7bxbZXHv/lI2v4ykQa33Et/M5Lb5Qc\n22c9104B+HwuaG/eiE6aabenjEOcg7UD2s+jkqSUsTyeLvDixfTqI5nvsYV2zky7zLSXMqILNEw7\nZQuZwk7e/nbg1y8F/u7VEujInmmnTPuoHIOtmlQxvRO/E4obUq/fx22P+iH8S/B4/KH/Alz9xOfg\naVft0f8C0vzYjShmpg5FACwn5R6/QGfajZh2OacdqM+FeOyHidELgApMu+IgX/MNduSHkvutpTNT\nU0uZMa9zpj3UKCvSzH4xpn2kRr7VmNNuaeTxWqadpijkZE6nSkk22CwhsVRnsJPHoEy7OHcWuxly\n70ymPW2mVmYRIzHtdrrJScc3pCLy+BN8Qb5WNsC0c+qXos60U6bdGwIf/20g9CSmfWCTxp2mqdlk\nNrE0r1lAHk8bILPOTN6g8vicmXa6wF8b+rUBQpVpl+acK/yK93/xMF7+1lvx4S+n47eaKDrT/pgL\nRDN7lky7Ggd69jHt4l7i2EyO9ytwWwvCMGlYJjXakMY66rj2RPdYjv/j/hZeevrNeLP725PvTzei\no/WgfXnyeLtn2tcG4pr0dRct4tF755Rmx4yvO6Xk8U1FvmWkBXyNgPbpd6Cd+popmwdxKlsCMGnU\nUiKPXz2Y+R4bufJ4yrRvlmJoZPf49Ky4BHTWDgP/8KPR48//BbD/ccnzRnAz5sU74MwG4wHazIM3\nNp9R08ZBAWBkO63Qh0My4zvdOfzXFzwbP/u3F+CKPX388vOvy/4FC5ckD/ez6CZSzohOdY930Ha4\nLI+vgWnvJEx7ffL4RSmnvexMO8lqZ1sYeaHMKFaskRdKmdyZxmi0lNnwWuXxgcbx3JKZ/Y28RcaM\nZtoldjiRx6f3XUCY9sxGoa4U6XkpiaWueQggIEz7+uYAF+Ikfrf1JvTX+sDwPdGxKoH2jGOi1Use\n9hLQbradaW8NjRHdBCx/8EtH8C93HcUrvulyXLNvXpbHY1GacU65x9fAtEs57ZbqHk/217EvRYBd\nqS1HXAO4RqXQpAx93ZhpF8+ZNeMlM+3Z1zrbYpjvOFib/G1rAw9L/QJNxynlK5JpU3MyXY38AK/+\n89sBAB+95zi++uvfVm0jCxQF7c987F7cfTS6T37h4Cr8IISjG72ruVRn8rPZPV6VxxdpAvkhT9YO\nSY3WpfOnTMNVrSCIvH0eY0Xr2sdaB9DG2MiIbsxtHHIvRXy7T2ba/e0BgpSAiCXg1EujzhjHIlW0\nsSkx7TWO5dCix+WOe/xOndMlyeM17vEJa7OWDdq38uTxinv8RpkLbijLKKMNExeBKNt6cvJ99Dfk\n19734eThiGcw7YwhcMWCmY/NZ53DEIqzdPR7mENB+xgtAtp7vT6uumAX/ubVT8UbX/L10gU2VfNC\nhnUhOwWGsNS8eKAF7bI83qZzuxVy2oH6QHvkHl+daUdbvG4XthIn8rpq5AcJgAUgG4BllcJY15nT\nHvJ0xJ8aMZfL1vhy5BvLcI93KzLtcrZ4dCzq5PE+OcYyrzm6kmbay0W+hbrrEGR5/NrGFl7v/jWe\nbH0F1w9vi2TdALB6QLxRb7f+F7hC8RObMZky7er5LdRTVPHjY2Pk4zXvvAPvuu0gvv+tt0bgSZXH\n00YsyfqzEdTC2lDQDmYn8W3RH0Iap4c+p339pkUad0pcHlDP4j6rpJn2Aky7LI+f3Ux7EHLpnMxj\n2gFZIk+jC6ttg8K+WvRn5RbNR1flxvosouNOb4r9cc2+Xdi3EF1TtsYB7jnaTBKJWqmc9rPM/jqU\nVBWWoqoo6R4/3kC/5vPHC0P5Pg1gAZtGRnSH+Ar8llhPbDfTPiLneezdtJ1jObJ7fPY6aBZGdBJo\np0Z0Z9fpU7p2QPt5VDrHczrTbidMe7Y8foN3isnjsSlF5RQtrlssS+7xk9iqk/cDt/+Z/Nr7BWgf\noqU3ogPAyYLZ8sxBe2Twlmba6ahBKyRSfe5gV9eAxWjPJfuyzXzsxno5GX8QwlEj31zZPV4qk5x2\njZFWXd1dz/OTm2L0C6oz7XNsgFHNmaqqPD6TVaWlzIbXKXnUjUOoDPnYDyNH8Q/+AvD3PwpsCYdu\nmWl3FXl8s0y709Iw7Zb4Xta5rC1lpr2U0ZZO8QPiswFgfTDAi+2Pi9d85i3R/8e+Ir6357H696+B\naU97a6SZdhZ6OLI6SPwmjq+P8KkHT0ry+ON8QU7aIE3SunLaQ1UeT2faqTz+cAZoJ/J45tbj+VG0\nZOnn9MacxHjNcPFMAU6vZcOy9FnJcS32xN9yRhNxVqb8HCBXdqb98OpA+noWjBll2pf7Ldx4qSAk\nbp+RRF69N5xtTGFKVUEuIYVBu+oeP1pHT4p8q0EeH/A0aGebRkz7Ab4HnIwPxjPt22VEp2Xa3e1p\nFgLAGgHgeY1N+rO1nZz2UrUD2s+jYpr5R5uywwnTnjfTXgy0R0x7XfJ4meXzQ44zn3ybHB0FOWfe\nYy29yRsgXXztMqA95HL+eTzTTvZlJxTvO0KrEEMj1byIs9rHTpa6eVHzpwAWwNjEPT5jW0oy7bFE\ntS4jumC4njh3D1hPmrE1KiKrn2+AaR96QWLKBqCkPL6+m74fcjipnPb0uYO7/h745O8Bd7wd+OAv\nkjeQ3eMleTxNcGABAF56XpdJRnTR+7oa0O5ZNTDtGGPsh8b7WZJzZ4D2zS0ZTCSy7uNfFt/be43+\nF7hp0G66iAlDKE25OMaTKn681Pv+zW0HgQ0y0w6FaVfk8XUATx7IChA5p306075hkYaixk+jyUXq\npgKGp9V2MV5y3Nv0+w2da89ySzetXEfxkovmQwpon0XMFjWiW+q1cOMlS8nXdzw8GzO6cUoef3aB\nDro9lqXOtBcD7Wl5/Bp6FHxWlFBzzuGHHC3Ix/cCNvTbmDHTfpDvkcftJkrA7WPaydy2rZHHb+NM\ne95at+mcds65nBawA9p36lwtzrmU025pmPYiM+3F3eM3pRO58HZqc9plia8XhHjoq/fnvo9v5TDb\nrYpMe6h3j6ejBu1AyLuHaBWahZRqQUjk97OTpfZlSDKa43zplm1J8nipSka+1S2PZ8PV5PHA6uc8\nc0q1ZSO6upoKcUVMO51pN5PHS6MeNVSYUoA4gGVHDRtEoxCe5yH41B+K53zuHeJxaqadvLllSQys\ng6B0xJ8U+TYZ03Fb6fPVY4RpNwLtMtMOmIMnroueBBCQ5sXm1hAjrnzmYQCcuFd8ncm0i3Otx6Jm\niek57oehlmmXoyf91OLo/V88gnBdZtqzIt8i9/ga5PGkGQumzrSPxP/HvgxdrTOxYGattInf0Asb\nixSj17XcsaZJ9Ujm/SxBO22S9wts55Ikj6+JaQ8o067GgJV7z0NnhtLX3gxmuynTvtRv4XGEaf/S\noVXdS2ov1aT0bDOiSzPtZeTxKtO+ITHtVc+fGKi1NEx7an+Ggaw8I3WA74HV0cnjz4KZ9knD9eyR\nx29fTnvVY/JcqB3Qfp5UyKFki8euzUQeH89H5oF23imc015KlkrZc01Ou8sioOMP1nLfJmDZoJ2R\njqnjb2U+L/O9VXA0aS5YZF92uWgGDLlbyHVYqgXBtO9nJ0qNGoS+eE0YqwEYk9hCqYyM6JoD7RiJ\nz7YaaKdGdPXL44degBadITeUx7fg4wsHV/F9f/xp/M6H7qm8PSkPg8lnHpBxCN8f48A4ozmTN9MO\nyOchfGx55TrlMtM+mWm3HSnVAFCYdiN5vJzTDpgzsVrFDwBOmfbhAAMo15kT94j9OHdBFFOnK608\n3uwcD0NoP29LacSuKfPKAy+Avy5m2o/zRVkeLzHtvBYWm+5PWHJOO49B+1G9CR0ArGWAdsrUNSWR\np82+bgEjy+0yhKKfUxGmXZbHN8O0sxrk8YfOKEx7zddxtTjnOE32x1LPxdV7xfH34InNmbB2KiA8\n25hCNSnA3D1en9Ne50x7DOJaTAHt0ETADk4D0O/jg3wvrC4B7YkR3Vkw0+7E8vjtM8CUmfa8mfZm\nmfb8Y/LsOn/K1g5oP08qy/HcIVJKGz7AObB2KPN9NnPl8XJOeynQTpl2zYymM3GP7xL5+b8Ej0+9\nzW3O41Lfi8siMnDb3zQ2tgkzmHbLpvJ40QwYsbZZZBUgmdHtY6dKMe1BkAbtANmvapVk2rtsDIDX\nB9oJ0z60DbZJLSKPb8aITmXaTSPffIyDEB+/9wR+50P34jNf1Xf5i1Z0jitGdAACJm6UgTfC/Vtd\n5YWTv8FT5fH52z4Yl1uwWDrzNIulFCB/9XkhVSwrjxdGZaagXW9EF5JzxwpGsvcCANzzAfE4i2UH\nJHl8F9F+rz7TngbtOqa9jTFafmSmNeY2VtFXjOgI087CWuZKQ5oXb9lSUzUB7YfuyHz9KgRosqgR\nHfMQL7SbYpcGpkz7NhnRbRTMaI9rsQF5PGXBbSUGrLQ8XgHtTTPOW+Mgkaa3HQtd18ZCz8XKXHTM\njvwwtU1NVMqI7ixzj1cbNKWM6BqeaY+PFd1Me+o4ovPsi5dKPzrIV+D2ZOUecJbMtDs6I7rZzrQX\nTdiYPdMufiYdk94AGMxmzKXu2gHt50mF6gJvwmLLRnQBMN6QZwxJBZxhiFbhnPZS8vhQw7QTN+TW\nJCbLJfLzD4VPkN7jbf5z8e+tp2X+DpuA9nZozsCmc9on+5LE5/W42D4/h/XPLIlpP1kuHoMslEPC\ntlIQFxd3+5Jr9NSyrBRrPKhJfm4Rpn1UBbRTeTwbSHNgddTQU93jC3zOlp3IrW0mn5OfuC87I7ZI\nRUx7Wi4dENAZeGPMt2Q07p18MHrg50S+AYrzfVC6SSOnWIj3VEH7qie+Nop806hAjCXeXM+0h+Tc\nWWFr8kw5ANz9fvE4a54dkEZ0epXc43WgXSg+bO6l3ncPE02xE1gAwBSmnbrHh5XnSgFIqSBgNjx6\nDYpB++HPZ758FWJ/uW4rObZthHAnTaCmFqqUtSrEtJPnNCnbV4se40XGsRaIPH61Lvf4QF401+Ee\nr8rj1Vnvuks1oYvVAlfsEfei+4437yCvgvazjSlUWU0JIJWNfAtGmHPEcVz12hOns6Rm2lUjuuEq\ncOubxde79kvPf4SvwO2RNJoJ0z7yZ3d+05Jm2jWgva61WNFaL5iw0XROO03jyWXa//nngDNfrf33\nz6J2QPt5UlkLeselTHsADLNl55voILXAo0WN6LCFzXFgfKORZKlZTHvA0Q4E0357eBXe7j8b6/Yi\n3ui9FL/sfz9cN2fRQkD7HBum5KPTKpLHk78r8QeQQWxcnlVANq2WxLSfLHWBo0w7zZeGjmk3MaGL\nS2E0a5tpHxPQ7tQD2htj2pkhaAfArbQTO1Dd1CbttRCDdgGQQm+MdiiPhBx+4EvRPF8gFlAjuBrQ\nLm/31tgvFb9kSYaYNE5NBkQDTs6nijPtxg7oGTntlGnfxzTxQAc+LR4XZNrLusdrPQwgN0Isnmba\nV0BAO5+kVGQw7XW5x6vyeOoNkDDt64fFc8j+AYBRKD4D17YkM7p44d8U006va50CoN2ymJR7PyuJ\nPFWTFDHMWyLy+NMzcI8vO1OaNqJrGLSTuDcai3clAe33H2setKt+J2ffTLsCkCyzUYiAa0A7gHkm\nmjR1Me2pmXYQpj0MgLc8G/jsH4sn9FeA59+CdXsRf+C/AEexjM6cUJJShVUd10fT0s20y0Z0s90m\nOr6Z1zDstxzEl4StcVBr5C0gnyOubcnHJD1/PvvWWn/vLGsHtJ8nlSWldMkctoMAfJgtGdlCBNJa\ndsaCwO0mC/s289DG2PyCNm2mHT68MESHRKpt8g5+0X8Fnmv/Mf538EIALJ+ZoyZQGBq7NqfAUTLT\nrgdtQRnQviCD9qqmflQeH2oM05iJND4uCRyNMawJFNuEaR9XAe1UHt/QTLvEtDsFFRVKVntcVU3p\nUjPtGtAe+ONUYsKph+/CcCB7MAAMqfAFW/aWCHk5eaBkREcAoppqMEBZI7r0TLsp087JgpTOsdPR\nkgsxZZyhKNNeUh7vq+MQyUw7dY/3NUy7uMYf59FCtJMx024hrAUMq6DdJyaDCdM+IkBo6TLp9RSo\nuTaTIiebBu0DCbQXOw57JCN9VqZQ9F5bhGmfxUy7DNrN329t6KXOi6bBq8y0i3105R5xzt5/3NzA\n1qRi13NawVmW055i2k3d4wONPB7ALgLaq85mx/dUdaZ9npGZ9jMPRV4ktPZ9PXDTK/Df9v0VfsP/\nbgBAtz8PIPobe2yUXHtLjYFWLLqW0THtMzeiK+geb1kMc+TaWPe+S6s/qhthnm21A9rPkwpTku6J\nMZktg/ZgIFiYg3xFeo9NHi2UMk3VGFPY9k3jWVIWphehMlgI4PshehBs4QYi1uXQmrgB5M6Qt2Sm\nvYwsVesPkMG0BkWiwNQiTPsFOI2toX5kIa+CDNBOmyBJmZjQJa8hkWpsq7SbuFoWMQf0nJqM6LBV\nuzy+1Ew7AK44yMdV1Yk2ZZCYzLSL8zX0R3B8ecHpHbsX7/mMSGMYTszVpjHtQLlFFZ1pp6ywD/m4\nHJKvzeTxcjMJKGNER2ewxe+mKon9OqadVh7TrpPHG14rw4wmDR3TsbmfNCUZQrxu7oP4LVekB8RM\ne5Z7vIPA+BquKxr5xpgNn8rjY4XHeF187+KbpNfTc0Nl2tuTyMkmzJc45wpoL+ZNIktXgGzCAAAg\nAElEQVTkZ8+0F4t8o+7x9YB2X1k02zbT/qxoHVak8cBs5fES075XrBseaFger7sXVGlWfPnwGg6c\nMjfdzSt5fjiH1cyogGuM6CBM3oDqLHbc7EvNtIOYCQ5Oyy/65l8AnvwqAMAaHTnpuNKapz9h28sQ\nKlWLngPxvbFLG4UzNMDknMvu8VNMl5uUyKsz7TtGdDt1zlYW0KQAzkGAYIuC9j3Se2yig2v2zeOy\n3bJ0USo1q93wpJQMoGJWizHJATvwZAOoGLTTypTwAymWqy6mnTrx0+KE+StcTht+N9r/NuPoDI9N\neYHm9/rihtgIaKceBtisL1KNzFaHdol9F5fbS46bDvPgjdMLwCoVuccb5rQDQIY8fhxUu9EGupx2\nyJLuwPOk0RIAaK19FfcdOp58nYB29RSSQHs8R2zIYHMOi0gnGfklvpJqMKRMu4l7PDEqKzvTLjPt\nehPHC1kO0754qRSBmd5GakRXfqZdVk/FPiUyaI/f95X2P+HH/D/BPBOL9xOIFqFSU4Sa7dXEtEsK\nKstGSCM5Aw3T/qQfFmZQ3/I/FabdymDa6188j/wQ8eHq2iz63QVqO1ivDcOcdsq0r9Ykjw/IeeNY\nTDpvxyWUWKo0HmieaacZ7csEtD+ayuMbZtp1IwBlQceffOJBPO93P45n/fZHcV9Nsv4w5KAKeItB\nOjeKNKC1kW8Aelx85lsV4ybjfZZrRLdFQPsVNwNPf32i0tsg1+T5jqMQAdF2mq4f6yg66ifc47cn\np33kh8nn3bKtqabL1EF+rWYzOiq3d1R5/E7k206dS5UGmrH0nM4vBvC3qHRyQcoh3uRd/I8XXJuO\ngqJFHOQXsGnMHjFlcZdsP1nQj9YFw7XF27j6QgEe48pd5BPQ3oc5056eJY1Bux608ZLAkxO2fW50\nxPz1BARSia82T7yMPJ6Aknm2Vd/spidu5KxMwyN5McPIFp91mOPXUKYipr2EPJ48zyWyvapd53Rj\nbgLaKdMepGfaV0YHcWZVsJxDnsW0i/eJF0Gm8ja1sUCVPoEC2uuYaS/rHs8U47S4OGl45YL2p74m\n/xdIjcNyM+0B57IRnjbGM2LaL2VH8RPOu1PvIeTxZKGlGNHVwbSHZH8yy4ZP9iNL5PGEae/vAX7k\nM8CP3Q489UfT8nhNQkATs+Om8+xxbYeT86ZhTrvkHt8Q007P2zIMuc6lvfGZdiXuLa79i93k7zmx\nMcJqTSMFutKNSpVxj7/36Dp+6b13AYj2/6cemKIOKlgUAMXRftJnXeAzCkKODks3iyhor8q0x/tM\nN9OeeCxQpr0rR3SmGGQC2mNFwHbL47fbPV5yji8QbTxLpt3SpVcEzZ23s6gd0H6eVJZ0VpKeI8Dm\nmrioD+05jF1xkWr3d+EpV+zO/0US027uIM918njILNxgTbhsb7Eurt0nJEvJtuYx7eTC22dDrA3M\ngYfOPd7JmGmnrJBJMTLXPj8uwbSTzGMK3JhOxl3GiE6K+NusTQbKSV44a1UA7QDGVF4/qhm0eyFa\nzFwenzXTXnWuNOSqXDo6f0ICkEJvhC6XF8L7cAIPHnwk+Tpm2lPNOY083tR1OuWtQZ3toTLtZUG7\nZqa9Sk47aR7Sz26eZcQ+LV8J3PSD+b+AGtFN5PEbIzNjv1T0ZKz4STHtPn7FeeskmlGuWE0lM+2y\nEd3WOChlOKhsrNhMy0ZAZ9pjpn1MWMD2XHTd3H0lABmQt11bUlOIxkwToN0soz15bmv2rJdpTvsC\nAe2rA68W+Sh9D9e2ZKbdAGyHIcdffeZh/H//eFfqZ01nY58mTPtSn5xLFsMVK2Su/URzEnndvgq5\nWWyeH4R43bvkRIbaUgLIdsRzw/SzLvIZZTHtHZK8U1WlkjDAeUz7gDRfu0vS81JRZkqMLLA98njd\nTHt3m2baNwo6x+ueUzdoT+W065j2cbMqmaZrB7SfJxWEHDbTLJYtysoE2FwVFzC/NY/2nLiIXfOo\nfdN/keogX2WmnSweKej01wVoH7Aert2vAe25M+2yPL6ULJWlGyBZoJ25afl+kbJ3XZA8ngvXjJkK\nLjlgU+Chc4+vKI9nmxjWtZjyxY3cKrnv4vIc8ncN17OfWKKGvhr5VkwezzTgF6jOdvmBmtOeZtp5\n4ElMBgBYjOMKHEi+jmfJ83LaY2Mf0+SFMIS+eQggUFINJCO6LPNLXWnc440XMVzfPNSOlgDis3c6\nwMv+Ynp8osaILgi50XYGqSZNzLTTyLcAva1DeLr9RQCRE/6/XPIafCl8FN4fPBH/Gj4OgMq0y0Z0\nfsir5xHnuMezYBy59cfNOmal3ONpY3Wh60qNmbgZ0QS7ZJrRHtd2GNFtGEa+ObaVLKA5ryc3WWXa\nafPc5P71/juP4Kff/UXtyJU3UyM6+Z4+Kwf5LFbdROJ764On8IWDq9L36jJjVRlNQAHtBa4Xfqif\naafjW1XP6RjESc11AIsgRnRbBLT3BNNOr8eMRc7nOi8S6pw+q5Jn2qPrUm+b5PEbBTPa46Ly+Lqz\n2j0l8o2uYTiPxvPOe9DOGNvNGPshxtjfMcbuY4wNGGOrjLF/Z4z9IGNM+zsYY09ljL2PMXaKMbbF\nGPsCY+y1jLHMOyNj7PmMsX+bvP8GY+zTjLGXV/0bzocKQ2jd4+n8YosFGG0QqVB7Hq05cRHr9tMy\n9FQRyfQCM5fHq7OPcUmxVZsCtI/snpZpz5UyUiM6DM1lqRmmfnaGPLos8GSE/e5jaNwAyXKP1zLt\nvSkKCl1ReTzqM6JjhGm3KzLtgSv24ebaFLdvwxp5ijxeN3agqSzQXnWuNAJx6XOcmqf5w42Uky4A\n3MjuSx6v8wgw5eW0V2HabY0aAJDPcUDI9IEpyhm1NDntphLGLMUPz/qMv/svgOffAvzQh4C9OQZ0\ncZHFXwQ6owWkybXID/SJILR56MDDyugh8Zr9T8STv+e/42f2/G+82vsJ4fmQYUQXX+eqzpZyLsvj\n6VgEC/0oJzmu1i6o0QVr6myphmlvYqFqmtGePJeyXmepER1Qv4O8mtMuz7QXB4z35xi9Nc60ZxjR\nAbIZ3X/cX4/UXFeerwfnJmqI4xtpFruOURdA/py7lgf80+vwskd+BYuIGuNFmnxhhjze9TeT03/o\nhZUUIFlGdG3miXWGJI8XJJUERltOpCggBphxw6GJvPFpJcnj3VgeP/tGISAD72KgfTbu8a5tgTEZ\nuAchlxVd52DVwbR/J4D/A+DJAD4N4HcAvBvA9QDeAuCvmaKzZIy9EMDHADwdwN8BeBOAFoBbALxT\n90sYYz8K4L2T933H5HfuB/AnjLHfrOHv+Jqu9IJ+cuJYFgJyGPibAtiwzoLEphaae1aYdmPpUJhm\njgCFLSTGISO7jxsuXpDmeVyb4T9df2H276CRb2xobIaRNWrgtjJAe7skW0y2s8+G5sCDzO5QAy1L\n11zYfZX59nXkBk1dkW8sEIsNu1WNaW/1xTaePFXvIivFtDsFjejI/m/VyLSn5NKJER2ZG9/Q74Mb\nLQHajyJq1OUx7THDa8q0Rw0v6mBETPJSRnRkP5kY0WlA+5bp4iDDPT6TaV++ErjpFcCFNxR7f8tO\n2HkLIq/YhLUJM5o0tHnoIMCFnhh9sFcejfmOi7f/4JNw3USh9PWXLEqzu7RJEauzKgNPSR7voNt2\nJb8UbJHjUhnVCUMuHWfzXVcL2psAx2Wc4wGV9dqGmfZ2sW1drNlBvq6Z9jymtsxst0nRnPZlBbQ/\n5xqhfnvfnYdrN9KKKwv0mpjw6fZ3XWoUyvh/L/sA8Jm34MbT/4yfd/4883er5Yf6nHZrvCGdP1W2\nOSunHQDa3mRcTpLHC5JqnVyLE6ApmZxuH2inpo5tOy2Pn1ViBSCnnhSRx9OGYt3NBfX6A0SKIunn\n5zhoL9aOza97AHw7gH/inCdnKmPs5wDcCuDFAL4DEZAHY2weEeAOANzMOf/s5Pu/COAjAF7CGHsZ\n5/yd5L0uA/CbAE4BuIlz/tXJ998A4DMAXscYezfn/D9q+Hu+JivL8RwAAlgJCGVk8eT0FiRgVmju\nmbprsoEx0GQZslQKPOyRAO2eM4ddHRfv+KEn46N3H8dlKz086fLduGgxB+yljOhMo5YwdZaUllsW\neLapImBgvECQnPgp0+5ogMfKo823ryubDtbVxbcIaHcqgvbe/BIwwSxrZ05GwDaFRsvVaBygTVnr\nLECnFNO4sAMRAK6yfX5GBBht2PCBXm1wNRPy+MM8WrjkzbS3EqbdPKZMly0OyOc4UHNOu+HiQDbE\npH4QGZ9xf4/++3nV6gGD6FjvYYQh2kZOxEHI4WqaNE5Lnml/FBMmltbkPF/stfD3P/KNuOPAGdxw\n0YL8WZMmhTVpsBg3PZSyiL8Gs130WzZGcJKmigTalebw5thPMnZ7LTsV+dZpMPJtWCKjHdie+VLq\n29BvlWHaqzvIS+7xtrk5me65P/mcq/GVI2t43xePTH42O3k83T8AcP1F87hm3zy+fHgNQy/EP3zu\nEL73KY+qfRsy5fEGf7vOwX2jJt8Hun3fxT+YPP5O52N4vf+qYu7xXD/TjtE6em0nuWZvjQNJUm20\nnYHePR4A2kEM2gnTTuTx2hgzNz2Ws+1GdAnTvk0z7VJG+/TPiW5nXWvGuALN2EbbtpIm0sgP0Tnf\n5fGc849wzt9LAfvk+0cAxIGwN5MfvQTAHgDvjAH75PlDAL8w+fLVyq95BYA2gN+PAfvkNacB/Ork\ny1dV+0u+tivteC4+emoA5RJA3JpbVpj2ApnZZDHVgmcu6ZZchvVMO93GYGI09vhLl/ATz7ka//nG\ni/MBOyCB4ciIrh6mPcuIzGnnROTlleIwbWzqR+XxEvCoi2mXTQfrMrmxQrFocsuqFCbVIUx7O9jA\nwdMZ5mElyiOz9yFzp88xx0Vl5gT0h9w8q5tW+hyfgHYqRVYzaeNNYuJmd5RHEsFU74AcQ2Xl8X6G\nwz2QbhJ4JOaxLGgv6x5vIo/3rE45I0dXk9VucI6H6qjBZDtdV2baL2eHxXMmxm5AxEA88bLlNIPM\n0vJ406aHWhJod9votR2MUIxpp42M+XhR6GrUFE3MtNchj58VaDeMfANk+Xcd8niZ6bLQJl4UJvPU\nVB7ebzvoEI+apuXxUuSbMtPOGMNLb7o4+fqvP3sATVSWPN5EZaBTK1RtvsVFwVGogRFFmPbAD7Qz\n7RivS+kHVYCdcI9PH9ttf+Jxs6U3okuZ0AHaqM7tzmmP/V62yz1eam4UuO7Q59RtHuoFctMQAFxH\n8Vo430H7lIrPFHoEffPk/w9onv8xAFsAnsoYtZfNfc37lefslKay5rABICCPO56IfOvOLQFXP1e8\n5spnTf9FStySKQixuAYMQ2bh2p6YfwzLRJVJTPugxEx7mBmfF/I0S+qWBu20uWCuWuBSbBWVx8vA\nY6u1IrmiFi4l3u/MllfdZRqAHQowXBW0079rDgN85Uh9DvLBmID2giw7AK0Le1xVYoR8P4TLNGCT\nbJszOoNpdYTH8vjp7vGm6o+0wz2NF1OPHfH72yUj35KcdlPgFOqvQzqmfdgu4QcBaM3oTIx5gpSp\n34RpJ0Z0LnxcRph2LAvQnllKDChQfbHvcLE4Z3YL/ZaNsQTahU+JaoopS+Mn1zFH4x7fxEx7WSM6\nl6SdzMyIzmzxDCixbzUw7b46016DPL5ls2QBDjQrjx+Mg6S50LItCQjF9aIbL0r+ri8cXMUDOfP3\nZcvLYtoN5PE60F4XK0w/50BjQ1Uoli8Yw2Kav2e0Xtt8diKP1/i4dPx8ebyWQdaM5ZwtTHu/7SSN\n9vWRXzuLnVXSTHsBebz82TbJtEf7JeWrsQPa9cUYcwB8/+RLCrYfM/n/HvU1PHKreRCRbP+Kgq85\nDGATwMWMsanoiDF2m+4fgALuQedupeTx1ACKMFpzoQDEvYUJaH/1J4HX3AFceP30X6RIU427kESW\nysg2UolvPxDAi7dKuJ4TMNzF2HymPccB29fcwFrdAgoFXdFoOpjPtCOkM+1kIa7I+NfnLiu1earp\noB/yWm5gjsS0l2x4xEUaC3vYKu4+Up+DfEjy5HnRuDdAKzOP68yg/MKZk3MnhJUAYp7R8Dpl6yXd\nhwuA9ngRZGxElzOmI4F5pcrPtEcmb6agkxHjNHp+WxrQ7nfKgnYS+1Yiq90PQ+11iJ7fbebjEnZc\nPGeZ3lYzilxra2PaqRGd00Kv5WDMyQKPmIuq8ngJtGuY9kaN6GrJaZ9R5FuZmXYqj296pj0ICzd1\nKehzbSsaiUjepzl5/CnSuFjqu+kRIUTqhJseJRjZh09tpZ5TtbLUBCYz7TqJel3Hosy0p4+1IqMQ\nzM/Yb6P12s6fPHl8J2baM4zo1nXyeGpEN5HHV1HHlS2ZaY/ODde28OiJUSLnwF2H6424zap1w2Yh\nvTbV3WjVzbSnGoejelOEZl1NMu2/jsg07n2c838m3481tavpl0jfJ8PUhV9TwN78/KyUdJbOkpLH\nyxAH9K6FlejBBdcVW+wBKZbLeKZdMoAi8ngCPBaZ2EbWKQHabTdpArgswNZgOOUFcuWpFnyNTUS7\nUxK0U0UAGxrNuwJq5Fu2Ed1gvuBnq5YS+QbUI7F0CdPe7lRk2vc8Jnl4jfUw7j7aFGg3YdrTLuxx\nVdl/ofR5k0s7+X29QFxCT3T0s5hHk5l25Qfk3I7lhmWM6ByWZoeBKaDdhGm3HeEzwSJm35h1yIp8\n0/hWhP29Zu8dl0sd5GPQbmBEl9E8ZLYjKX7i/X3KXpEaBZmlkcdXYUQ453C5+Lssp41+W2XaCdul\nMu1UHh8zw5qZ9iYkoSOvujx+4DW/qA9DLi2Ae64NPPxpYDPffHPPLnFO1wE+pZn2SU5yvHjmvDjo\nHOeAdr9q/GBOSRntvexG7Mqc2G+na1AoqJW1n6oy7bW5x/N8pr2QqsLLGFUbbaAnSagryOODWB6f\nfo9usB5FTdLkCkJE0GvxrkQeT5n2WB4/+8i3ETWiI14b118k1mRfPJgFl+qtDWl8yYxpr1sNQBUg\nrgXg0B3YTfDCeEcery/G2GsAvA7AVwB8n+nLJ/+btFMLv4Zz/gTdv8m2fs2Wn8e0M/2JtrS8Yv6L\nJHdNc9AOIo/PYtqXIORodhlZNyAt+vyRmbwt5Nn70td0nefnS0j4AVkej6G5aiHQs4Vqnry3WBa0\ny5FvQD2gnUpp252KTDtx876WPYS7a+w+hySarmhGe/TcHHl8BbYrzIj4o79vkZw7g84FQFvuc465\njZOIQFOKaXdp3M2o1PbmnTtaqeSkjEA7kLoOmcvj9bF0OqadzZUwoQMUpj06lkzO8YBz2LpxCOiv\nQ8dblxR7Y/r3xkx7hdlDP+TScR4z7Zkz7TlM+0J3+5h2E9A+a6Z9S9lO+0O/CLz1W4A3P0MGJUpd\nQ+JSv/RI9WujlukqEftGWWLXseASeXwh6XXJogA8D7TTWfeTG/WD9jrc47WgvS73+ClMe5HPyAoy\nyJLRugDJqCY/j/dXJtNOz43OgnTtk+XxOtC+ne7xaaYdAG4goP3OR2YE2nWKhJySmPbajejEfnn2\nxt8Db74Zf775/4gowh15fLoYYz8C4HcB3AXgmZxz1a54Gis+rzzP5DWz0YOcgxWm2GHx0esuugFn\n2L28lPr+1JKY9rEx0GSZ8nhxk1winTOnW4JpB6QFczDaQmhwMzRl2julmXYFtBvEQQEAiCSVSqRt\nBbSHyyWc4wHFiG4LFsJK8u64WhS0lx0tiGv+IvBOdBzPsy2MTz4kdamrVOiJ7TSTx+uN6ICKEtVM\n0E5UKgS089YcML9feotjWAKf3BZSRnTEhKeMcRqQ7XAP5DPtbac4YIreTFb8GLOwGc1D3Uy7O58T\nL5lXbloeb6KmSV3Tyb70NI3YM92CoJ3cG+pg2gMFtMNua2bas43oViV5fCxTTYP2JsDxYCz2r9FM\n+4xBeyqj/T9+P/pi9QBw65szX3ftvvlEUXPvsfXKjQ/tTGmJuXZPAiVMYtqLOJOXrdOk6aya0NGi\ngL4Rpj3jbwyMjOjS71GX8RfdvrAk084I0+5Z4nyGtyUBu2qgPduIrhOsS/PsnMyzq793rp2eae+e\nNTPtYl9R0P7FWYF2cs8qklrRbzBPnja1bti6FQDQwwDfYN0FIGbaz+3It1pBO2PstQB+H8CdiAD7\nEc3T7p78f7Xm9Q6AyxEZ1z1Q8DX7APQBHOSc1z9c9DVSQciTzF0AivQ8faJtoIde2wCIxKXOtBsb\n0ekX9FlMu9srNxHBFJnThsGiNAhCyW17qmrBrR751meDSpn30ky7AtqtPanTqlhZNtAWTM0ubEmL\nnjLFOYdLbrCdbkWmnTGwfYJtfwwexAPH6+m0hiSaTuvIn1U5M+2rFRaAoRTxR45D0rBZkkZL0qA9\nNqEDNEw7aXTFQMmYac+ZabfrkscDKVDnBdyoWZMVPcmctKKivVBSHk+acmWaIKnmoZXfPNzoF4ym\nqplp94IQLUaOE7uFbsuWZ9qJEd2RkdwYoZ4jiTzeTc+WNgLaS860d8nCdBZGdJsSwFC2826Nf28Q\n7dN+28EVK1FjNOTAlysadU6dKS3Ikqsz7Y4E2mcjj1fj3mgtz4lr+KnN+uXRWX9jHUx7HWaxtDlT\n1ojODgRoH7hkGtYbCJCMamxs3LhosfQ52PHXpLGcO09Z+Nm//UJyT5Pc4xOmndwD48i3s4hpv4Y0\n4e4/vjETF3kpatJ4pr05IzpK/uxm0XUtYtp3QDsAgDH20wBuAfA5RID9WMZTPzL5/z9pfvZ0AD0A\nn+Sc0wDHvNc8T3nOTmkqM6YMcuRbXJtWSYbTkaN4zOXxhGkncTGUyaQy2la/pI0BnSfF2Gg2NyBg\nOASThn8DjWqB7hOjkpj2kbkMK8M9vqOwu62Vy8ps3eTNiESebVYCnUDUQW4T0F41px0AcOHXJQ+v\ntR7CgZqMgzhh2nWzzpmVI4+vMl7AM5h2RrZtgYm/3WrvAhYukt7jCBfqmpQJk4Zp3xj5RjOmedch\nSZKulJERHSAz7RPAuGUAPKm3BrOzkxcAwF0oybRr5PEmM+158Xm669BwV0HQXvNMexBytGhDxnbR\nbzsy005mr//oU8dwx8PCHGptoIl8S5kNNjPTPqzFiG4GC2ca96bmyR+6XZaEfvL3gV+7GPiHHwNQ\nr6RWYtrt8vJ4daa9NSN5fF7cG63lHgXtmqzxipUJ2o1y2tPvwTkw9KrvP+rgz3VMeyEjOiGPH0qg\nfUtqPFWRn8fHo45p7wZrkgndyXAOf3nrATz3lo/hkTNyqtAujcInYdrHvpFSs2oFIU+aN4xBGh3p\ntx1cuSdaN4Yc+PIMzOhoU1KXtqBWkzPtVF3ikM9890SEvRP5NinG2C8iMp67DcCzOOcncp7+NwBO\nAHgZY+wm8h4dAL8y+fIPlNe8DcAIwI8yxi4jr1kC8HOTL/8QO5VZKdMimj1spU+0kV1yDruqEZ0k\nS6Vsob6D1ykN2gUY7GFodGOgGc5cOYVqZdoVkLQxNFscsIDcqKiyoi+DjH63ZFMBALrEjA6blZn2\nwThIFuEAyjc8aJG59uvYQzi2XtMiizLtGgY2s/KM6KrI4yWmPV/SDQBOdx6Yl0H7UYlpV15AjuNd\ntthOU3bYyQTt2e9ThWkXsW8G16KM6EkdaGdzZY3oNJm/BtfLdE47MevSNGL9xcuLvTFV5bDYPb4a\n2yUd504bvZadOdO+FnTw0j/6VMIIykx79mxpM/L4kkZ05LmDGkDStKLHzZ6W0jjlIXDfh8TXH/x5\nwB8Ct/8ZcOZAreZVviSPjy4gNK6xaFZ7HtNuAlxN60zBmfalvjh2TzfCtDdjRAfUI+cOCVuvdY/P\nyJmnRZn2kQraO/UAu7ihrJtpXwxOS/L404jWvEfWhrjlX+6ZakTXt6JjhXPZU6LpUll2tbl+w4zN\n6CQDTFN5fM057XR8hDLtyxLTfp6DdsbYywG8AUAA4OMAXsMY+yXl3w/Ez+ecrwF4JQAbwL8xxt7C\nGHsjIob+GxCB+r+iv4Nz/iCA1wNYBvBZxtibGGO3APgCgCsB/Bbn/D+q/i1fyxXkGECFGqA5dkrO\nipPFcgsexn5oKEslDFfGTDutztyi9vtTy5Xdh02Y9ixGE8gA7b3l9PeKlGUhcIQiwB+YyXp4hplW\nuHQZ/pf/ItwZXobvHf9s4UxfbXXk2LeqRnRbo3EiZQvBJIBbughov8Z6CMfWzNICsor74qZgGYH2\nHHl8BdDOQ3pcks80Q7rv9hZS8vjDefJ4AjLnCWg32eYwhHIdItuZA9qNctqBlLcGYCbxlpqHjIJ2\nzb4s6x5Pc9onygWTBWq6ASL2pa+5DrHFojPtaXl8lcWVH4Yy02W30G85GNPGgicWUhvoYByE+Og9\nUVSdPvItDdpHfmgEaoqUnNNe/BikjNNgJky7+B17HY2S6Mvvjf5XpdHjDQm033moGjMnu8dXmGkn\noLXlMCXyrUGmndy/KDBXa3dfXF9ONTLT3kzkG1CP8oM2TlK3CfjF5PGEafecOXHdCX3MueL9qzQZ\nEnm8BrSv8JMS036GC6LqtodO6w3WKJFiiWNllhJ5yTlec1+8br8YWax6PhcpY6Zdkcf/+70n8Ovv\n/0otCkipaUgSS76W5PEVVutJxe17G8BrM57zUQB/En/BOX8PY+wZAH4ewIsBdADcB+AnAfwvrhm6\n4Zz/HmPsqwB+ClH+u4XI7O4XOOd/WsPfcU7UkdUh/vaOgxiMA6zMtfHyp15W6HVpIzoCiDULvKBM\n/jmglaVujoLCRlIswwAKtv5Q7c+XMMsDUiyXCVsY5jDtagNk1FpCu1tyGwGEbg+2Hy1og6FZXJkl\nZU2L7WIAftv/Lvw2vgtACUBEi5rRYUtiKsrUcCC672O46Ghyco1r5WoEzIXNPVzMTmD99HEAj5n6\nsrzinIMFY8QkAysrj1dGFVZrksdTpl3neA4A7f48MC83lI4SebylHhYEKM1Z4t5qUpAAACAASURB\nVHNeM5B0B1wxoqOKAJ4jjzcG7bJ7PGDGFmfntOtAe4mUDUBrRGfOtOckgpC76BrvoVu0wUk++Lgp\nUIVp91Wm3W6h17axCv1xuYnos/vND96NZ1y9Rzq+FjSRb3TxPPCCak1IpYal3eObM1vS1cY00H7P\nB6M59kA5V8MA15JF/r1H1zH0AqNRAFoUzNU50z4reXwZpp1K6uuqrL/xbGHapdlhRXrexbBQc8Yi\nTLvvdKMm5igCVwuO2MYq2xtvp45pX8EZYF3YblHQ/uCJTelv3NVJp1b0mfjc14ceLlyoQRVYoCSm\nXbOuvuoCsXY/dCYjVq/GoveGIqDdtS20HAtjP0TIge/9408DAL74yBn8+Q89pdK20OuPBNpj9/gd\neTzAOf8lzjmb8u9mzes+wTn/Vs75Eue8yzm/gXN+C+fZKzfO+Xs558/gnO/inPc55088nwA7ABxd\nG+KNH7gbv/eR+/Cu2w4Ufl3atIi4x2uk57xdNkot7epr0oW0pJn26Wxhu1dyO1sUtI+xbuDMLkdr\nyaeQynB5RSWpGcXJXDs37RCG8hxpXKq8MDW7bFJdhWmvIu8GMBqIBafHShgh6sp2sbEgzPbap75c\n+S29gMMhCwEzpl18FioDUMV9n2fJ492Mc6e/AMxfLH2PGtHlzbT3WTmmPQjDTMXP/eyy5PExLgPM\nWc+0gzCG9DpkuzLQ9OEAZZtyLeqrEYF2E4AXXdP1xp3qTPthvlwoQ1d9n/izqhq71GIyaO+3HIy4\nfns2eXQPufORNRxZG8oz7ZrItx45FuueHy9vREeZ9uZB+zr5fJZtzaJ0tAocuDUd/+ZtYb7j4vKJ\nGZ0fctx9xKw5TKu2mXZfBu2zksefKpjTTn92Zmtcu8JjnPE3+kbu8frn1tFEooymy+VzrotxocaK\nQyLfQrsjNYXnbfE5VGLaY/d4pn8PfvSu5PEZyCOhDxPmN2kESkZ0ZERshg7yknO8ppm9u+E4QrXo\n8VTEiA4A+hpw/4n7TmqeaVbycUmN6KLr3tgPAcOI57OtGslp36nmqldyMZCSx0tMe/oEsrolZ8WV\nmXYARoBYjnyjoD3NyoRgYK2Ss/eUaWcjaWE4rSSmXdl3KtNur5SMU5sUa4uuKR8ZLqZIp1GKg6qT\nqaDyeGxWZtpHowZAO4BwSTRPnI1Dld9vMA4Uya+BjL8hIzoa+calmDL9fuzNLabl8cibaaceC+Jz\nNgPtyJxp/9wNP4PjfAFrvIc3zP136XVV3eMBs8VfVvSkrTRnNt2ltD60aBHQ3mdD420MghAO01/T\n1evQEb4s2KJppTGiM7k+prYzDOXmlO2i37Yxgv64XIdYvH/pkTV5pr2TZtq7ZPFcV6RVXOWZdmJE\n5wW1OHbnFW2ML7OMRem9H0yYzKQmrNOly+LcrsIcU/CqZdpLzrTPSh5/pmDkm2tbSRMs5DAarytS\nWfJ4k+ZA1lx53Uy7qzDtPTYyzmkPnK7smWLRc7qaygfQG9EBAD/6xeTxaZ69ltQb0QlPm9nK46eA\ndpJscLIBFQgtPwiTc9pixVWbRWbfyxQdz7HJ+neZ7TDtO7VNVbaDn870pfPi6cWc3Ss5K65x9S09\nS2pT4JHexi10NRregiXlbY6MXJslGbJyCvkKw9Xee1W57ZuURWLfmOHFhpHtpMDjsftKqhN0pbjH\nV51pH5G5fb9G0O4ui3ne7kCXRGlWW54vA5GSM+11GtFlMe068zQA6M0tAJ35hG0P2ws4RuXxKaad\n+kCIBYsJoAtCDisDaL78uU/FT+z/c7xi99vx0z/43dLrqs20T5h2I3l8BmhXVAuDVkm/CkBrRLc5\nKh7JFKopFuRaqHprHOK7i8s3LQ1oN7g+qqU3olNm2kltcgLaD63JOe2JER1pypAGUt2LZ3mmvTho\nj4BmdP4EIW8UaAIysFkEae5So8l7P6hh2iP5rEQGVDDV0hnRyfL4Yu8tzbSTfQnMjmnPi3wDZFBf\nNziqO/KtQxIF6jD/kkA7V0A7RoUMB10C2rnTlZVcBLRXcY/3NfL4gxAeJNbG0eTxKuaS+ENae3a1\nsWducj8h29gmAVezzGqnM+26ZjY9Lk9vjRt1tqcGfL2WU1i1WecIEy3p+hMSIzqsw0L4NTHTvgPa\nz7GSZuUMbq5+jpRS5x7f31VyMWq3EE1NR9mYFkJsGDDtdA7bmsK0D60KcWASaB9jzcg9PlsePwjk\nr62VK0tu4OT1HcG0u8FWYaYCUGaESWPm8pU+fv5br8FTr9yNd73qGyptnySPR3V5vD8ic26WARCe\nUt0VEXe16B01iinT1eYokOfRjXLa0+7x8QLX1LiRFsvIac+S7lvdSfPmRW8CrnkBvBe8SYrhyjOi\n63Cx4DIyolNn2sk5Pt9x8Y4ffhr+5jXPxiXLPfzOSx+H6y+ax2+8+AZJHluotO7x5XLaqSeArXgX\neO3dZttFi4zozE2aIKFBJFOet0agjDyttfbmModSaYzoqjCJUeSbzogua6ZdfHZ3HlqVDaHixZ7k\nWSDPltZZZd3j1ec3LZGn+2geZFF67YvEtenYXcDRL8kv9CJlU13bSq+rCdNeQh4vMe2KEV1TM+1D\nL0gaFq7NpgILFRzVWZnu8QYNCyqlX+yKba0jZsvPYdq7GBYzogupPF4G7XNEHl/NT2Mijyeg/Yil\nj+g8zefw9Kv3pL7//U95FKxYdkbWjS001yzMK3oOtTXXpLZjJ8duEPJKDddpRRtARebZk+e2za6l\nRUueaSdGwYxjERs7kW87NfuS818NmHbVtIjKujUz7fsuuKDU9oGxlIO8SaeUIYtpTy84h2Wz5IGU\nPN6Mac/OKN1SQDt2V5THUwkthkY3XMmJXzHye+XTr8BfvPIpeOJlFZhCQDKii9zjq3V2xwS0BzWC\ndntRzG7vw0mcqDjrFUXTyUCk+MYQ9/iJU/4uMm9cdgEQZqQF2BlMO+LRkituBl76DrSue77045QU\ns6VnGYyM6MLsFAu1XnTjRfjHH3saXvrESwu/f1JaQ0yDERiiUrEdOtMuH5NBL73IK1yuOLepsV9R\n1oYTUzE1xSJk8mfOlDGI3NIy7cUVAGp5QZgyouu2bIwzZtr7c+Ka8ukHTiaG53NtRzRvCNPe5tQU\nsd7FM22gmJqz0fnOKmxhkZIypUMigV+4CLjsm8TXd75bfuFkAdtphGlPu8eXjXybBWinwHux15rK\nGlLQXrcZXS1MO5HHU9VAFRAcl+RdoDLtbFRIWeJITLs8096rST2jM6I76uzTPvcg34MbL00rTL/n\nKaLhL4H2cIjY7bNJYKyWJI/PaGbPSiK/ZWhCF1c/Qx5fdYwoya9HCFvxWtjN1uCPxySqtwaT422o\nHdB+jlXbsZIRyrFBxE0QhLAZeS5ZmA1VoIkKM+1Apax2RoAHBRtMAzzGdj2gvVOBaecK057qoyxf\nUWrzkiLy+DlmlifPJCO6ZuRI6kx7yKuZsngUtJsA4WlFZKL72EkcW68W+7Y59lOS38KlkcfTeePS\nC3xDph2KH4S6SE01BcmCxQ0FaDeaaefZMWW1lptmYrcMjksal2jnNUDKxr0B2sg3oHhzISRS4zRo\nl/drd6Vg3Bsg5b23rOieEYS8tIFVoDGiazmWVkkz4C084Yo9CTtLr8uJczwgMe2tBmWqZY3oAGDv\nLvH3Vb3eTCuqZusFRB7fXQau+hbx9Vc/Lr9Qw7QPK4D2aTPtRUF72oiOusc3I/c9vi6Oo5W56ddz\nakZXN2jPAudGM+0EOEugvRamnXw+XGXaR1LDIKuYL+71VrsnEymcXg/LH49J5Bu5/pxw0w3MR/hu\nnMI8lvstPPZCoW58xtV7ZIWSZSf3bwaeNO5nKY+XmXY9hGuyoURryzCjXTxXfy2tOkYUz7TrIv52\nszXZzJmdm/D33Nzq87gYY+i5lG0vdrEI1PlHskDvtDU3qE4V0C5LU4veJDjnAJGlOgRo6ph2Kh03\nLiVuyUT+mWdEt5edlp/crrCNAECi93oYmpn6ke1kTYGjLp1pjxaAVczogrG4kXO7PqYdCwK07Gcn\ncXRtlPPk6RUZ0cnmWoVLI4+nTHtZ0E4/b54zhw0AI9aZ6gcx8JTtIOeMQ+J6jEB7oCp+GroFaWba\ni8rj/SCUUiwsch1yXHku3J6vAtqpG78AdYWZ9pzrkBrjubzPIMWCHDvUcL4sm+QFXDlXJsejJiZx\nA11cMN/BVRekTaHoOQLbTY4dG0GS5NCoPN6ASQIgeQgcXm0atIv92/XJ3HpvOV/tVbc8XjPT3q6a\n025bksS+Kaad3hNowyWrlueaA0ZZ+6mse7wkj69hVENm2uW/vYdiRnSuL0xn7fac0hQeipGxoPzI\nmK8BcafbF6Wed2cYXR8Xuy28+uZopHGh6+INL7xOs+HphvB2GdFlparIDvLV1jp5tWWY0R5X1uhJ\n0aZeVsXy+LbGeHA31sCpNH4HtO/UrKpLOlpFb7CUlVHnHy9Y0gDLpctKbRsARZo6LnxB85QFPWW1\ndPnIbr+kWR6gMaIrx8KpTPsV7mn16dWKsHFzbGAYn0cXyk2BdmFctoxIllnFjM4fE8mcXWPuaX8l\ncaNfYFs4dbpavMjWOFAkv9WY9jlJSltu/0kKEBrbpQFHnjNdpZJiVu1WcqOzuJ8AJZOGVyrFoqlm\nkpM2KivaPBz5IWym30Y18q21UHKMCJDk8TKzVJRpz5HHK/t1/6UGYzrkvTrkbcs6yAehakQ3Yao0\n58wG72C538J1+9NmmfOUaWdMYtvjhIA6ZeiccwwJWOgYmiHuWxDbd6Rx0C62s+0R0N5dAno5vgvj\nCWivSR4vMe0VIt9keTxLABzQnBEdVUMUAu2EaT9dO9Ne3T2e7qe6mfap8vgCn3M7EKyn3VuUmsLM\nH2CONOnKsu1xE4l6aqx20qD9C2GkiFzsuXjh4y7Cx//fZ+Jjr38mHrVbc590ZD8k4Oxm2puUx0sZ\n7RSIjzaA970eeP9PA366aZA10z4q6OeSVeLzTn8ey2wN1lg0iqBJzToXage0n4PVbREn0KLskZ+9\nwNvVVcCR2wMWH4XSpTDtReXSIz+QzfKmZE13FvWzSYWKgnY2NpvLJfOu6onfD8SCSRelZ1xEHt/D\nyDC2KsPUr86aE0zjClsDwCuZ8khMu4nkfFoxho2W2NbBiYcrvd3m2EeL1TDTrpHHl53LpUZ0lC11\nNKA9bE1PEEi5DDMmA83JgsVMpaIa0TV046zAtI/87Cx5V5lp7y7qTY0KVYuO6BDQXlA9xXMasW0+\nkL6+8hKDayVl2m2xOC/NtKci3yagXXN+b6KL3f0WrtufVnrNq5F11EE+jhetkWkf+WEyT99SssKL\n1EyZdvJ3u+Mz4gfdZaCf47swYdopG15tpl2cN3r3+OmL8jDkYjaVRTJ71/A9ytQxyrTPF5DHNyhB\nzjKjLOsevyCB9vqYdoZQXrMhIkGKfEYdAtrd3oJ0PYQ3kOaeyzLZsREdbRpu9S5CyOVRsDt5xLTH\n++mS5Z60z6SSUlQmzcLtco/PnGkXx++pBrPaJSM6Oj50x9uBW98MfPoPgX+/JfW6rJn2soqKuOLj\nkpqTxrXC1sA8MjpUNnlqm+vc3OrzvHoucZAvuBD1iNRVZYdTLOzK1dUOaLIY66A40z7yw8xYOksj\nP55fMTBXUosw2B1Dpt33skE7lbOzix5fevPE+xGmHQOzmXYaW2Ui3zap9nzSee6xEfoYGsml1Qo9\nsritE7QDGPUEwArPHKj0XpE8npx7GmCcWRp5/HynOtNOZ9qlrO2WZtvmpsu6teCRLlgmQNPkmJwZ\n0+7KBo5A8Zn2qHmol/C7yr7sLVdpHPYRm+G0+TBRLhRdVMspFsoM+0hWkqQAb16Rz6RtEdBe8rwO\n1Mi3GLS7aSXNJiKm/esu1oD2rnKsUF8SVj/TPpTm2c3vh/sIaG+eaRd/tzOioH0J6K9kv3AiF6VM\ne10z7TojuiIMrEfnpW0LjLGZyOOPrVN5/HSVF5Ugn6rZPZ7eA+i9oexMO52/N4m+zKo8RrOoPL4b\nCqmy21+U7i3wtmRz1pKgOFYb0OuP3ZrDKcjq0i+Gl8O2GHYViSJz00x7lXQN05KYdkff9N49I6Zd\nMqKj7Pln3iIe/9uvAYrBXNb8e2V5fDwOwdLHyzLWYHk78vid2oaSpWwFpZRUOqt+7OrCee+1pbcN\nQHqmveBNIhe0a0CRW2WelDqVGs60e0S1kGpuvOhNk+87wIv+oPz2xUWaAH02NOroWpJ7fEOgnTEJ\nAO5hZypJBfmYgvYa5fEAgnnhIM/WDlV6r82xn4qxKlxUHs9qnGmXIv6IpFvT/HDnp8u6tQ1BRaEC\nmN1ovSCUjeiakqgRoLKbReqXogu/kZd9HVK9NZxdFeTxliWNlywiWlAUla/6njj+uHId2hWcKr9d\n1IiOyuNLNpP8MNTGI1quTh7fxe65Fh53ySK+6dEy2ExFrkn3mfoZr7IZ7XFdOE+Z9kHOM6tX3Bi3\nEMIaxWovFnmOuN2U6WRSk5z2JmfaWyQFphBoV+bZAUhGdE3J448TefwFhkx73fJ4eg9YJIC7LNO+\nSEZL6pByC0YzfU3osRFCjqmxqj0upMqtOVkej/GWlL5QGrSHHAxhktICAK12G3OQz8dTmMdC1y2W\nM66MVgKzBe3STHvGyM72GNGRa2RHGV09dIf0Zb8peXzeTDtbg+XtyON3ahuKnhyDcbGDPKALPPVg\ntRRAt/expbcNQCpuqSgIGXkqw5UP2is5N0uRb2OM/LDwvJ3v5zDt174QeNUngB//PLByVfnti4s6\nTGNoJBOTmfaGGE0AmBPAZS/OVMpq59RRVsPEVSka+9bePFzpvQapmfaSoL0h93hJHq8ZLWkvTWeI\ntYyMYuAImIH20diHJaVYNHQLIpLgaGyjuCop3TxUDNDiRprby58XLrSdApguT7az6ALVk65DivEc\n1lG6yLEjM+3lF866BpedybS3wRjD7333jdLPKHMNQJHH18+0F8po94bAl94DnLgv9SM6096kPD4M\neTL6sUAz2jsL4rPMOk51RnQlmfYw5AmhxhiSbGtTebwnOcezyf+zZdr3FGDa6Ux73WwmPY6XiEw7\nMPjbs2bay6ZASO+tiVKLqztRNk1z+Z/jgvXs9JdTTDv1eSk7h++HIVyqiLNb6LUddFh6jUIbG7ml\nUfhUWfOYlsy0F5lpn40RnSR53zgmP/GL75K+7Gca0VU7NnUeBnHtZmtwfAraz034e25u9XleclZ7\nQVaGLPDUmfbUXOmea0pvG4AU016Y4UrNklK2sJzEN7O0Mt9iF16fzrTr5L0XXg8sXJz+fpmikW8Y\nStE+04oy7VajoJ0y7auVjOg4MS3RyWerVGe3yPveNTpS6b02R6p7fDl5fAsB5tqOwrSXNaLTM+1M\nc+5YBc6d1Ew7oGUZTBbRVKUSoMFON/VaQMQ8Flf8ZDcPwRjw7P8RRTk+5w3VDR4JmFpmEdAuKo/3\nPQIUchYgXsvQsJP8va0a5PG+6h4/OR6dVprNXOO9ZMG51G/hn1/7dFyx0scVe/p44eMUAymtEV19\ni+dU3Js/Ah78GDCaNEQ2TwJv/RbgXS8H3vLNwJrcCKRz0cfWR1OZx7JFj+v9LcIgEhVH5ly7Nqe9\n3HbqWHaghDxeyWgHAJc09zwDB3WTOnYWucfT47gs004bJAvUPb4Gpj3MkcfHkvG8z5r74yTmMuAM\n3bl5mWn3BrI5a8ltjpIr5IZhr2Xj/cETk2+9O3gaAMXoMq+cdLOwbqVFXkkz7RmgfXdfHL8nm5xp\nl3LaJ59X4ANrj8hPvPPdQJjBypOqKo8P8kA71mBT0L4z075TsyrJPb5gV5xGvqXY4ZQ8viLT7iqg\n3WCmPcukSs+05xjsTCvKtMdzSQW3M/D0jGYjRSPfmBnTbs1iph2QmPY97EwlqRjzBSNl0c57DdXf\nK8wVl4JjRvOBag08xYiuQk77W15+Uy1Me5Y8PqWkATIbXt/+9cIn4kU3pl12JS8INn1xppZHbvK1\nGDVmVV9uJAFm7vFZRnQAgCe9EnjNHdH/VYuC9kn6QtHmAmXauXIND79NmP+4L3mz2TZR4z0K2kvL\n4/WRb3YrfX5/CjdIM7yPuXAXPvJTN+Mjr7sZlyz35Ce76YSA5mbabeAvvxv40xcAb/8OYOsU8Lbn\nAYc/P3nyKnD7n0mv77h2MlsahBwnGlo806b4hS4B7b1l8Thrrl2X016SidVltAPmOe1jHWh3SE57\ngQxw0wpDjhMblGmffj3f1XYSJcDWOKg0VqAW/Uwlpr2kPH6pT4zo6pxp1zDWMRjPU1V4W2vJ4w30\n0HKdXNBettGQSq6wW+i2HPz//kvxULgX97NL8Sve9wCQ1Qi55aabhWtDv9J6wqQKzbQ32FCipZXH\nrx2UopsBABtHgUOfS77MNqKrBtrjY76tOS6X2RqcgM60n5vy+Abpt51qquSc9oKgnS7wVFZmoMSU\nkUzrUqXMGhZeLOfI43VZ03WB9vgmU4ZpZ42D9vJGdFlZ07WXAtoPVJGKEdBut+pl2t0lwbTvwymc\n2RpLLqsmlY58KyePf9Sii6uu2I3jRJq5bqCmoCW5xzNF0q3WnH4W+5e+/TqsDz0wxvCTz7k6/QQN\n027i5uznxJTVWt2l6PrBA8yzLbTgYWtU7BwYeUrzsEkZnYZpL6pMojPtamPBuvF7Io+71i7gqm8x\n2yaWAdrLyuODUHuuuMr5fZwv4LbONxSbKwWaZ9rJ6FnfAXD/h6MvDt4K/ONPACfull9w+58BT/8p\n6bO4cKGTSKcPrw4kR/m6ijZyL2htIRnX7RYA7eP65PGyc7w4Z9qGkW/STPsE8Dctjz+1NU6A6ELX\njZo0U4oxht39No6sRfesk5sjXNzqTXlVsVqrZaadyOMlpr0O9/h0/nlcRRRYw43TiLdog/WwACjy\n+E30qRFdyWacF6STK3otGw/w/XjG+BY4lgUf0X4qI49fbvmYTANgbeBJPgdNlelM++mtMTjnxa+r\nBqU1ojv9kP7JJ+8FLn6C/FylRhVMMAHKtOsi3zbQ8cn40I48fqdmVd1WCdAeUFZGOWFO3i9/XfXk\nVmfaTfKRMxgudS6Xuz1JOm5cGnl80UUpbYA0Dtpp5BsbGRrRUdDeJNNOWE2sVnKPZ4EArzomrlIt\nCOZ4PzuJkxvlZ702RwHatcjjazSio2oaewpoz2h4LfdbeNt/fRLe+gNP1C9AFANHILpRFmUZxmPq\nrdHg7ceyZDM6rGFz7IPz6ds58oPZONwDCtMey+MLgva8MR2nDdz0CuDrvtP8ek6ZdladaQ8CH84k\n9z4ES7bVbcvn918Hz8DCnAHo0Ua+NcO0rzib8g/vek/6BWsHgfs+LH1rFg7y9J6w1ybyzyLyeK++\nnPYiTHuhmXYlox1Q5PENgHZTaXxclNGsS4Y88oOkuWFbTGKci15nQ+WaTJMX6pDH580Ox/eFvAbN\naFMQRVtscs6rTHtN7vGSCabTItJsJjVBaHMkt8h1Z8kV58qs5tpHBWbaO66N/uTv9AJeOkZ2WmmZ\n9jNZoF3gjLmMmfZhZff47OMSAPb4xIB4B7Tv1KxKNqIrCDSlbHHlY695dlg3015ssRzCZnrQ3lJY\nGVaFZQe08viznWnvY1BaHm85s2Paq4B2KxALH6ddM2hvz2OEaEHWZWOcPnN6yguya+D5MntYUh6P\nyd9bR047OAVx5LjUNRQymPapRc6bOUt8zkUX0r5PQXvDQi8ikV9hqwh5dv4xrah5SM3ympTxi8bC\nEjME7WSmvdbrEI24q0EeH3oCEAXMTZoIKmh/Z/BMCQRNLYcmGUS/Y2sc1CZTpeB1hW3on8Qs4Mbv\nE19/9q3Sj2eR1U7vCbtt0lyg8viegTy+JGinzG72TPv09x77U+TxDbjHHyPO8UUy2uOiSq26DL9o\n42lXx5Gd8wse23Tuv2Vb6Lo24o9k5IeV/RXy3OO7LDaiy/4d400RS7jFJmuclBGdOCbLgvYg5PI2\n2q3MuLGFEkz7Ykts15maY/+yqgjTDsieC1UIiryiqo1kv555WDxhF4llPnV/+rlK1cW0645LALjY\n+6r4okn1aYO1A9rPweqVYNpDSR6vLPBu/jnEecH43ndX3bwUaOe82HbmMlwqW1jFhC5+v8l+cFkA\nB37hRanMtDd84tPINwyxYbBwtgmobJZpp6B9tfTiHgBswrS7dTPtjGHdEaZcm6fKO8hHRnR0MWCw\nfzWgvY6cdh7Qz5ueOzWOllDQbotFStFZNM+bEdMOAHPUQb64Gd3ID+TmYZMyfsK07564xxeVrwZB\nQ9chiWkX21JWHh+SRlzAyHzu4qNwIIw+o78LvhEH+AVY7hs0v4gUedkRx1VZKa1aFLTHowup2n8j\n8I2vFV/f835pdpM6yMcy6rqLNnmWGAHthYzoItDeqQG0ZzLtxvL4NGincnu/ASM6mWkvTmKsEDVS\nXZ4FGypoJ/syKPi308aGazMwxqQ54s2K8/eJDFmTh10kVcTfXE0eD+0JaCcERTTTTubwyxrRpdzj\n25kmaIVn2sn6dsEhoH1GTHsR93hANqNraq6dRk4n+5XK46/8ZvGYMO2ZkW8NzrQDwMXBQfGFzuvn\nHKgd0H4OVqfMTLvEtCsnzMVPAH7kVuDVnwQe/ezqG0jl8RNgU+Sim54lpWyhcoJVZdoZk24SXYwL\nSysDCRw1zLTbDkI7uknYjGM83JzyAlGzm2mXc9qrMO12SEB7p2bQDmDgCvZpeOZo+fcZB0r2tAnT\nTo7lyYx3HUZ0nMy02/R3KICOdxYTB2/jImxIKaZdGtOZLdMOZDjiKxXltOsNMWsvwoAuxfL4Eokg\ntTLt5HNxiDy+7HnNiboiJAulXruFbxv/T/yX8c/hZ7zI1O+iRYNznoDSPY6QhVdpGtKi4HUxC7Rf\ncTOw8mjgsc8X3/vwLycP5az25uXxCxS0dxbE435W5Fv0fEkeXxLQyTPtGt09uAAAIABJREFUVeTx\n6Zx215aZ9iLKvWk19AIcmzRSJKZ9m+XxEtPedmFLDYuCTDtVK0z2P50jriqRz3Ppjj2C8u4JwZZg\n2od2HKFJmfaBBOzKuscHqbhJVzrWaRU3ohPNwnlbbNdqhdQckyriHg8gMcEE6o8kjEvPtFPQ/kzx\n+OT9iDMhM5n2mtzjs5h2SR3Z9PqjodoB7edg0QO+6A02DKcsQvdcDVxwXdVNi8rRuPoWAe15rs1q\nV6wqaAdSplpFZcnhLOXxgNRc4KPi+csUeMxqpn031jD2/NJ5mw4XN5dWux5TH1peR4B2b+1YzjPz\na3PsV4h808njqzPtoM0kJ5tpZ2Wl8YC0YOlb4rMq6iAvmac1zbQT6fkKimeg5+a0111Ewrzb0Igu\nIGCY1dmUowagZD+UBsOBIo+fVK/lYA1z+GR4PUYTW6rnXGugoCIqhT22kK/XNddOG+KLWbn3V0wW\npc/67+J4vv8jwAMfBQDsW6Qz7QP11bWUxMwy8jva8+Jx1v0y9IHAQ4cs/gdeUAoUS0y7XVPk20QW\nzxiTGgFVJfIbIx9Pf+O/4sm/9mH8/ecekTLa984XZ9oleXxNEmR6/U8x7QX/broPY5WCPIJVDWD6\nOYZfwogue1uDgQDtY2fi20Nn2seb0j2xNNMe8NQYWybT3i060y7WjbtssR9nJY/Xusc/cjvwtm8F\nPvRLyc+oGV1TTLsc+RbPtBN5/EVPEGrR8TqweRwAknl7terLaS9wvOyA9p2aVfVKmMZQeXzjMWUa\npr2IZHHkB3CyQLsKiqrK4wEZtLNR4aiyIJgRgx1XV0i6W6Pic9g2Zdp17vt1ldNOWC+HhVjCeilW\nbuyHcLl4nVO3PB6A3xWL13CjPGgfjANt9nShog2o0AM4R69lJ5LSoReWMluijTmJaa9ztIScM30m\nFqmFQfssmfY5DdNeUB4vNQ+bbC5UmGkPmmoekveyqRHdwCsF5kKfgHZy7KsSyQvm27jxkiUULjpa\nYFHQXg/jtUUZbL6WfoLTBS55UvR4z2OAx/0X8bPP/yUAmWlvSh5Pmzw9kN/RFqNVmTPtADDehGNb\nCZsdcrNEiLjknHbiHm8I2nWRb+rjqhL5P/3kV3FsfQTOgR9/5+fKG9E1wGauKfJ4OmpQfKadqhWi\n19OZ7aqscC7TXsCIjg/F+eQloF1h2gk5VXbkxQ9CWcJvt9BzM2baCzPt4pymjettNaL765cDD30C\n+PdbgIOfBSDnzteZqkGLNjb7LQfwhsD6ZOyQWcDCxcDuK8QLJhJ5x9bfU0cFPGfyKvZqyDKik2oH\ntO/UrKqMe3yYJ4+vu5TIN6Akw5Urj68DtMtmdKXk8TNg2tnCxcnjlfBYYYBkgYK4pgGSMtde4gY2\nGAdJdBMAM3O3gsXInLO1daL0+2yN1Zl2E9BuyTeMIIpYo46qZdhCKo+3HCqPV47RXoZUtkhJUYmE\naS8qj5eY9obPHY08vsgsZySPz8lpr7NSOe288Ey7pPhpiGm3eJCYlIW83Cws88VxIsnjFYnk867f\nB8sycLon+26JMOFlTavUovfWXaEGtD/6WfI16vqXiMdnDgCQ875PrI9rkXWrJYH2kLjHU9CeFfkG\nJGZ00lz72HzxXFdOu6cxoose15fVrjKPZeXxK4RpP1ET004/z10dVzKiK2qyqJPHS6C9IsD08yLf\n2BgMUxrPBLT7rl4eX4d7fCSPp6A9Wx6/UtRPg94DLcq0b9NMO+fAKmG3H/4PAEC/Tfdf9Zg/XUnu\n8W0bWD0gfjh/cbRu3/1o8T1iRqerJuTxIY2+jKuzUD0la5tqB7SfgyUz7QUl3dPk8XWWYkQHFAMh\nI0+Vx+cY0eUtQoqWBNpHhSVj4YyZdgra97NThW9gjuQe37DpRmqu3fwmO/AC2UDErZ9pd3aJ7WwN\ny4H2MOQYeBVy2tXn1yWRJyDOzksL6BqwmWq1MkB7wZttY+ywrqgRHSagvWDz0MlqHtZdrX7igt5m\nPvoYFo6mo4aYVq1GdGRZEAZSXFSZZhyXQLs47rtKFvbzrr/Q7I3JYmyBC9BelzyeNijmAmGchcuf\nDjzph4HnvVF+wS6y/RO2aa7toONG+3PgBZUNwHRF7wedkMy0U9Ce1wCtKavdz3CPb2tm2j9w52G8\n9p134AsHhUw6Lt1MO6BktVdk2tXIqfLy+CZm2mV5fCmmXaNWqBO0x2/fyjD86mKc28hlYwHag1g+\n7VIjui3pMyoL2iMjOtl7RiePdyyG/YsFP3dprJIw7TNzjxfnZtu1gNWD8hN4tN/navQwyKqUPP6e\nD4gf7r4y+n/5SvE9YkZH1ztx1SaPJ8cln78o/cQ6xmu3qXZA+zlYpdzjt4tpZwZGdH4gG9HlzbQ3\nII8vApQ459K+bNyIDgDIRWcfO1lIKsY5l5n2xqXIgmnfizOlFvdbYz9RZgBohGlvL4jt7IxPlXqP\neEGryu6MakrsWyngQZh2J8/DoApo10QlAsWZ9oAy7Y0b0Ykbs3BmLyGPb3o7KWPM1gunbYSB2Je1\nNg/p/YGHmK86CxvomfY9CqN502UaRiSviB/AXChAdV1SUBqn2vMJaH/KfwO+9Y3AgrIYpF4RG5HJ\nJWNMYmOPr9cfvUTvB+0spj2vaspqpywwZYdb5B459kOc2hzjVe+4He/53CH88NtvS2+OJqc9elxf\nVrsKGCTQbmREN4PItxLu8boRg3pBezbTDkQS+bxGrjUWTTYeey+QRB+EHuYccTyVBZ1+oDGic9Nr\ntouWupmS7VSRqMkOxGc+M/d48tm2bBs4+iX5CWtRw5Ay7U2A9iDkSYQqY0DHtoDb/kQ84foXR//v\npqD9vuThn77iSXjudRfg8ZeK8c+6mHZ6XLId0L5T211dMpNT2IhulgyXbqa9jBGdJI9XQFHt8vhR\noUijcSBLZ2diREcWh/vZSayPpt8cQg6JLWyeaSegvaSD/MALZNdPpzjjUbR6S4IN6/vlctpjd29p\nMWDaYJBAe+wgTxhNQ+DhB6HUpMn9vCuBdtroEpLS4kw7bco1bUSnm2kvEj05Q3k8IDl770bx5oLk\nrVHn+U3/3jCQZiNLxb4R0M4V0P6Tz7ka1+6bx1t/4CaJUSxUpNnRDwRzV9RQdFrRY6XrEUY4a7yk\nuyRSJMYbwCias6fNiUZAOzlWWn4G055Xmqz2Mg7ydM6cOp6rRnR3HRKflc5RP2umXcorrzmrPb5+\n9Vu2BHamlTTTvlHP+ANtOs2VdY9XIt8AKOdxPUZ0WS7dHTZKGiv3Hl3HA8c3pJ87hGlPQDtjitEp\niXEcFVMf6bZTNaKzLJaoX+K6dNnA9JbcA1ucMu0zmmknc99t1wKOqaD9EQCymqQJhQ9t7HVdG9aB\nTwpQ3toFXP8d0WPKtJ96IHn4+EuX8EffdxNe9qRLk+9VnWlPIt/IcUmVqknVodTdptoB7edglWLa\nwxkuQjXy+GKgPc+ITrmRztXtHj8uBIZzGwtNFbno7EMxpj0IuRJb1TBoJ93M73c+iNGZI8ZvMUyB\n9vqZ9rllAdoXwtVSjE28oJXzX6sz7XJWuxnwSEu6cy7tNYF26j9QdD8GBMDVmi2uK3JjXsY6bATF\nrkPqmE7TLvcK0w5Mv15yzsGb8taQmPZAOi5LLfaJIoDK4wHgNc+6Cu/78afhmx9bItGgvSu5rrXC\nQaLSacI9vu2RBl8WaGdMy7bvaWDumRa9Hzg+AUdFQfs4AvqdivJ4iWnPiXzbGP1f9t48XpazrBP/\nvlW9n329e+7NThbICkmAhBAQQVkVl1F/gKAzoLiDw/xkFB2GmXFDZRhlxBFUVFQcICj7EkJIIHvI\nfm/ufu529t671vmjuup93rerqqu6q6rvjef5fPJJn3O6z6nbXfXW+32+y2NgHpt4qfKgb2CUmB5P\nwL/aK7MftIIYvTjSeMB5z1xwZFj2YE0tqWSmnRLAUT3tRh+mvdo2cPczq/jy46cj/05afowmLZdp\n/9b+FXzfB7+Jl/3hHfjYXYf4MelkGgMdTUjuL0W77YFr3bQHksj7BdEBvXkag4N23nQaVr1A68hq\nA+//3OO44+nlnp+JTLviw7SfAAAhyC8Npp0GdVYKOeC+v+Q/fN6P8qlHlGk//Shw5G7h9xSFzIvh\nmgt+AYlMVkQBW0z7VmVbw8rjs2Xao2+kOroFhQWBdgnAlaYxdFGmnXUiH2OmLBzgBHp0aydbjXTz\nMi072GqQRl3xBm/e6g62hhc/+C4gpu+wrVuipz0Fpj03yTfVc6yK9QF8aG5Q2MBBdIA0q905hmGC\n6Bx2mH7eKXnaie+wZMdPjzeNiMeYRKl5z/esMBuzqEVOj89FfS+TKJLsPdcNVOu3rstqgESD6ASm\n3RAZugGk54yMfLOTHD3JmCCRd8Po6hGar1GKniv5DmXaQ2T8gq/daVymzbS741QVWFANVx7PRI8w\nALycz4/H1B7+2Idpbw/iaY8QRKcZFuqNOv6x8D78ZeH38Af5P4UlgUYaohboaU8JtMuWjShFfe0r\nCUjkax0ZtMdn2jUfiwEF7V978gz+3Z/fg5/9q/vwmYeWYh+jEZIeDzigXTctfOUJp3Fl28D7bn8c\nf/9dJzAtT5pLSomMJiSZKcxoYRtpopyuxn9vHXl8L2iXJfJ75wYD7TmLyOMT9LT/5mcew0e/dQhv\n/j/fFUYJ2raNpXU+1rGYV4DTj4sv7jLtYwlkAoQVvUdN5i3gyX/hP7z+p/njyizwnFfzr2//RSdl\nvlve2DoML4/3FCB0HzmxHaYtqbi2QPtWZVmC9yzCJhSA4HdNH7QP6mkPmY+cLwEv/EXHT/TS9yaT\n/FgQ5fG1dn8JVscwobKsmXbqaV9DLUJH17As5FiGwGNyB752xQdgdRfHXdUHgOP3xvoVvUx78qBd\nTOuuYbUaf3ayE/5oo5iYp92Vxw8+oqVXpSJ93he9nD/e9+JYv1soyoQgPminPuzUmXagx9ceJZm9\nY1jiNZ72ejkA0+40D8m/Jcl1SJbHl4bzwjIqj5ebr8MWvZ67713STHseBnIuM8hUoDgV/KIJyrQ7\noD1tT7t7bx0HndE+0Ws/ueHtwGs/BPzUp4A9N/Dv687rxH1Fgky7KoL26cOfx/mKA+Zeo96DtsSu\n+Um7gWTl8UGMXhw/u1tzCc/DpufvZCk/4Jx2+h72Mu1H13j2wa/+w8Oxj9HyCfyiVWYdaKbdoyz5\nrc8+hvWGhqLJQXuuQggYOqtdbwmfx5kBRiYaluxpd5l2cb08b1ZqcIUVOUbVFJl2uQE1aFGG/c79\nTmDuw8c2cPPvfl1o3BRsHVh5Wnxx7RRg6qI8PgXQ3iDY40r1KGB015/pvcD254pPftXv8nntK08D\n93zY+1ExT5n2YUe++TST8mVssEnxiWMLkQnPs622QPs5WLRL2NTNSF4fmh6f+maZMO2lWCPfzPCN\n6Cv+C/CfjgMveXcihynL403LjsRwKSDvd9q+XAAoTqClOLNMi0yHXus/X9xh2jMM0wJQ3X0rvmRd\nz79Bx39EqLZupS6Ph5pHjTk3D4XZqK2fjv0rmpopSuOVXPzzoG96fEymXQZx8uf96g8Ct/w68NOf\nB8pDqFTIhqVEpIGRg+iID5tlEeIozWqP2jzM1AJDPe0RA/OctTKl61sKopsdGzIh2yJJvknbdAho\nn2YOEEgatE+TcXKozIZf6+PZM+3uvXUsaEa7W/kScO2bnAYeaVi78vih0+MDmHYKvA3LxtwZUR4r\nNwiizGkfWh4f4J3dFlMeD0hhdAnYHxJJj/cZm0dB+7DlHodwHyQ1jhY0w+pp8nUMC/vP1FEyefaC\nWvGXx0NrCnaFMwNcO4acHp/rgnYptyAW007IBKa3MNH9XZYtqiSSKvd8+PM7D+I4YdknijlMNA4B\ntvwZ2ED9NMZSTo+n1+3VeJL/4Lwbe588tQt4+W/xrx/6W0d+AUkeP8C6Q8vws22oRawzqdE6No/1\nBBpso6gt0H4OVk5VvO61bUfsTgkzfc/SkW9RAqCSlIFK8nigv/yzRx6fBdMOoFYkDE61v5ytpZvR\n5dIJ1VQ5jzM2AYSNeCPVWnJ6fNKsXLcaOS4Pr6/F9943OiYmQQOfJoOfHFSCPH54pl0OSOy5dqbP\nA277DWDvC2MfqlCCn28Qpj2lMWVBRawAU2hEDKKTAXGGTDuiMe1tPUU1gMS0z08M58kWmfakQTuX\nqs/CZdqTksc754rL4Dt/L8DP7hZl2n1Ae5qe9nEmMe1hJY3XAob3tFMfNWWHGWNEIm9j9+YDwuua\nbfE98RtX5jxOkmkP8LQPwLTPU3l8AmPf6F5pfMD0eBoKyIPo/Nfb2AGQ8PcO0xpDG7pp+Tb5NuoN\n795h2AqKZXKuCkx7E9smqDw+HtNu2zZ00/bNnqFAEQD2DOhph97CVIUokRIIo5NJODdY89BKQ/j+\n7/3I81BYeRK+VT0hjcxLnlWm4XZXmuQ49rzA/wXX/BRfd1YPAMtPAUhWHu9eHyL5U8CmxLTX1ZnM\n0v6Tri3Qfq7VxjHgzj/Eb+Q/gbep/wogmpSNBtGNJj0+QsibHiNMK4nyCdXq11zoHQeVDWhvljiD\nk6/1B+2NjuTLzWCe/FQ5jzWQG3BzNdbrO1oHKnNuWCbU1I65VeAbfW0zPtPe0g2P1QMQ7nENqqyZ\n9qSKgnbi54vKfFlZNg8BIeRokjUESV9QZZ5bQT3tXYDYT8afamNBCqJbGBKUMIvKUxNuxJFZ7TOJ\ny+Od3xMLtFOm3Q2io0x7wqBdNy3v2ptkZNxbYTz8hQLT7o584/fb4T3t4r272AXfF7ITWDBOCj/T\nmjXhawraqR8+WU97gDx+0uf83DwO3PUnwPLTvT8DMDdGmfbhQXtdCKLLD8S0a33k8bQmfeZl9yt/\nTzs/znHWgm5avnaB+iYftVpHGWUaCicBYvp5xPW0u2+VIOHvrj9yoOZ4jIkBcmNhmoD2QTJy5JIb\nZq4tgLLs3/2Nl+GVV+4AVvf7/5LqkqAmSDeIzsYlGvHVU+sNrXwZuJjY9J64HYAcRJeCpz1XQlUR\n1YVfPBJNoXw21hZoP9eqdhL46m/jzbgdr1EdmVkzwg3WFjztacvj/Tzt0RiuocZoxS3CNlS6/tx+\nLE2PGiAjpr1d2eE9zjdOhjzTqZaWonw2oCbLeazZFLTHY9qNDt90Gkp6n71e4iDJqMYH7Y2O6TGi\nAAYLdvMJoqOgPe74mL6e9qSqwK+ZvB1/5JvAtGfQSBJAO5qDZWukfY37eNr7y+MtsSmXoqd9bsj0\nc2bSTXN68vgZOI20JHzFtm17m+cZWR4fVn5BdCl62qlyZC5P/t19mXYKPLrp8YTxGgS0B3naAQ6+\nb1Ue6nmd1qoKXwd52hOVxwcy7ZI8vlMD/vJVwJf/M/BXrxMUi27RILokZrXL8vgcaYBETXr3C/ML\nBO0DyOY9RpPmupBpHRNdefyaD4htVTlor9kVIUuhh2knoP1MLR7T7jZ2RKm0828dSvGSK/JZ7aaG\nHUX+eSXB3srTgU5utlFt657VoJhT+JpSI3tBaj3aXEJFUs4MMiUgrNy1ZzdbwbTZJWgKE8Di5cEv\nuuy1/PHX3w/8/qXY/R0ekDlseryrwBGYdrXYA9o/eyD5JkZWtQXaz7UizMJ0d5MSKYyOeNqVtBku\nIawqxsg33URJ6JCVg5+cRAme9q48vs/IlsxnOHdLH+egvdLqL+luakav7zrlmirnsU5A+4NPPoNv\n7Y8O3E2N35TTBO0W2ejbjd6RKv2qpZmYZkSqVh6WadeB9iauaN6LcteX+q0DK7ESX53wtAw+b7Xg\nKWBytoFcd0MUhfmyLDvb5iEgTJmYZI3IzcNs57TT0XQOgOknT27rsuInLU+7KQSpDQJKVKLISJxp\n9wmiW653hmZi27rlWi6xqFJVTT95fC/TPi81PZJkeCi4nlHJ+xwLtPsF0cV//wSmXQ0C7b2hZx2J\nadd8/NjO4wTl8QGe9h55/NfeD2w4ieeonQDWnul5zbBNLVqmZXuyY8aA8cKAnnYfi0E5rwrvoVs0\naDLOcQIS004UQ+OsifWm5tvM1Rp8EkMVFXH8Wog8/kxMpt13LF33vjuUjYEx4TrfU+BNpyQS5GVf\n/MnNNo6vcZZ910wZzA1irhLQvus6/rh6AorCMEau6SgqszjlKpGuY0/xb+6+Pvx+efErxL1P/RSm\nH/kL7IKzDxt2TrvvZ54roJ7j+wCb5fD4WgJB1iOqLdB+rhX18LFo44EAwBaC6LKXx0eRLNoGB22W\nUkg/5I2CduYstv097Wb2c9oB2GQO+lgnAmgfgad9spzHKrh3qFM9g1/+5EORN6l6h9+YrLhp7DGK\nkXAys9o/1E+upmZ6jCiAAZl28u8z2sDHX4uLvvRm/PXYHwNwmlyffjD6KJ5epj2l85IxMQuiayuJ\nwrTLvvvU1yGgh2mPNvIt4yA6H+AZJRAztc+brruWJY60qg0pj88lfF2T++H2vNNIs+3hGW26wV3M\nkQZdHHl8lwUrF1QvrEo37UTnOdPzZDZHg+j65Gz4BNENP6fd39MOuKDdxnOVQ5DL6GHa+wfRpSaP\np0z78fuA73xEfMKZJ3pek2R6PGVZxws5KAoTUvMjM+1U9dB9PWPMl20fztPuz7SPo4VTASC7U1/3\nHtdQEZPcQ+TxcZl2t7EjBtE5v2/PLP87l+0YIJNmgpMouxTehEgi3ExWWZ3cbOH4Olch7pkh126N\n7AV3kxBgn7FvzYR97W5z6TqFSPSDpPFulSaB81/S8+0LFGetHF4e76orRHl8neQYaaVZLDfOTT87\nsAXaz70qTXtM1yRrIgej7wbPtu1s0+NVCto1AHa02bkGCdJJY9yXXISNmICzKFb7etplpj2bS4iR\nubpTWn+g2WwbyLFs5fETxRw2iKd9BjWs1DuRR2uYGv/8zRSZ9vltvAHS2jiNp07VQp7dW03N8FQu\nAAb0tJPN06lHgJOObPR682FMdn/339xzJHLDQ+uZ054i0BSyIDre3+9XvUAzY3k8a0YK5HGyNTJs\neJVn4PpBp1kDORh91VM9kzZSZNonijmPKW3pZqTGBy3VykYeT8H1qQHGQ9GiOTHzSgymvTLHP4v2\npsdip5UgT49zSumTHk/LJ4hu6DntJvW0S6BdVbCATVGh1C2zFeJpJ0A9l/Kc9kJOEcPavvI+ANL6\n6wPaZyoctK8PCQZqZJ803rVLJZUeD/hL4YfJLxCZdn5tTLAWTm+K1+BV7AD+Y+7v8Ial3/e+1yuP\np6BdTI8/XY2nUtH9AFx3/fnvP/Q8MOa8t3/0Y1dH/p1eTXLQfl6eg3bqOx+0ZHn8Sl3DQRJCt3uG\nvEe1E/zxrmv546rz/fEUZ7W7zYW9jFgMd1zV/4W3vKunqej+jmHk8ZZlezkGgqddLaCZ50w7DSI+\nF2sLtJ9rpSgCszeNRt8gOmf8F5HHp81wqTlv46Iy52+3datvl5jpfJG3swDt9H1k0awGo/K056f4\nTWLCXA95plOtDt8YmlCTmWvfpxSFQStQJYjDoEQNhjJ1fsOzU2Ta5xY5aD+PncGffuNArNc3NVMM\nohtWHv/M14UfXZN3RuU9eaqGB45uIEplCogJ0z7GnGu2E2ETrfUw2BncfgSmvRGRac84bFJRe9b0\nfo2uds8UiwTfS8HTboAxhvmxwdl2xeLPZ4nL4/m1N0euSRkwxC3KtM+xGKBdUYAxruTxk8gnCtp1\nfpyxQLtvEF1yc9rzUjO7kFNwkeKvHDLbMmgnvyfnPzpOTyE9fnGiyGXHSw8Ah+/sfeFyL2gXRiIO\nybTWhBA6Zw0fJD0+KMzPj2mPmkdCy59pX/AeTqCF04QZf3fhU/hM8Tfxjtzt2Gbw8+CIvSgy7QXa\nTHLGqbnNpJZuxhqp5h5jnvruu+vPiy6axx3veinufs9tuHR7n2vFryjTntv0Hh9e7W1KxS0/cH3f\nYZ4DsNtl2vU20OruBZkK7CDNBx+mPekwOvc4p1gMJRLgjIR79wHgtvd63+KgffBmnDC/XmLaTxfP\n975cyu8b+G+cDbUF2s/For52Vuu7wdNNWxwPlDTb4Vc+Y9/6Se4YkccLHde0iiYPI3pq8yjS40vj\nvDNZtPtvSNsa3zxYGTUWAGDZ4jfdGdTBYEUfwaRxCZiVizGCJW7t5De3G5Qn8O1HnsTR1WbIC8Rq\naIYXegVgsLnnFLQf/67wox/ayVP3Hzzav0EDZJgeDwjJ1G6Ao27030R3DGm+fRbrkMS0NzUTVp/m\nYW9jLoPbpCCRr/YNF3XsECm9l4oUkmjb4ti3mL52xe6dk5xYkfdtyuYy62GZdnpPnWekcRZlUyqM\nfUs3QZ56zydjjXwTGU0g4TntPp72S9hx39dZnbrwddCc9kKSTLvPv0/ws3/7T/jj2Qv54zO9I7Zm\nxsT08GEyC2pScjwgMe0RmxWixYC/3g+0DwKU3OMQktklebwbplqEhreonxde37SL+Iz5QvyZ8Voh\nAFGc014HY0yUyMfwtbvvQdEniA4AzpurCEx+rCKe9m3ggPpwjH1EUPmB9u8e4n/Dk/bXiTR+Yjsw\ntZuv3dUl4MyTqc5qd+X2wvjbqHuhXFG4rvZ1QbtmWANfP7RpWJQsEWulvfh1/WfxCeNl+MTYmwb6\n/WdLbYH2c7HoBg+1vuyRZlre5hqAGPaRVvn42pt9Fg3F5Bstls+Aaa/Q5oezcej3Xo5qTvtYTNDe\napPZyBmC9k1NQdV2bio5ZmESzb45AW7ZFLSneY5OnwfsuRGAc4yvZHfjG09H97a3NBNTQ498CwZZ\nl1o87Ciq/7U3PC1F0F7koH2sG5ynmf03+ZphocToOpRBY07ytAP9p210DAsKIxuHLBpzNIyO1foy\nnW3dQjmt9zJX4Js/2wKMjuDbXYnJFKsC054waCeN1zGTM16nhmTa6fu/YJHRlSRbJLCEsW+9s9qT\nZNrp/WqcUaa9z8i3FOa0h6bHqwouDgDtkEB7kLQ7JwTRDQfa/diTmmgwAAAgAElEQVTlbS6AWz8M\nPP4Z/oPXfZg/XnsGMEQ2vZhTPRmyadl9LXZhJSfHAxgoPZ6OfKOv9wPtg8jjLduHaSf70nEyfvA2\n5UGM2c7XS/Yc3m6+G9d0PoJf0t+JRn4WCj1XSJid2/ASw+iiX9dhnvahizDtkwZfH46uNodOafcD\n7fSc8ph2GkI3scP5tz3nB/j37v9YqvL4uubDtJdiEBiznP3eq3CJ/aBsu0FUKKIlooBCTsE/mC/F\nbxhvwwPVAZQVZ1FtgfZzsSp0Nm297w3W6AHtGWyWCdPuzkAPUwTYti2B9qw29M4NY5K1kIPhhWsE\nVWYp3VJNTHDwUbbbsPvI5DSSxJ4l037V7ikhQX6W1SJvYhSdL/522o2l5/2I9/D16l09c1vDqqGZ\nEtM+ZBCdVNubPNgl6tzXnhFgaQJNImF05fFRPe1ucB2AbJqH0px2ILx5aJhWj50ok2tcGF3WvxHb\n0c1030sqU9UaUgJ6THk8YdpZ0mM8ixNegyFvtbsZKsMz7Q0yg3jeIg29qQigXRj75sO0JyqP5+fp\nmMC09wnXoqC+K7Gl8vjh57T3Mu0XE3n8fou/j7YmgfYAT7s48i0deTwA4MBXnGYV4IRm7b0JmDrP\n+doygNVeO5XAtg8hkafAypU2D+JpNwLk8X5J8QMx7X6edolpd+v16l3e4380X4Iv6NegA+f+J0jj\nAaeh7lY3tV+Y1R4jjM49xrxPevzQRUB7vnEK892wTs20cHJzOF97P3DtedrpuDd3zbnuLfx7D/8d\npvP8Ok46Pd5ZI21MDcK0A8AMB+3n4TRYl3QYGLSTNaEgzWmn18DhleHVEKOsLdB+LhaVdUeUxwsM\nF92QpVWUaXdntYcsGrppo2jzCy0T0K6owiITJR+gY5go0RtVFscJoFgsQrN5TkCnE35j0DokH0DJ\nQIbcrV+47WJsMr5hnEU1sqddJaAd+T5M0bB1+RtgdpsZ1yoHUKgeifzSlmZgJklPu1RTjUPe6Lf1\niPPaewMSs5HHu0x7FI+pZljeaEUAI2Pawxpz7oZBTWsGelAR0D7HqhHWIcsLAQSQ/HtJ5dVaXZDH\nr8aUd+cI064kLY9nrKfhASTAtJMZ7QW7e/yFCeF8CiwBtDsb63mawD/MuCn5OMl54rKZAPrL46f3\n8sfrRwDTEOXxA3nag9PjiyoTmPaHbS6NVXpAO53T7g/ah2Xa/QKvPKn0xjH+zX0v7v7wOfx7fr72\nSjK+drqXc8d1FQngiOo/jyOPH6RB4+tpF0a+OfuTSdRxq/KQ9/1Pmy8Sfk+5B7TzwF0PtA849s1l\nXosS65pIkSA61E5h7xzfUw8LCuUgOlrlvMpVTzWJaQeA828FZvY5j9sbeEH7W/z3Jpwe3+yYGEOb\nBx7nyvGUDOVpb+9UYjq2wWkeDhpG5zZpVJBJOkwB1JzQ/NOGXDtGXYmAdsbYGxljH2KM3ckYqzLG\nbMbY3/R5zQsZY//KGFtjjDUZY48wxn6ZseAdEmPs1YyxbzDGNhljdcbYdxhjb07i33BOFWXaUe97\ng9VHzLR7nvbQzbKJEiM3uyyC6ICefIB+vp+OYaGCjL33cMa1tBh/T2rV8IAyo80BsJn2vHtSL798\nG6685ALv61lWi+xpV+j0gELKLOzYHJbm+AZi38rXQ54sVjNleTyDjcuYs2GJOve1Y1jIZaUAoZ72\nuEw7y5hpL4x570WZaShAD73GXdCeeWOuh2nvL48vCWxCwscoMe2CPD4maFdpMzZpph3wHZk3PNPu\nvP87GfeSYmp3tBePE097N4hubmy4WfdBRZn2ih3D014cByZ2Oo8tHdg4MrSnna4BNOkdcHIaZrtr\nZsMuCkw708XwLi0AcIpBdMN62ntf/4Lzu+v4JpHxu3aIBQLafX3tNEE+GdDuzi8vDZDqH9T48APt\nhmXHboJ4gDjA0z7RZdpfq96NYjcI7gnlYhy2CdiFD9M+uRuu+hG1k4ChYRtl2uOA9u57IChQCgmR\nAcJox1M4n4yQOzRkGF3Y/Wk3ndFOQbvbRFAU4FoOia7d/Gqk3ztI1TvG4Cy7W7N8r7iXOYqmQWe1\nm37qj27wIGXaz/VK6l/yXgDvBHA1gL7DhRljrwPwTQC3APi/AD4MoADggwD+PuA17wRwO4ArAfwN\ngD8HsBPAxxhjv+/3mmdtVUSmvd8Nthe0Z+1pd25i/RiukiD3zAhoSg2QvvOR9READ/dvM/6eNuvh\nY8qsDgXt2R0jACik4z7DatGZdoMfs9LPk5lAndrG54XurH4v8uuaacjjmQrsvMb78grlMAB4YT79\nqieYLE12mMrjuw2sKJI2LW122K8YE9jRCTT7gHYTgJ29IkDytPef026me4wSaF8QgujigRKVyOOV\nfApTIaT7IeAw7cMEgrn2hB2M+NmjSOMBiWl3PO101v1qgkw7PU/KFtlA9wPtADB/EX+8sh/lAt8O\nDgLaBcCZF9ef84yj3uP99i7Uwc9XVQLtAkucC5fHDwre6Xr1hz96Ff7qrS/A8/d1z6Mq2cK6n/ni\nZfx7fRLk1yI2Wv2qKcjjnfdQSPWP+LnQxkeuD2iXnx+lTNOHaS9Ne/k5JaZjHE38XI5nA3yr/NKe\n31MuSM3lXIFIz22genzgWe0u8zpOSZYo10WUKlT4fcXScdkU/8yPrAwH2sMS8sVxbzSIjjRDrni9\n93BP/WEvODnxIDrNGNzP7pbga3f+PYPK4921oOCTYVBQt0C7XL8C4BIAkwDeEfZExtgkHMBtArjV\ntu232bb9bjiA/24Ab2SM/bj0mn0Afh/AGoDrbdv+edu2fwXA8wA8A+DXGGM3JfRvOftLYhb6dV9H\nI4+ns5xdpj2c4SpitEz7DKv19f048viMN/Tu31b432o2qiHPBCwS6mZnyLQDAMaI1BfRmfacyTvi\nrJj+OVpdvM57vKfxPSDiJt/sNLzGja3kB+veUxkgALzmj4Ar3+h9eSU7DCAGaNctUQaYZpCjbxBd\nBNBupuzDDiqfBPmg6ugW8jC55E/JZ5NyL63p/TbnbT3lfAABtNcFpjhuEF3OJkF0ScvjAQG0b885\n617HsCKHOPqVqwrbyVb4N6OE0AG+TDvNBIhrLwgreu8vWTHk8QAwdzF/vLofRZLi3R5AHi80ECQG\ndZfB7UcH7N2o23x9Ug1RThzF026YFj5571E8931fxM9/4oFYx2lZtrBeveGaXbjlEj6uDJsEtE92\n1RWUaV9+uud3zlYSYtp1H6Y9JzZTojSjBE87USj4zWkHnPUkTmmmD6uZK8Ik98Kfy33WU6oY5QXc\nN/vqnt8jN3cASL72Y1IQXRym3fk3jcUJaIxTrlIFwEVl3sQfduxbmDz+ip3EniMH0bk1c753bEWz\ngcu7e4l+k5HiVr1jiqB9aKZ9uFntLtNelM5JQGz+ucUyGIOcRiUC2m3b/rpt2/vtaK3tNwJYAPD3\ntm3fR35HGw5jD/QC/7cCKAL4n7ZtHyavWQfwge6Xbx/w8M+9opJuRAHtI5DHl7iv2Q2ACls0Orok\njx8B0z7N6t4Yi6AaSZhWt3QC2luNcKbd1qg/PFumXZD6xmDa8wS0qxkw7cbspajaznszZa45qcER\nKq/xholdnnHY3Lh15Q8DN70TuOHtwC9/D7j2TcCOq7wfu0x7nCC6clZqGp8gOj2KPF4fgacd6JnV\nHhb0k+n7SIuoU2ZR7R9EZ5jppccDYiNKa2B+YnDPLmXa1TSaSWS92Vvma8jJIXztDW0IeXwfpn2l\nMdxYMFoUKBdMyrT3CaIDgHkC2lf2D8To0hL89UWRQd3WPuw93m/tQnmcX5N5U2LajSBPuyiP/4+f\n+h7auoV/+d5JPHI83C5GiwL2Yk4RN++WCdRO8K9dpn2OqBLWDgKmeH1SefzaMKCdrE2udDynKl7z\nwrajMZFB8vhigEw4LlBy16eCNE7NLvBm0c/lPst//wt/DWPjvXkQPfJ4oMfXTseyxQmic98DGoqX\nGNMOCNf53gKfXHFoSKadEkc/deN5ePll23DLJQt480178TM3c2ba19MOOPuRfdz6d4PiKEOSZtob\nsjx+IKbdD7QPlx4vhNCpwUx7Xv03DNpj1m3d/3/B52ffBNAE8ELGGDW/hb3m89Jznv3Vw7T3SRI3\nZaCZAdNe7pUsho1a6pHHj4JpRx1Nvb+nvTwKGT8AQ+V/q9PsA9p1wl6k7Q+XiwCQORY9iC5vEdBe\nSv8cLRfzeMAiG9dj3w1+crds20bR4BtENkgIHeAA3+//r8Cr/gdnFrY/1/vxJewYCtDR1MxIG6qO\nYYr+wjSvH7IxG+tuiKIx7bK1ZASgnTVDAbGjpBlt89Bd08PmyffaidJNjxeY9rhBdFQenwrTzu+H\nu4p8DRnG197yk8dHZdrHFuH5cpsrgKGhUsh5nnHNsEIlsLGO07un2iJoj6L+EZj2A0N72inYkMHY\neRYPd9PnLsWPvYjLzQsS0y5Ku/nGmo4uk4Mv4zSSqGe2B8TWzzgJ8YBzXrnXf2mS+5i7GQC0ZhMC\n7Q3B087fw1KeH2cUX3vgrPsg0B6Dabcs22sWicnsRQG0u3XMXkDpxrdiutJ77Zd8QTth2jeP9cxp\nj9rwMi0bDJanBgOQnKcdEIDydrbuPT621hpq7Btl2n/o2t346Juvx1+99QX47dddyd9D2/b3tLu1\nl4P2GxUng6GeYHq8adlo6aZHyAEYjGknCfLurPZBPe0tzSd4MBfsac+fo5L5URz1pd3/92iMbNs2\nABwCkANwQcTXnATQALCbMdZ358IYu9/vPwDP6ffas6YoO4x630XcMG2RlckCxElecSB81NLoPO3c\njzwThWnXZYYrO0BMA+X0PqAdOpGaZ9hYANATqhVVHl8koD1XSn+WZqWg4n4BtN/T9zUdw8KkzaVw\nrDKAnz2oytNe8muBmV7a8mYEiXxvMntWTHvH+/v9qqPLnvYRyOPRDFf8GCnOPw8rydMOAO2QZk1b\nz9LTXsfsWAFuIPhGU4/lJc6TTVQqTDtpnC2q/No8nQDTLnraIzLtag4YI3LrhhOwJKgVEvK1u+x2\nEToUu3u+qEXHG9yvJE97Ka96oqG2bsX2i1OmvSzJnndqHOT+6k+8BsUxrgSgzVpAXEsEeTyViUvy\nfTmtPqxoE7Qoy7P9QujcmhebHLQoaI+qjvKrlk8QHRDf1x406/6G82dx3mzvuhu21sjlki8MFvI0\n/FTNw/ZReDyoXAE1X8TsWK80/5JFn/v8lMi0TxR5w6ulm5EbXrrlKEwV1gXQ+Uqyo1AJUC61zghj\n305sDD72jSrBJooBgbKdKuASM/lKr7KGgPbnK0+CwUqUaXcb39M022cQpp00aNy1Ns65SMt93/xA\nu5/C5Fz1uY/iqN0d1GbAz93v0zMg6msizGN5FpTMtPfZMDvy+IwTz33CgcK9pNIotVGkxyOKp31E\nzQUANgmU09v1kGeKSexZhLoJJQCQauQ57QWbn6P5cvrHXM6ruM++lH8jAtPe0kzxRjUo0x5UvhL5\n/qA9UwUIAXQVb+RbBNDeo/gZBdPeCA+iS9srHlR0TUcNgB2+XvZ83gmvl5I8XlXYwGxijqTHpxNE\nRzI0FM78DCOP5572AUA7AEwQX3vNJ0E+IV+7e5wTg0iAp/Z48lE0zkDVqoL3/nRMpUIzSB7fWIHS\n6mYD5CsYX7wAedKUFbz4kMeVEdBOgPmy9P7F8WRT+W3PZr5KQLv8ec+JTQ5ayTHt/mqFuOP4gka+\n5VQF//pLN+NT73ghrtjJgV4cdtNdPwsSyw7GfM+9zfwiAPgy7S99zkLP92RPO2NMSJA/E/G8NE1b\nZNmTlMYDoiS9dhJ7SDPk+HoyoF22mXgl+Nm391r05i/uKn6AadbAc9ixREG72/gWPe0DEBjji15o\n7hyroQhtYKbd17Lhetp95fFboD2pcs++OPqSyK+xbfs6v/8A9M7xOFuLdLQm0YSmhd8kRiKPF9jW\nLtPeJ4hO9LRnBNoF/3Udbd0KlTalntocUjYBS0YM0K4WR+dpv1o5iF9eeZ9veA8t3bRQIaA9C6a9\nXFDxsHUhDLu7DJ5+DGiFeyObuuk1oQAISo1EavvzvIduGF0U5qajm6hkxRCTzY/rF4zGtI9IpSIz\n7X1GT47k+i6MeeGdRaZjDO0+IzLl9TJdeTwggs7lGGF0VEKrpjzybRo8byIu6KTV1AwosLAdxNM+\nuTP4BXLRkVB1x9eexqx2l/UcZzFD6ACHdSSeUqwcwM5pfr6f2IgL2vnnLATRLT/FH89fAigK8oRp\nL9oiwAkCnHSTLaeI98uAoCUw7TJopyF0MmifF4P7aM1UEvK0BzDtpZjWBcPy97QDwHgxh+v2zghN\ngTiz2hshjCbzOfeaZQfc0sYG4IQzXrnTh2cTQPvgs9oNy/LmxQNIVhoPiOvBwa/jfEJ2Lw3BtFMl\n2HgpALSffpQ/nt7b+3PGgL0v9L68RjmQ6Jx2t7k09Mg3RRWyARbZ+sBBdA2iOvIqZOSbHJZ5rtQo\nQHs/VnxSel6c14RHaj9bSs3BKDj/ZIXZyGtBAgSnRiKPL4seTaAP025I6ddZJZ5LQXRA+E2xZ9Z0\nFkn8Pn/L7ISHneSITzCXOWgX2ecX6vcAX39/6EvaIwB0lYKKJkp4zN7X/Y4NPPWvoa9pdgxMY8ju\ncljtuNp7GGfsm6XzTaylFJKVAcpFmfbuZxZp5NtZwrT3ax5mvla6JQU4hq2XbT1l770kjwcwcBgd\nZdrVfBqgnV+D4ybfAgzjaW9oJhawwacIUH9zlBKY9m4YXQqz2t2U94HDtqhEfnU/dk1zcBRX4tsM\n8GMLI9K6KeylMX5NVnpAOwGcOX95vDyvO04ydlvwtEvrJB33JsvjaQbASrA8fjjQ3jvyDRABRiRP\ne4A8nhb9t8cJ/3Lfa5Fpd/79rNQrj9+x50IAwHRFlMffeukCFD9bA22WVJcA0xB87VHD6HTTTi+E\nDnAk6C6BtnEUP9r4hPejpQGZdsuyRaZdHonn1rHv8Md7XuD/nAWuItzOVhNm2rugfdiRb4CYDYD1\ngYPoXPutEETXtQnJUvhCTglWMZzlNQrQ7rZdL5F/wBjLATgfgAHgYMTX7AAwBuC4bdtN+efP1rJK\nHBgVtXB2UDdHkIhM2Y8uaA9njuRN6CiC6LrNhT7y2VGNfFPJRtrSgkG7bdtQLcpaZyyPL03DKs+J\n3zsTLmRp6aYUGJN+M6SSdxbtfzVv4N985B9CX9PUTO98BpCCPJ4z7ZezI1BgYSOKR5JkGFhpW0sK\ndORb9CC60XnaRWVSeHq8zLRnCNqFUYnhCfKdHk97tkx71LFvlmWLTHsqoJ2/byWDN7BPDSmP3zlI\nCJ1bVDrbHfuWxqx2Nzh1gg0ITqQwuh1ThGnfHBy0C2CDMu1dIFGq8GMsoy2M29SCRr5Rebx0/g3M\ntOdlpp0H5vUy7WKDg9ZUOe9lPlTbxsDz42mmTrA8Pkp6PHkPc/5+f6oyiAXaPRlyL9OulnvPvVfc\n5IxWnZHk8S+9dNH/D+TLnrQbtpPmv20yPtNuWrY07i1h0F6edsJku/WCk3/n5dAsbQwGRWR7hBqU\n1XCU5O8EgXYJDCcJ2t176NBMOyAoFraztYFBu8u0F3xstvLItxsvmEOMGIyzqkYB2r/W/f8rfX52\nC4AKgG/btk2vzLDXvEp6zr+JsghYKBn9QXslc9BOmPYuGA7zi3d0Se45AqZ9psu0h8pndSP797Jb\nNFGdhYD2ti767pWs0+MZg/W6/4XvWDzb0d44GjoHvWccWAagvVRwlr/PmlxGhkN3eB5Uv2poKTPt\n44vezbbMNFzATkTytDNih7DTvnZoEF0MT/vZkB4/xRqhYZO9nvYMQxwlpj2syWnqHY8Ftlku+Vny\nkqcdkGaNR2SKDctGgfF1n6Usj8+3uZx9OKbdkPzse4Kf7Fd0Vns35Zm+f3ET+IPKPUfchjOAHqVT\naNHxWrWTkjw+LmgPkseThu2ikxpfKhbRsh0Qp8D2QrVs2w72tId4UOMw7aHp8ZshTPv0Xo9RRv00\n0OaqDlVhgmc7ijrKr4Lk8XGT/alagabu06KS+0Hk8eJorS7T7hNEV5pz5O5zkjz+5kvme57rlSSR\nX5ygWQvRrh3dtDABOkUnBfLi6p8E9t0MAFBg4TblQQCDy+MFaXxgCF2dyOMZsPv5/s8joH0bW++b\n1xSnmn6e9kGZdhm0DzC5Agiwbaj+TPutl/hkKZwjNQrQ/k8AVgD8OGPsevebjLESAFdD+6fSa/4S\nQAfAOxlj+8hrZgD8/90v/yyl4z07i9yYy0a4PF43DJSyGgfllsC0u572fvL40TLtTsCYHdqRNA2N\nb5aVFDbLIUV93kwP7uQ2tdE1FtzKPeeVeIv926jaziaQGS2guRr4/LZuil3xDEB7QVWgKgwnMccb\nDLYFPPqpwNe0NNnTnjDTDgi+9ivY4UhMu0LsEHbaQJNsfiox0+NHP6e9Gd487EmPz1IeT0YlIlwe\nD9KksdJ4H/2Y9gE82bpp+cpoE63COAcNRgvjinNsG009Fhih1dJMLJIxTj0jlfqVMKs9PabdBe3z\njOwBxmJsSGlzoX5GkMefjO1pD5DHU5VVl2kv5hQ0wP+W0XIAsGnZXm9XYRCYxtnx4HMnHtMeUR4v\nM+1yBkCPr53vBQaVyAcF0ZXipscHND5oDcq0uyyrX3CwLI+3i5POuDwAi5Ml3Hyxs8b9fzfuxWQp\nZO9E3/vNJYFpjyqPN6yUg+gAxzt+yfd7X25nTtNw0CC6eoe/p4Ggfek+Z58CAIuXC/c3ocgatI2t\no61bMAZUgMjlnqeTSTDt0ui8wZn24CA6Oafv1kv/jYN2xtjrGWMfY4x9DMB7ut++yf0eY+z33efa\ntl0F8LMAVADfYIx9lDH2uwAeAnATHFD/Sfr7bds+BODdAGYB3McY+zBj7IMAHgFwIYA/sG377iT+\nLedKMQIWxvqAdlvjC4jGSkBA5zXRIgzkNOpgsPqAdkkenxXTni95G/M8MzGOVuhNkRn8JpA6oykV\nTVRXjDDQPkKJL6mJUg5LNlkcpdm2tNo9gC79Y2aMeQzGp00+IgWP/XPgaxqaOXxiar+au9B7uMA2\nI7E29LxM/doh0wjiBNFppmyBGUEQXb/0+FHNaQd6Pe0h65BC7BCpfN4+nvaFAZjieseQZjmn0ORk\nTGi+XjTOP79BJfJNzRT9sEEb46DyDaJLgWnvniNzjMT5xAHtY0SiXD8jyOPjsIWmZXubbcaAkguG\nm2veyDvkSl5oFmMMTfC/1W46xy/42SWwuWs6+DyPwyJSeTydfw5Dc+a0AwBTRIuDW0KCfLK+dtu2\ng0e+UVY8Znp8oDw+T0F7jJFv3b/vkjHOAXbvgxIwZlLj4+M//QLc9Z7b8DuvuyL8jwi+9uOCp305\nchCdLI9PySYogE4HtJ/caMMaYFZ7PUoIHZ1yc94N/s8BBAZ7W7cBGaYijVP1JD3tRNGybQh5vMv+\ni55257yRR0KeP59hFlXClRR6uxrAm7v/uW2nC8j33kifbNv2pwG8BMA3AfwwgF8AoAP4VQA/btu9\nOlrbtj8E4LUAHgPwJgD/HsApAG+xbftdCf07zplSyFitMTM8f88mUmpNzYjBVvNA0dnoqMzGJJp9\nmfaReNoBkW1n9dBNvQCWM55/XiA+QNUI3lA1NXM0MmSpJko5HLeJBG7jWOBzW5qRuacd4DLOr5nX\n8G+uPhP4/JZmDDZeKU4RieEEa0ZKj1coaE/7885X4A7sKDEdKkwYlt13k6JrGopdqbQFJR3W1a8k\npj189OSI1ACA0ACaYg20woCI8HmnsFb6yOMHYdprbcOX+Ui8SMPjgjH++Q0qkW9qhqT8ibnh9xv5\nNj5YkF9YuefyPAYE7eMiaB9UHi9I4/MqDxjrSY4njDHjf0trOMcf5GcHnKYRTZOnFUseH8S010/B\nG0A0vg1QfUATTZBfOyj8iHq2B5nVrpmWl/qeV5mQeD2MPD5KEF2ckXleCBn6g3bZYqAoDLumy2Ay\n9SmXwLQfF9LjIzPtppX+vRoQwPEu1bGqaqbVM5YwStXJeNzAEDrBzx4C2ivzgOL8jhlWRxFaYr52\nByDbCXnaZaZ90PR4P3m8c7958cXzHlD/bz/03P7n31lcicTn2bb9PgDvi/mauwD8QMzX3A7g9jiv\nebaWSkKLJuwqbNsOPhE1DjQNJUMwXJkBOo4KYJbV+gQrWZKEP0uGa8abzzqDeuimXjHa/KrJmMEu\nVfjGMWeGy+PPDqY9j+OUad8MBu2dTgsqczYaOvLIZ2Q7cCWIayCyvvam47/3uZ6amumFrwEQAHZi\nRSSGE2hFYtpVkx8TSzvDgDEHxGiOTaCCDmqoQDMtlEJS622acK8WoWR14xSY9gjy+FGoAYAedVL4\nOtTiLfc0jtFHHi942iNuSGttXWLaU2rUEOXZ3go/zwYZ+6YZFnTTxkRuiA0/lZ03zgCWmficdspu\ni0x7iFdYLgraG8uYq+RRyCnQDAvVtoF6xwiW6ZJqxUiOd6ujlIAuVtSbzloiyLolv7miMGyfKuHY\nWm8zIQ4YCfS003GfQQGjE2TMV13MPqFNmUHOO5q1QUE6ED89nr6PuSDQPiDT7jZIpv0UZ/J1MhUz\nwNEtCvY3l4Q57aer7fD9brcM08aYMPItfdC+k/FMjePrLUHWH6UEeXwQ037iQf44KIQOcBS149u9\nve0iW8eJjZbQmBu06h2HZPGma+TKgzdkJaVCWJZLWLnXj0D+dJvuxZyKL/7yLai1dcyNp9Q4zqjO\nxjntWxWhlIqYiBwmKbGJ/1lXRyT37OPRHFl6PCAmyLNa4AbAtm2oJl8QUgdHUtHE3YLl3Lj8qkce\nn3UQXbcceTxl2o8GPtf1NALdzVxG5W6OdORguY0i2wQ6Nd/nNzXZe5+C5I5sfCYQjWnPkfMyk+BB\nIYwuWoK8TZqHZpZNOYFpb4Sych1jhCoVCtpZH9Ce9jrkB0zSvRYAACAASURBVNonBpDHtzXkmfPv\nsMA85ifxIveaXQV+np0cQB7vbhrHBk1kB5wNrHtfsS2gfgYzlbzXB1xv6kP7Syl4W1AGZNrzZQ5m\nLB1KZwM7pqivPRrb3giQdQtM+6IM2vl5q7Vcebz/jHa3qHyfVmj+g1SB6fEd8h76jC4DAIyT97ax\nLPzognl+L3jypP/9I6yoHUYeRxV3TrvItPuD29KgTHu36TktMO3d/WhRspHIuQBRi4L96hLGiznv\nXt3WLdQiNGk6hmRxyUAeP2OvQ+l2ogYJo6Py+Am/ZpnRAVrdxgBTgel9fY6N+NqxjqdP10OeHL0a\nHSMZlh0QZfxYx4FTg03u9s5L2kwizdxCTjnnATuwBdrP3SqK7FFY95UJTHuWm1ARDIeOUpPl8Zky\n7WIYXdAGQJNG57Gs5fFlvpEoox148+6Vx48GtE/KTHuIPN4gc+e1DM9RymAYBbLhaPvnRDQzl8e3\nsNEKZ9qdEX/8vFSyOC8paO82MfQ+XjSacG9l2TzMV2ArjnKjxHQYnWCVirMOjUilQuXxaAQyDoZp\noWgT0J5KEB2VxzsbPZr+vFrXInk2G03+mZvI+apXEimyhm/P8c93EE+7O0Zt6BnPNAF7/RByqoLZ\nSjLzvAERqM4P6mkHeiXyA/jamwEBakJyvMS003XebdrqRrisO8jXHs/THiCPJ2nwgQqqsWDQfsVO\n/ppHT4TnDPkV3R8J6fsYRB4fbDNwa3Cm3QVHUeTxA4L2SVEezxgT2PYzEZQMmy0d42mOfHMrV/Sa\nhioszMP57AeZ1V5v83u97xxxes6NzffPpyKgfTtbx9On4zeT/Kon22dQPzsA5MuwSs75k2cmji8d\nhTlAHkCobeNZVFug/Vwtwh5NoBnaKaWbZTMrTzsgBSvV0dTNQHa4Y5jiyLeRMe3BoL1jSGPpsgbD\nhFEro4Na23+j0iuPH52nPSrTbrb4QpulGoRuMPU8lcj7j1Fst9oodm0cFpR03lvC8oyjiY2mFnjd\nAA6rQoFmJgqQIp3V7myM+jHt4li6DK9vxoS1aMzYDGQ5RzryjbAV06weuDmXG5wsI3l8Ka967I9h\n2ai2+9s22g2+SdTTVNCQz3de5WvJIDJlV4kxPqyihgRKujkZg+QCBBVt1M9iwPR4QJLIi772qEqF\nYHk8ndEugXaVn2Nm2/nMwjztALBz2v8cChvjKJcI2oOY9oDgQSm4j9blBLQ/fboWKZyTVuCcewBl\nArDjBtFF8bR3BvC0T0fxtA8qjx9b4Faa9gagNbAYc1b7ZksXrWxpqOLcIraJHd1RkYPMaqeKFV95\nPD3nolznUhjdU6cSAu0dQwrkHQK0A2DE1z6hr+DAmfiKAP+AxBSm+4y4tkD7uVpkY9+XaSfhacaI\nGOwZ1GDbwTKsji6NfMvyOMmCM4VGoPe+N6QqY9BO/l4FHdQCNs1nU3q8EES3eSxwVrvZIaA9S6ad\nMBhanmw4Wv6g3Wrzm56RG0uHOZSYdt20Q1NfR5J4TjZALtPeb5NK16FUxpSF/W3i850NSWavdwxU\nKFgbIdMe1Dxs6/L1ncJ7mSs5CdoAYHYA01lrRNDZf+PcbnAwqaspvpd0xKjNr9FB5PFuMvLYsNLa\nWQranaRx6ms/VR1sLJRb7vlRhMbnUTM1PutFN//S2LeoYXS+8vjWhjejHmoRmNknvMbI8fPBXVf7\ngc0gP24spp1c+4FM+wDy+OlKAbtnnOPTTRv7z8QDSEHj3gCReU9q5FtpQKa93vEBR+5er08QXeRS\nFAFwYnNJnNUeIYxus6VnI48HpFnjTlL7IEw7JWJ8syQaK/xxFNBOmPbFBJn2pibL44djtNkkbXqs\n4aFj6yHP9i/3+pnxU4A8i2oLtJ+rRTf2aKIdsugqOvWSZjl3WGSwgeB5qg57REBopky7mNoc5Hnt\nGKY0/zxjBpuwXxXWxmYriGk/O+TxE6U81jGBpt292XaqgQy2TUC7keE5WiaMRjsnhdH5lKUR0J5P\nKeG+SJl258a/HiKldcLTMm7S0FntiAbahYT7LNchQFiLZlk1MLfixGZrNGPpgB5Pe1B6fG9YXgrr\nkBs26JZPGF0Uplhr0SZXiu8lYVQmLH7tDsK0u83Q8WEDJ+l4sDWHab90Owc19xxck18Rq1zwNguy\nEY8imZVLmtW+Q0iQj8q0+0i7Q5LjAcDM8fXT6lowKPlQyEUH7fE87YRpp552uuYHfd6laaBrtYFW\nF0J+AVEi/9iJeN7cQLUCZE97n+wQ247kaR+UaW96nnYf0CZPfRkUtAOSRP6YOKs9AtNebRmSPD6F\n0Fi3CFO8bYhZ7TSIbqzgE+raIEw7VcgE1YTYTFhtaImMm6x3DEwmJY8HpKbHGh465r9PDCtXbbMl\nj9+qs7NKsqc9eNFVTOolHZHsvLuxCLq51tr66DztZMGZYiFMu2GhzEbIYFPQHsK0t2R5/AiD6AAW\naeybGFKW3fFW6PxblbAEQc0FwrRb+ZQ694StmGTO+xIGPrSecYkZe9ojyuMVOqYw64ZXhZ+DM6gF\nNuZObrRHZy0ha/oUa6LV8QfFvTadlI7RRyIfl2nXie3FTKvJBQhMe8ng4OtMrRPbH+mOXUpWHu+M\nB7vlEn4e3vHUsvyKWOXep+YYlcZH2MjLJcnj98zw9fehY+uh1hx+LFTa7YJ26me/tOc1wvnQbdou\n1/g5Rc81t3YGBNHFSo8PlMeT9zGIaWesj6+dX8OPxwTtglqhKMvjCWjv06AwyPmeU1hgyrrItA8g\nj/djNBkDvu93nL3fbe8djnyRwugWaBBmLZo8fnwE8niXaT+4EryXDCo68m2i5DNBJ648ngbRdY8r\nCba90TG9PT2AoeXx9P3bxtbx4NH4oL3unZfJKQDOxtoC7edqlSSmPUQyRTfLVpZA05dp9z/OtVrL\nSxm2meLMec+qBHl8iKddHwE4opWnnnYNtYCAskbn7Bn5BiCSr93W+AbAypRp55uhpkI2kQHyeEYU\nAVZaI2SkaxsAToTIfJ1mEm14ZdCY8wmi68e008kLmTeSCKibY1XfzZRuWjhda49OpaKoQq6C0vHf\n9Ld1M5uwvL5j3/oz7SaRHFupgnZ+r1Gbq5ipOGuPadmxx6u5EtWhg+hmL+CP1w4CloUbL5jzvNpP\nna4NFJTnlnvPnx903JtbUhDddXtnPJD4zHIjUuI0BZyeeomCdik5HpDOh+75RZuT233GZQV52juG\nFTmNX0iPjxtEB4RK5EWmPV4YHQ2iq+SDmfZ+I9+iSOMBeU57jCC67mc9FSRDftEvAb9+ELjl3ZF/\np29JY99ElU9ET/swEyDiFGHaLyk755Fp2bHBZ00A7UnI4/lxLaIL2hPwtTc0Q5Shk/vrQEUaNDvZ\nCp4+HTzFya8M0/IaT75ZC8+i2gLt52oVxp0gLABlpqHTDpbiqEQeb2fqaadBdM5CEeQ9q9f5QmLn\nSumlDPuVJI8P6o62jQy8pGGlqNCZwz4ozEaz6b+Z0jp8fqbFctk2QEi5N50l2dfuVzr1O48GtDcU\nyrT7b7iYTm54aXXu8xXHmwon7TwPI3T0UqfnvMwiiI6/V2MR5fHCuMTMmXZxLar7bAic+b8YXRAd\nAJNMBVE6/ufgZkvP5hgF0O6sNXE3ztT2gjRBOx0tdfpRXDu26n0Z19de6xhQQS1GrFf2G6Uqs/ze\nYrSA2klUCjk8/3x+v/nm04Oz7W5zeQ5DJMcDPeFq5YKKl13Gv/e5R070/RUtPz92SHI8IJ7rhbYD\nRk4R0O4349qXfexWWO4HrcA57VGC6IDQMDqZaY8yYcEtQa0gM+0xPO309wjyf6nov30gpj3M05zE\n/o1e09XjmCfKi+UIa0+1pWOczuzOyNN+fpGfR/cejmeBoaPsfIPo4srjSTPBUQDYeLob8qabViQV\njV81OobItA8L2smkjT1sGZYNPHI8etPLzagpQuPrtloYbN0+y2sLtJ+rxRhahBk0WsEnON0s21l6\nSX3k8X7SLtu20WjwzR3LsrEACPL4adQDpbOrdU3aLGfPYNNk9XbDn4mj49OMLEdrSXXRonOTXLaJ\ndKrpfxNTiDzezhK0EwajxsgCHyCPV3X+3rJSSp17xgRQPI5maCDUSBQgPvJ46qP0q8xnydMiDOQc\nar5p0y64G9nINwA2WYtymv85eHKzJdl00gLtvWPf5mOmn1sEtLNiihuo8UXgou/rfmHjp+zbvR/F\n9ZbW2rp3TgNwrsVBQQj1tXfD6G65mAPrO4YA7e69VJTHDwDaJXk8ALz6eRyE/MsjJ/tu7uk9c8zP\n0+4D2ltjHJSNNZcAAKc2+Xm9fSqeYiiqFDnY0x6Vaafvl/j5bZssetdIQzPxZAxWM3BsHuLJ42kz\njY5plCsOe0+r3jFQhIaKuwYpuXQa2BS0b8ry+PC1x7Rs1DpyenyKTDuVd4PvcWKDdsK0T/aVx0cA\n7cVJ7x5WYR1MoomnTtVw5/5lXPs7X8YP/sm3fBvY/arRMTHLkgTte72He5hzTT1wNHoYnXs/72HZ\nsyT/Mqot0H4OV0vlC6XVDAbtOYuMWspUHu/DtPssENWWMVoWLiLTvrTelKSz2QNigyQwa03/DYHe\n5sDSynK0llQXLozjJ244Dxvg5+naymnf5yoGBe0pdsSlopujGvrL43MEtCtpyu1KYoJ8P3l85qMI\naRCdK483wzd+dB1Ssu6AC1admq/ix22MjPIat8laVNT81/QTG+1smod95PGRAo00cr2k1eRy60W/\n6D28ufFlzHXHoD1+Mp5MudY2kvPC0gT5bhjdSy7lwPrO/cuRZd1yuYzrXMLyeAC49dIFD3wfXGng\n8ZPh/mzK/pYLOUBvA1UHiEPJATPn97ymM8E36pNt57n95PFhFdRs7/m7QfL4yEw7eY8bItPOGMNN\nF/Kff+UJ//udXzUjBtH1A9gU0FKgK9cgTLtt22hqppQcPpsOOJqUPO0x1p5qS0cZHais22zKlQHV\nh7lOqgijXWmfBuD83QePbgh2hX5Fg+j85fGkSTQeoUHHmMBiv0a9G4+fqOJDXzuAWsfA4yer+PSD\nS5GPD+iSbJrh7ekBJCCP3+1NK9mGdRSg474YDQ/3fu6bs/Asqy3Qfg5Xh4L2EKadMlyZbkKFkW91\nKLB8pV3L9c7oZrQD4nxkNAJldksbrZGycABgERVCteb/mdNQNytr1YJUv/WayzE9xzeFy8unfJ+n\nUAY7QxaWyg6rNmXa/d/bnJkRCCHS0Qm0cHKznzw+4+uHALrxiPL4vMmvHaWYtaedMO2s6rvBd5Oy\nR2mBYRS0G/5A6eSmvA5lGUQXD7QrOt9EqWmD9n03AzuuBgDkbA0/pn4DAPDoUrxAsHrb8HIaAAwn\nq6VhdPf8GXDoTly6bcIDpNW2gXsPxx9vBHDGdX5Ypn1MYo4tC6W8ipdfzlPl/+n+46G/ooclbkiB\nWX6AqTLvTRYpmzWgtd5XHg8Az9nufx4NxLTnApj2oCA6QJLH9yolXkHety897n+/86um39i8bsWR\nxy/X+Xu4MBF8LygOMPKtY1gwLTsbcESD6DaPY7aS93oDa00ttNmV6bg3wFFrdvdaitHC5VPOudjU\nTDzRp+FFq//IN3K+Rb3Wr/p33sNfyv0zoDfw3UMcEMe16LR0E7YtTa0YFrSrea9JozAbO9kK7j+y\nHtlewpn2Z3cIHbAF2s/p0nJkIQoAGQCQNylrkCHDlSt6EvkcszCHTd+Qt9V6Rxz3ljXQzFdgq46M\nrMh0mJ2m79OWNloiOBpBKrta4p/5iTOrvs+xCLuVqR3Cp4o5Fc+7iDMqaPlvUFVyjrK0b7CkqOxw\n3e4vjy8a/L3NlUPYmGGLsPgTrImTIaOXtFGPfOsCnDC2xrZt5G3+b1BHGEQ3g5rvBt9j2kdogVHG\neKOzHATaN6SwvLTWS19PO5fbRgmio3kq+VLK1zVjwPVv9b68QjkEwAkEi+PdrHV0PvccGC7AiobR\nrTwFfPzVYA/9Lb5vQGBHy72Xzg/rac+XeJPQMrw1+oev5fLkT91/PFSWTe0mDmjvDy7GSnkctQkA\nXjuE05sUtPuzxH/849dg22QRFyyMeRYsIAbTHuRpjzLyDfC1E9C69dIFb8zao0tVLEWddd+JKI/v\nB9pJsjplp+UqCUF00dhg7mfPALSXprniQW8i1ziF2Yqz/tg2sBYyBrXa1qVxbyk3DCVG+0+UP/DW\nEAqQw8q2bSE9vsfTbplAk+z5ol7rL/j3XiDdItvAT6tfFH787WdWY6oB3HnoCYJ2oEciX20beGa5\nfwgmPaYtpn2rzurScmQhCkgaBoC8RUF7xpvlSXEUhp88fqWuoZg1U0iLMdiE2cwFyFKX1mUvafaA\nuFThn/ny+rrvYkubDvYIJPxyTczwTU7Qe5sn8nglTe+rVHRztGaRz9NHHm9aNoo2P85cOSN5PJpY\nbWiBssjMRoDRIo2V8Qiedt20BTA8Sk/7DPMf+XZyswUFForMbSAyp/GYYeUqfKNRNmu+YPPEZjub\nDAOfOe1xmfacSUB7JeWNMwAsXuY9PF9xANVKXcOZCCOi3KrJTPsw8vj5i3u/960P4hWX8zXxS4+d\nHigQqu0rjx8AtANSIrrzvr14dw7/e+wj+EDuo+i0m7j94eBAuh6WmDLQAYFZ++bHcIyA9vbyQS+I\nq6AqmA3wY1+6fQLffs/L8NVffQn2zPBzPzrTToPaguTxYUw7fa9Wen48UcoLEvkvPxatKdMk6/sw\nI98E0B4mjx+AaXfXzUzAEWPA4uX86zOPC/acsGt6syX72TMgAm76Oe/hRa1H8L78xwEAjy5Fs+e0\ndcsb11fIKaJ1A3AAu93d75VnogcMFyrAre/xvnx77nZhlnm9Y+CBI9HVPo2OCQVW8qy2FEYHAPdF\nPC732g+caPAsqi3Qfg6XkeebIBYK2vnixrIGmhM0vXLN94azIsvjR+HDJhL5gub/XvYw7SMAxJSt\nKpgtHFxuCD+3bRvV6iZ5/ujTM6dn+casFMAeUr+zmiXTTmSI6yb5PH2Y9qZmYIJsBFILogOkIDrn\nbwalYHcMc6RBdJUI8vjehPuscytEq07DZwb6CXlGe2Es8yAblTDtk6hD82nKndyU16HsPO2TpZw3\nsqypmaFAyTAtFIiCplAOAUJJFfFO71O4tzTO+K1a2+gNohu0tj0X2Pti8Xur+3Gj+hSfrrHR6usZ\n9ysvPX5YTzsAjHPmH0v3AwCUr/0XvMK8Az+R+xper96FT3znSPCx6DLT3n+e9CWLEwLTXj+133u8\nOFkMnC8OAGp3/jgFt5HT4/3k8abOJ5gwJRzk0X9PvZdpB2SJfDRfOx35NiYx7bL/PEw2HBW0U6Y9\nqqc9c++wDNonaBBmOGifyGrcm1vXvQV4xfu9L1+mPAAAOBYxCLNG/OyTfn72uCF0tK7+KXSmHNXP\nJGviHbnPCj+OE4hZa+uYQh2KmxdQmk4mL2CGMu3Ov/W+iNYh99qfeZaPewO2QPs5XQZJw1S14JTS\nwigDoASmfc33xrpS74x2/jkARvz3FavWw2C3dRMrdQ1ljCgfwC0BLHV6QpbWGprgaS+UswPAQbWw\nuN17PGb6n6d0c69myLRTBmPFJODHx27S1EyJgUsTtNMgOufzDBr71tEleXwW9hLyb+dz2oM3zdW2\nkX1YHq1cwVMm5ZgFq9nblDnRA4ZHcH2TjcY0q6OtietQSzOx0dRHlh7PGMNcRIl8o2N61gkggyA6\nwAGt3XOzYrc832UcX3u9Y3jXHIDhNvyKArz5duA/HQeuf5v37fxDH8dtz+Eb7y8+Fj2wzK2WbmIM\nLcyDetoHBO0XvJQ/vuN/AEYHuO8vvG+9Xf0sHj6+iSOrDZ8XSzPGC6oEMPxB+1Qlj/Ui3x+sL3HQ\nHjWEjoLbZsQUbF/Q3iH3pX7TAvrI4wEII/MeOLoeKWywKcy6F0G7ojCUCDPeDllr6Ti0hbGc05Dw\nKcq0R02Pz1QeDwjKGZx5QgqjC157HKY9Q3m8Wzf+vBO8CGCaNVCEhqNr/nZLuVLxs7ul5pB7+W96\nX75F/SK2g0vt44F2I9nkeLd8EuTvPxLNWtDcksdv1blQVoFv7NUAdhgACsRLmurYHb+SQHvLh5np\nAe0jYNoZHfvG6j3ee/9k6RH4xYmsuMw6ePyE+LkfWmkIwCNzZYVPTc/yjeQkGqi2ejvkBYufo7ks\nZLTdovL4dS0HKF3JmdF2EpBJNTVTlNxllR7f/ZtBCfIdw8oebJLm0QLbAIMVKo9frnVGy7QD0Ar8\nGldaYh6EC4ZLWYDhsKKTLFBHUxfXyxPdQMJSJkx7rzweiJ4gX5VHp2XRMGYMmN3nfbmPOdLkqBJV\nwGfk27DSWkVx1orr3sK/9/hn8OqL+Pv4N/ccwWbTH1wFVUsz8Vr128iz7r1q4bLB3+Mb/gPffG8c\nBe77P8KPT8Npah9aCQDtsjyeysbD5knP7PMeGiuHvMfbIo57o4FtUUdXdXQfebzgZ++TVVKZ85Ku\n0Vr3BcU7psrY0f03tHULT5/u780V5rQXekFbVIm8y7RX0MYNX3w18N/PA575Ws/zihLTHsWiwb3D\nGQV+hcjjl/vI4xObABGnFEVgwRfYJpZrnUhNEepnn/Ab9xY3OV4q9crX45n8JQCAEtPxtuJXoSpO\nc+qxE9XI10+trSc7o90tyrQrzr/18GozNITXLZcMnNpi2rfqbC6LsHF5I4xpJ6nNhYw3ooI8fj2A\naddGzrTLY99k7/2Sb0jVCI4zL8qSH5NA+8Hlxsh993IxNY86nONQmI3TZ3rZiaLNF+bUA6tI0VE6\nT5yqYROUbRfZ2EbHkCR3KR6nEETXBe0BTLs2ipFvs+d7jOYutopblYd9pdxurdQ6Iw14AwCjxNU0\nSlvs4LtgeNTHKDLtjR7v/cksE+595PFA9Fnt9Y6RPWgHhPC3vcxhsOV1Mqx6Rr4l1Zzb8Txg57XO\nY1PDS9RHsGva+ezWGho++JWnY/26pmbgJ9Sv8m9c+6bBj600Cdz8Lv71F94j/Fi3nXUyaOa9OPJN\nlscHg/axbXyO/XiTJ9RHZtqLhGkfRh4fddwbACiqCFR8fO0AcNVu3iR85Lh/sCmtRsicdiB6GJ0L\nZl+lfBel9f2O7P+v39DzPFVhXmCebSN0/XbLfY8Fpr2SEdO+/BQWxnkzo588fjxrebxbpEm1AOdz\nP77en23vy7QPI48HAMbw5Pl8jbihcgLnzfJ7XJRjBJwRzSLTPhv85DhFPO37VH5N3bnf//qi1ci6\nmTTC2gLt53JR0K4Hg/YSTW3O0C8MQJi1uQ1hnnaaHj9aT/sU6j1e8aV1l+EaMSAmTLsjj68KHfKD\nKw3Jlzt60A4ATZXfNFekWe21ti6EJY5NpJjKLpW8OVoxgiXyjhw1QQYurIpiEB2AwI7zSDzt+TJw\n3Zu9L/9D7nOhvsjleicbSXdIWWW60RblgDw5fsRMe4muQ42eRo1/cyGl9ZKCbCIfpmF0qyEb51rb\nEOTxmbFdxNd+Qc7Z6C5ttEKZObdMy5lDPZ7UyDe5Ln6F97Bw4l689wc5KPnre47gwJng+7hcC7Un\n8FzlMADAUgrAVT8+3LE9/22it53UInPAR1ASOm1yjxVleXywZH9xDw/q24EV5OD8nuignXra48vj\nvaZt1HFv3h8mTOem/zi85+3h97GHI4B2ujeSg+gAoFSgae/+oF0zLKx3FRt7Fak5XusNxJPZ9n7l\nsrGZBX5VZjnxY7Sxj/F/U6jKpyXmz2QK2ie4HdC9bqJI5OmM9pvNe4D/+XzgS+91OirA0Ew7AFx9\n7Q3e4wvVM9hNghyPrUXz3lfbOmbo558U0z6xw1M6TlmbGEMLv5v7CG7+0g8Ch+8KfamXtbDFtG/V\nWV2kI1wIYdqLNl/csvQLAwAmOdO+g63hsI8n7mzwtAubZdbAg0fFAIwlX3n8iD3trI2Npi4ElB1c\nro9eDeBTnTw/VzdXRcB07+E1L8wMAMbGMgis6pYM2qsg14eUIN/UTImBS/E4ybU97jHt/vL4ZscY\nDdi88R0wmfP+3ag8gdmNRwOfulKTr/Hsm0nlac5O1NbOCMF5dzzlnJNnF9Ne75Eju0x7KQs1DWVQ\nCAiLKo+vd3SMycF+WRRh2q+qcEXFvYf7+yNdiepYWtf5eXzTjGPfwSuv3I6bLnA2vaZl4/8+uBT5\nV7249gXvce3CVw/PeOWKwGWv8f3RAnMZwwCmnQLOfE4CGMGs4IW7FnDKds75HLOwkzm2lajyeCoj\nb0Yd+UbT412mPeq4N7cWLuWPH/xr36dcTZj2h471t2cII9/yfZh2zR9grzb49VbOS9v7Jz/X83zq\nk+9EGPvmeYeznIdN2PbdxmHvcVgTrtrSMccSyHoYpEjjy71uogDianft2cNO42fOfABYeRr49oeA\nI3c5Vr2jd/MnDzglYtcFV/Bf0TyO86a5aioq015rG9KM9oSYdkUFpvd4X/6Y+g38aO4O7NCOwP7q\n74S+1JvTTpn2pI7rLKst0H4OFyMd4aIR7JkqEUCkZp0mTuTx29g6HjtR7WEMV2rayD3tope0gQeP\niYDNZdozn4ctF2Gr3E7y3c9wf+6hlQbKlCXKZ/x5B5RZ5BuY+obIANx1YBVTdBMQhelIqErS5qhq\nB8vjmx0DY6OQx3eZ9iB5fKvdRo45Gy6L5aKPghm2pnbj4OIrvS8vXf5C4FOX66P3tJenOHiYsjbx\naDdRvNbW8ff3HnOeM3JPu6j4OSTNqT3pedqJMimt4yR+Y6wf9h5Glcc7o9NG4CsloP3CHAePUeYl\nuwnO42kd967ruR/69GNgWh0/eSOXhT5wpD8j69b5xgHvsXnlG5M5vgDQPsvqyMPw3djbti2w3OWe\nOe3BoP3ixXEhQd61M0Rl2mnTNQrTblq2l72hMCDX9fRGHvfm1vN/hj9++O99U+Sv3D3l5dk9fbrW\nd1RbSxj5Npg8ngLZnXmJIHni9p7nF3P92XtaXkp3Peo3agAAIABJREFUloFfxNe+0OTnfD95/LwA\n2geQkw9aAtPukD+RmPa2AcDGB3J/gQIh2nDPnwJ/+yPAse/w7+28ZrBjK4wBE92cKcvAZWW+3sRj\n2lPwtANCGN2b8tz6Y594EDBCQk+9qQbkuLaY9q0620qt8A1eyfIPiDFNUTqbL2YMNMszXpr1OGtj\nHE185Ql+g2t0DLR0E8VRM9hlGkTnMO1Udn78bPG0T+32HrobnH+635HnmZaNI6vN0R+jTzHy/rar\nYgjY/QeWMNYFS5aST5fBloqO0gGATcq0S/L4pmZmJ7nzSY8/staE6TPqp9Pi176pZtvwWt3FU6eb\nZw4FNhZW6rKnfQTnJdlczLAa7usyr/9w33FP9rlvkqRGjySzoux9hgVmYmlZvFacMEI7m+bh5C4e\nzNg4A3ScjXpUpr1ndFpmTDuXxy/onLmOwrS7vtJxpCSPL00Ci122y7aA4/fh2vP45vLh4xuRksYB\nYN7iXs/y9kuSOb69LxJUZ7QWsOHLtHcMC+6ylFcZCswCmu57zUI39WPFHE7nObv2XOaE0W2bDB5V\nJr/erShMOw37qhRyfKwclcdHuf/sfRHJJ+gA3/lIz1MmS3lcMO+c86Zlh44d1Awe5JlTmDdWkRZN\nlA8C7fR6XFAlIufQneRzcUoeJdevuHeYgKOA8yWxIkz7ZI2C9vD0eHGqwmDM9EBFmPZFuEx7NE/7\nq5Tv4mZVUqw9+Tng0Df51y/+VWDHVYMf39yF3sOLVG6ZiMW0pwXad1/vPdyHE95jxewAp4OVfB7T\nnqUCZES1BdrP4VLLXEJbNnuZ9n9+4Dhu+G3eXW3ZBeRzCcxTjFOMCRL5bWwdXyFzS92RQWcb077e\n1HF41VnE1hsaDi7XocBC0fPes9Ec5xz3AF6gnAQA3H1wFUdXm1hab0EzLVRG7cv1qRyZP63XORBZ\nrXewcppIQscWMp2NLc8C3rSD5fGtdtvLXrCgpKu0IGzPtMLnoPvd/LU2Ae1ZjHsjdeVlfEM1b63g\nnX/7gG9jYbnWyUbSHVZEIjnLqrjv8DpMy8bHvs1Tq19xCdmwjyjE0SKb4LUV0Yd6cqOFPExPWQEl\nRWWFogrhQNhw5nTPjUeblVxr6eJalBXTPrETUB3QV+isY6rb9Hr8ZBXVdnhCu9u8SXVKxJ4X8MfH\nvoOdEzmPWW5qJp463d/X3u50vJArACjN7g55doxS82L4F6lFtuGbhE0Z5HJeBZorALprQGW27wzn\n5WnOGt6gPIH58SJ2Tkdbx+Iy7WdqvBkjzDCPy7QzBrzoF/nX937UaWpZ4ntz1R4qkQ9WUaw1+P5n\nopTznVFfipAeT5l2AVgBgG32sO3FPPW0R2DaOw4bPAfyfqUNiAnTXlx9ytserDW0ntG8bjlMOznG\nAT3gA5Ugj3caB1Fmtdc7Ol6qPBT+pNv+M/Dy3xrq8KgS6aLVO/Ce3N/ianYg+jz5to7pNNLjAeDi\n7w/+2dL9gT9qaAaK0FBx9xhKLrv7Tca1BdrP4VJJamfF7mXaf+0fHxZmdjdRRE7NDhB55cpx4Pja\n735m1dscuTNFzyZPuzvr8cGj6zi21sQb/tddPgn3lUzBpVez53vyyl1sBcXuMX3yvqM4sOwspAI4\nyord6lPFSb6wW02eF3DPwTXMkY64EjYeKIOqhqTHay1+o9LUlD9/AhSmVb7RPHCmtzmnE9CedSNp\nfIHL2XawNTxwdEOwa7i1UtfOKqZ9DjXcf2Qd3z205skCpyt5XL+THNeIGl4qaS6Y1VPeZvpb+1ew\n/0w9W4uOj0R+XgiiC2a7Wu2mN47MZDkgVwh8bqKlKMJx37roXDO2Ddx/ZD3gRU7V2n7y+IRB+3k3\n8sff+G/AB3bhZ2cf9L71wNH+Evn6ygmvcbOGSbAkz9UX/ZLvt12prxxGR8HyWDEXO+X6OTfxjfoN\n+f345M9ej7wP0+xXAtMeIT3+DAG1AmiP62kHgMtey8+z9gbwqZ8Bfu8i4I+vBjYcu801BLR//tHe\nIDi3KMu5e8b/mqby+LZu4vh6Ex+98yD+6u7D+O6hNdi2LYD2KdtnYsIDHxe+pEx7O4KnfaOlYxIN\nFFn3My+Mp2sTA7r5Ac69lq09g+3kVKfNDlqbPZ720crjj601+47Uq7UN7GbEViJbVa59M3DLuzB0\nEaZ9/um/w9tzn8NfFH4PZ9ajWXNSZdp3XQuUA7zoYaC9Y2BSZtlHsT/PoLZA+zlc+TF+QxizGzxl\nslu2LXZbN+zxyDfDREua1a6ZFu58ehkdw8SXu6y74NEcMdPuXvz3HVnHL/zdgx7jXhm13xVwwoK6\nvh8FtjeH+MNffwZv/dh9AM4CCb9PjU3xTrfS3vBuYF987BTmWIZd+z4lMO2SPP7gcS7XMnIpN0PI\nxnEcfEO33we0mx3+cztjpt3ZoDg3x3lsIg8DT/swhcu19sg97XQztZ2tYbWh4UNf2+997zXP24kC\nmWIwqjwIhcwZ34tTOLbWRL1j4D9+6hEAGTc4fUB7VKZda/Lr2lAzVi3M8VFir5w87D3u52uveUF0\nVB6fItMOAGYHr6v/g/flg30aCwDQXD3qPV5VEg7ZuuSVDnC/4KWOB79bLmu4JDFyAtPe42fvf2wv\nvO56WN3Gfslq4ULjmciHKjDtEeZMU6Z9MZBpjzi9RFGBm97Jv37680BrDVg/BHzFYUNfeeUOb6za\n/UfW8eiSv0T+GAHte2b9r2nZ0/6Ov3kA7/+XJ/Cbn3kMP/qRu/HBr+wXQPuY6fO3lu4HTj7ifSnK\n4/s3PQ6vNr3zAEBoyGBiVRjj65Bt4poKP7/8JN2WZaPR7ohhaSMKotvJ1vCDyj24SHsSG81wlU+t\nY2AXI+PNXvwr3OO950bgB34vmeObvbDnW3Oshos6T2GzFX6MQNfTnhbTrqjARS/3/ZF57L7AlzU1\nE3P/BvzswBZoP6erVCqhZTsbKBWWMEvXlbC5nT4AWMb0iEA7kcfDOZ4vPHYKb/3YvfizO5wbdOms\n8rQ7wOiT9x7zJG15leEPXs+l6SOdfz7Pj+PqSu8My8qow/J8qkSY9nGrho2mjgNn6vjcIydGDtpf\ndSUHcxsgrEGTM8a6aeGxQxy05ysp++4JUCiZDbhyUz+m3ejw655lPeJPzXubFIXZWMQ6jkgTIlqa\nCa3Thsqcf4Ot5LMLy6M1xaXezubIxreJKuA1V+0EdAraR9TwIkzI+copHFxu4H9/86DHcG4rk811\n2g1OH9A+Wyl4JMZ6Uw/0YGtNvokysm6AXPx93sOb6l/DNqzhhcqjeOBgb2AYLRe0T6QZODm9l/va\nuzVXfxqT3XFFDxztD9q1NT5mbD2X8JrJGPB9vwO86dPABbd6314MSJBfJo2byVI+cnI8/XvKvhfz\nr/uMd6JF0+PrUUB7lR/rNhp2F3fkm1tX/6Q/M/jop4CTD2NhoogfeC7f//z13Ud8f83xrtpHhYl3\nrn4A+MgtwOnHhOdQT/vRtSa+JzUAPvPQkvdZMFgo6YQ5vfx1/PH9H/P9nScDppO4Zds2Di3XvfMA\nADC+PfgFSRaRyN80we2Vj5/oVRPUNQPTds2736A8k+39hoD2OVbFhwt/gk8VfgunnwmXvtdbHexg\nRKW28BzgbV8GfvKfgLd8ziFskijS0KR1g/JEZO99akw7AFziL5FX1/b3WBbdqncMbCNYhzbon221\nBdrP4SrlVdSonJd0i92O6wKRHi/bU1CV0crjtzOH6fjswydw1wG+QC3myGKRYRCZV8LItyYYLMGb\n+wu3XYyXnE82b6NksImv/VeuZnj5ZdtQ6HbM58eL2DdFw7TODtDOhFFWDSxttPBHX3kali2eo5l2\nxLv1X9/wXPy/9s4zPI7qbNj32VW3JFty77bcOy4YDAaM6RgwnQAh9F5CgBBIIG/IG0ggJC8kgZAC\ngSSkEPIlQEJvBoMBYzAG3HvvRZYtyZJ2vh9ndufMauW6uzOHfe7rmsu7syPr1uzMnPac59w1aRDf\nPmEAax2jAmasvztj2RbULq/BXFCS4bXko/mJ707hJEb+Fm5o3mg3p8BEst1oh2aRNPHIlDgba+p9\nc4RVUNdkSWVi9LxM1fpWLOhUXsSYnhXQYLgH1mj3KlW91FqWbNzBlHleY/OG8V29YwMIj8+LRqgs\n8UbbWwpRbazzrtVYthvtgycnkui12TKLD4tu4C8F9zFxze93myV7e6ol39I9N1IpuOhfcMpD3i4c\nxuXpqI+lm3ayaTcRDACxrd6zaXtBBkc7jcpve7fDPXl0c9EG7z6qat9qn8PjAeh1uPf6tbth+QfN\n5oenwoz6WF9dTyxFPg2TdUajvUOL4fH78GwvKIGxV6b+7PV7APjGOG8K0XOfrWLrzub3S3ykfUJk\nJoM3vw5rPoO/X+Q7xpzTPmNp846dZZt2JhqxZewk4rjnr7Acxl7lHfjZ32DFdABGdvfK5SffX7rb\nEO7NO3ZRXdfoL6+zNZ2to9doH5bvdZ5/sap5o33bzgAzx4OeBpTUkRNVDrP/84tmU0tMCnaup8Cd\nTtRQWOlmeu+oOyDT2elgPtMNDonMaXFJR5Pa2rpEnhBHRfY+MmVv6TPRW2EDWOkYdcLVnzY7vL6x\niTXb6nwDlGab46uGNNotpig/ynbHq1he/ps3uOZPM3jm4xWJtbvNC3m9E1DIiFGp75uvR4bNsuHM\nkV05pL0RllPm9VRmjbyCRIU+SoweJV6FYUDHMq45qk84KvQA7bxKfaeGFfz+4jHM/P5xTLtzItO/\ndwy9So2Rr5A02n3TD9QOXp+zjv9+rhPpBT3SXtmqgCuOqOKEIR1ZZRYQ21YkXr41d71vnqvK9Dw+\n8HVeVbrnaNH6mmYVq5gxOhzN9uoQAK29RmQXtYmlSSPtG2rq6eQrUAO4v0E3lox1YLsZoYiThncm\nElFJI+0B3TtG+GIvtZY5a6r50hhROryrsRxUppdHbGHZt7Z7sexbk5EDIutTDUoqod/xzXafFXmL\nmctbDpGvqW8gQiyxmgWQmYRGZR1hzKW+8OqTy72w8A/3tDzddq/hsrM4g6NKZibsFkbaFxnRP307\nlOqVBuLsbSdsz/H+90+cAK98b48/VlaUT0WJbtDsaor55qynwhceX34AiehMDr1WZ/MuKIOJd3kN\njkVvwJaljOpRwZAu+v+sa4jx6pfrmv0X8XN6eMQYXd+8CGq3wNbl0FjvC4//eFnq6yPeYeoLFS6p\n1Nnu27sJBht2wB8nw7JpXHhoj0SI/OertqXMRxJnyUb9XG/vG2nP0rPcSI7Yo8mLVvhyTfMpAOu3\n1/nnsweRJyfFecmr3ciVT33cYsdSq1pj+l15mhJLpiI/dXTWqMgCVm9qeYUD0NEWqs4oy4srdEh7\nOimp1FMDIvnM6noebzUd5H22qnmI/ML1NTTFHDph3BMy0i6EkaK8CNXGElVbNm/k5S/XcvuzsxLz\nH81QpvVOhpfmaImOXijgSLWACP5wyhuP6UfELOizFXKVjNGw/OvXujO2VyUDO5XxywtG6pFsc7mU\nLK4l3gwzvGmTHp0pKcijc+tinXXWaGyaUxMCxRxpp4Y/f7A80XEzuNyoaAU4p71T62LWOF6ol7Nt\nFcT0tfrWvPWZneeaCiNz9y2FzwE6DGxtteexqzFGXpP3PlIQQGdSuddo76Q2s3JLrS+r78btSWF/\nxvFZxzin/Qq9+/m0EW7HYhg65ozw+Cq1luc+W0WjW9Hr074VpfVGpb88wyMKFd4oIVuWJe6HPS37\nFos5bNjsfef5xVm4X5IZ1nzt8naqmsVfTm/xR5ovU1eqE9tlCiMs/BA1J/H6/UXNpz2Z5NWsSbze\nVZLB8tIcaXcbQs1H2r1Ge5/2pbDDcN/bBlPbPlDR279v+u9g26rUxxt0r/Q61/a0HrbZqO9Q5jZe\nNi2CDfO8g/Z15LC4Aq5+B+5YBkd+G3of5X224iPU+7/g3tJ/JHKTvDWv+RSN+Eh7Pkkh/g8Nh4eG\nwZOn0CrPG0gw233lRc2z8x/U1ohSKGmnOyzP+p1+Dbrh/uK3aVdayDljvAbir6e0nE9gccpGe5Ya\nxEZ4fJvtXh6SeWu3sytpqbrPVmwLbrm3OCk6pqvUGmavqWZZC9do611rjDfdUx6TNoypYnGK1S6c\nVZ/s9sfqGmJ0dLz7W2UqiuGY78P31rDu8P/lM8eYg7+yeTK6eWt1B5UvPD7T5WKASKPdYvKiEd8c\nXDNBSLxXNBSN9soqKNONx6LYDgarpYmPDuldSe+KQqOgV8E13Lp4PXpdVr3KM9eM4+Wbj6R/R7fC\naYw0+ZZCyjZGeDwbF/rDFmq36t550MseBdUBkowvPL7GV9HvUWSMzAbYaC8tzCNaWMoWR99TKtYA\nO9azYvNO5q+ryWxG6VSMvznx8gzeZpw7CmPOa99R3+hLTBZI6Hm5f3WIppjjS1a1oaaeLmajvXWA\njXajMnTbIcVMGNCeuyYN8pZmqjU65rLRMZOK0o6JcPJytZMKxxs1G92zAqqNhkymO0CKWnuhnk31\nUKMTX7bdQ6N98cYalJFjpbBVAJ2cA05K3QBb9FaLP1JT15jZ5d6S6XEo8USOHXfOSzTs3l/Y8ogn\nQGGtl4m8qTSDHbMpRtoXrq/x5TEwR9r7tC+Fam/EcK9Dk5WCs5+AYed4+2KN8OGv9/ij3Y1s63ua\nk7the1J4fP12+NsFEJ/6VNG7xTm/eyQ+4tjtYG/fm/8Lr32fg5Y/xWXRlwF4d8FGX6dmY1MsMZ+8\nu5k9HLwIgJUfMbg69Vz/i4zw+ziHm5dEfM5xp2Fw6UsQdaNk1n0O1Wu48ogq4rMmpy7cyM4Wls4L\ndKS9bd/EdJdI9UoGVOg6T0OT0yzx6WcrtyaFxwdQp0hR7+qnVhGliXlrU2T1ByobvM7YSEXz7zSt\nTHSjWErasqGj13FYvHrabqeYbK9roK8yyh8jv1LaiebTv2Mpn8aM+3HVjGYJt+cmGu3GdSkj7UJY\nWaO8h2YP1Tzsymy057cJaNRVKR2e5TI+3+vV/trY7m44nXsjtmq3x3VdM8bQs7zXnz/T7OHAVi9j\nbyKrZxCUdfJCNuu3+RP/bDUS3VT0yuwo0b5gLqmHl1gNoHUs2DntJp1aF/lD5LeuSITxl2azMg8w\ncJIvgdB38v4K+BvtNfWNFAedxDFppB3whchv3L7L32jPZOjfnjA627qxkScvHcsVR3jr1vo75gK6\nx5VCtfWc4itEQLzRbjSKshG1kHLZNy88PtWyb58s30qJMWKtglgzN78YznkSBpxMbc+jE7u7b/2o\nxeR51XWNDI4Yz9CWlh9KF8UV0HEoAMqJcWi+HkVcvHEHa7a1PL+0VV2Woi2S1pxWxKiua+RjN8P9\njvpGVrtT8fIiip4Vhf55p/tSqe86Cs76PXztr96+938JL30Hlr3f4o+ZI+0rUmQTN1lfbWaPL4IP\nH4MNc/WOvCJ9vRxouG83L+O+WWe4Jf9ZQD+zPzbmpK+trktE0vSMthxhMWbTcySnJMqPKs4d03xU\n1jfSbpap7ftD90O890um0LNtK3q11Z2EjgNLN6Y+h0vc3AX+Oe1ZarRH86Fd/8TbSRVeTocvV/tD\numeu2BrcGu1xUkyfK1QN9FJrmbe2eV6aWMyhfZN3T+e3zXDZM+Jr8M1ZcNNMHKPe23Pbx9z74pwW\ncxtU1zXQN2KUP+0HZFSze0UJq/O6Uh2fBrxjvT+SFLPRbobHhyTCNAOEpEYv7C9rot7F2StFo709\nXqP9/Iljmn2eNYxEM2dWLgWgX4dSThraGbYb65cGOTI84CRvBHXzYkgOFTIbxEE22pXyjwZsnO+9\nNhsdLSQcCYT8YpyoHp0rVA2+0eHiBuNhG/A67Z1aF7HaCJFn2wpe+EwXUhXmPMFsTY846QFQuhI5\nXC2hNTW8Pse7z6vrGrK7bncqkkbaAZZu9BrtG2rq/OHxrYNstBsV3KTCH8fRIeBxArx/lHF/9zYa\n7aN6JDfasxAGaJ4Hd63cPYXHz1yxlVbKDDMPZvk8+kyE8/9K0ele0rfRzGHOyg0pD6+pb+D0qDGi\n2S/18kNpxQiRv7T0w8TrFkfbYzHKdnn+0TYZnv/qRivk0ZRYRiu+VOtiIwldz7Yl5G/4whsdLuus\no+z2lf4n+hpofPgYPHUarJ+b8nBzibTdhcfX1Deyw12erjAvQnlxnr8z4Nh7fNF2+03X0Xs85G0j\nRN7LEeDQhdTXJUDBsimc3du/JFf/jmX0qCyhozE/v6QgSq9io8OnJKnjqWqC93rx24D+7uIkr/4R\nJ/VIexbLa2Ne+02rv8ND+b8CHF8yus07drFs086k8Phg6xQmA9WKlEui1uxq9OVYiWQjkrOiJxSV\n02HEicTcaJ9DI7N5fuqnKadwgO7U9I+0Z7bRHoko+nQoZ1bMeI4krdcej1zwZ4+XRrsQUtZGvUpb\nqpF28wE7cvCgZp9nDSPRTL+6z3nzliP4z03jdUbUGsM7qCRVoEdmBp3qvf/8Gf/nvgp9gI120MuB\nxFk323sd1ka7Ur4Q+Xjm7oriKBFjabXEnLuA6FTuH2nfuHpxIglY34jRuZStc1vWKVGRjCiHsZG5\nvLdwE1Pm68pdTZ0/PD7jS4ClwhjtjTfOl27amQgBbTbSHmR4vNnZZnbCgZ5WEm9w5LcKNurDTEZn\nXHd92pf6VjXIyki7Ge475aewfS2djOWyUlVCP00aaQ+s0e6iKnqxIV+XlSWqnuWfTUl5XGNtNcdF\njErhsHMzLzfivMTLcbXv0tVtuL3fUlKwHeuJohufm51SSkszHPVjVIB7unWMV2evxXGc5vPZzeXa\neh5OYm3AfSESgSNu8++LNcAHj8K/r4Nfj4cXb090qvcwRtrjS6elwjfKXl6omynGmuUtrQ+9z7Rq\nl7JT31ERCtCN7tdmr0usix4P6W/PVgpJndQxzlWlU33vB3cuRynlywJ/WJ+25NUZHeHJZWqVF3XC\n4rfBcejVzrs/l6RotMdiTmK/f532LNbXqo7yvT09+j7D1BLf0nefuUv0tgs6Ed3Ir3uvlRe5MTCy\nnHmpGu11SWu0Z3P6ZXkXVM/DAJ3l/tToNN6am7rzaHtdI32UOdLeP+Vx6aRfh7Kkee1eMrotO3ax\nrrqeKE3+jppsXpdZRhrtljNvl/dA7qXWcewg7wFVRD3l8Xm40QJfoynrtOuX6PFUdVupii2jMM99\nmJmN9qDnYA835tTNfdH/WVhG2kHPT4uz1qh4hLXRjn/Zt3hn0qgOoOJL0xS11ln8A6RzUnj8yiXe\nVI5hRUZB1jaDc7mSMaaWHBLRyaoufuIjLn9yOv/9fI2/sE/3mql7g1Gp78BW8mjkyfeXMvj7L/PP\nGStZuKGGzoQkPN5M8LM1aaR9yxLvdWXv/WtwpAtzrXZ3pH1w53Kd4d4cac9GB8iYS70R0/pt8OK3\nObiXN3r30ZLNvjm6O+obmbe2OmmkPYDw+CQ2dzws8Tq6bGrKYw7a8V5iusmuyoHQaWjmxbqMhF5H\nABChicvzXgJgWkvJ6IycBmudtrQuzvAa1J290efxBTp8f8XmWuau3e6bqtO3QyksMxrt5jJu+8qI\n8+Brf4HeR3r7PnkKZj6t52J/9Bv43dGw7H3/nPYtO3EcJ2V4b7MkdDXrYKd7jvNb7V9UQEt0ax7V\nqJwY/fL0CObijTv4xuMfcdmT0/n2s7r89s1n7zQMzv0TXPJf33SBqvWv+f7PeGN74sAOdFMb6Mwm\nJg3vDL6O8KQyoctBXq6H7Wtg4/xEeDzAsqTw+O11DTz94TKd9BRzje4s5x866Otw+mO+KMMRkUXM\nWrmVhetreGzKIr71jF4LPfA57V1GwsX/0XkaJv8qsXugWsGSjTsSHTZxttc2JDXaM5yILgk13Ouc\nPC36HgvWN+9YAKjZuSPRcQfsf/6HfWBo19bM9M1r9yJg46Hx7dhGVMWn2LYPvB6ZSaTRbjnz6iuJ\nObpy2UVtpF9bL0yqQ3LCkCAroUpBT6/SxJJ3vdfbzUZ7wKFMvY7wRiu3LffWnK2rTkrwFnBPnq/R\n/rn3OsSNdnOpuoER3WAaUWGE+wWYhC5OlzbFvvD4mvVLAVDE6NKUpQQsyRjhs+OiXojoG3PX88dp\ny+gXMUZezQiMbJFXkOiQiygnMeexocnh9n/OYtH66sRcdyDYkfbSDt79XbdV39dxwnTvGCPtfdyR\n9hsm9oWGOq+hoaLZeQ7lF8OpD3vv5zxP9/p5dG2jw5J37Gpi1kqvkjxr5TZiDkmrLQTfaC/oPS7x\nuvW22c0+r6lv5Mi6N70dZgdupjn8m4mXF0Tf4BvRV1i7bSdrt9U1P9botFnjVGa+0W40vk8o9bKL\nPz51ia/R3qddiT/cPHkZt31l4CS46LndN6Y/fZoubYoTVZs12+rof9dLnPTwu76kc5DcaC/0l5ud\nhqY3/0sLIfI3DPdef7hkM2/O9cKQuysjJLmiFww+TT/7+x6r6xxAZOtSbhuvO8zyo4pThusO07NK\nZzGl+DamltzC6RVL/Y325IihSNTfGbLozRZH2usamjjz0fe5+zmdBLWS7USCyj8UicBB58OoixO7\nhqolxBw49udT+MlLc9m6U9cngl5GFoDeR+g8SZ28L32gWk5TzPFNKwGo27qGIqXda1Rp+tc+3xOD\nJ+NEdEP3oMhi6tctSH3cxsXkKd1Buzm/U1YiqE4Y0pGZMa88dNbMhCadLHGuGxrvq198hZPQgTTa\nraeeAtagH+JR5TC4xGuod8CY4xF0IxP8BcUSIzyxxgg7DvqGi+brNVfjxOfP+JLQdQ8+wZvZaF8/\nB5rcxu9mY7Qw6IZHMkbhNVjpqIWBZeFY7i3OEf3bs1Z5HvGMrn0Lq4nGl1Yrrmw+TzCT9Dg0sfbv\nILWMMsyREIf+ymi0dwig0Q6+udVDI9412BRzaMc2CpQ7slBcEWyotFL+OfXmvPYwNdqNTqGB0VVM\nvXksJw/rrEfG4pR1Sv8auS3R+0gYcmbirfqwwQkOAAAgAElEQVT4D3y7zVvckvcMhezyjQrPWqnL\noDCFxwN0HHho4nVVwwJqd/lHuxZ8MZ0jI7ohF0NRcFAWQuPj9D0WOuilUYtUAz/Mf4p78p7k0+Vb\nmh+7eXHi5dpsNNqNSJ/+9V8klmx9dsZKXp3tld1D8lfpjjDQnXjp6NiMRHyNtGbM+y8FKkaX1t68\n9oYmh7lrt3Pfi3N8h/qT0BX6I9TM8jQddE2dP+jETtu5bkKflJ918420GpF8eQXQ2Ss7r+27jce+\nPornbxhPz7atYOdmov/5JtFYA9FYA+rNH/mX3UsVfWWGyM9+jl4tzGl/fOoSFhgdM4Fkjk/GyDsw\nzChrPBzaYiaiC3ggqF1/iOjOje6RDZSzo9mUom1rvM6wTXkB+BZXQL/jEm9v3PU4m7c3n2qSv8Vr\nzG8u7pUNM7pVlNC1e29WuYMpqmEnrNLr3b/oJgn2z2f/6i73BtJot552pYUsi3kPzz5Rb9Q6sLlH\nLVE1wXu9dKrX0PSNtIfA0yxw4/NnwhQaD7rRGA/1baqHjQt076PZCAl63n0yZqPdzdBcVWI0QAPO\nHA/QtU0x/ft7Dd8ubkXq6iFGBT+bo+yge93dSmWEGC9M9hpqHdhKa6XPYV20VXBroBsj/A/lP5JY\nng4IT+b4OOZ8QbMzLkyN9pJKL6t4rIFu1W5G7mwnoTMZe6X3+pOnOH3tL7gp7998J+9vvvnX8VGk\nVsrokAtBeHxJ50HUoqMsOqqtLFi80Pd50fRHE6/nlI/P7rxSpeCcP/im3VwQfYP5C1OMeM17OfHy\nc6d35hvtxpKt+Y01XN3fa8TFV4cqzItQtfUD72d6Hpa+yL6DLvSWKYsWwHUfes+52i2w7D26VTRf\nNeNfn67iw8Xedelb7q28KGmkPd2N9lHe2uIRbzRabVrIt08YwP1nDeP8sd354eQhieUm+xcYz8nk\n548xch9d8wknDu3MoM5uMtTX7vavILN8Gqw2kuimarQPOs2ba718Gl3ZQJ6bmn5ddT07dzWyvrqO\nR9/y7pHD+7blshFGotOgGsPGoMqAyMpmeQDK2UGhcpetKygLZkUVk7yCxLMcYExkXmJtcQDHcZj9\nsTeIVV8WTL1NGc/3idGZ1L76v82OKan2OheqS9M4nWQPnDK8M+82GffoJ3/i2Rkrme6uwtA5khvL\nvYE02q3nJ2cOY3XEu0g7x7yeb194fJAJ3uJUVkFrtyK0q8YbxQ7TSDtANyO0LeVIe4BrtJskh8hX\nr9Jr24Ie6QjB6JYPw1ePtDt0yTeWPwnBSDvA2UeOot7RFa02agcdChs4uYvhmc357HGMUNNe1TP0\nvEWgvxEav6WkT3BTYI68LVE5bKXqebr1rzl/pO6E6RyW0PiEgzFf0Gyoh6nRDimzPGd1jfZkeozz\nZ/V2uTj6Cl8uW0Ndg+7YWrbZbbSbSySG4VkUibK62Juis3G+l6md6jX0W+vlMFk95KpsmmnaD4Br\n32NrpR5JjCqH8kUv+I+pWY+zfBoAMUfxpjOGkoIMR1skLdl624CNTBpmrFrTtoSHzzuIgllPez+T\nrqRuoJfsOvsJ6HcCnPe0jiYaeIr3+ZwX6FFRxHGRj7k571luznuWw92IiR+8MDsxv31ds5H2DDba\no/lw1dtw7TT4+j+9/ZsWoJTivIN78OMzh/ONcb34f9cexn9uHM+kHkbjM7mOkWogAfRz4dM/794l\nVaO9tD308Ubb82b/07d03rJNO/nFmwsS2fb7dyzlqUvHclZ/o4MoqAGWotaJKRN5NDFA6YGKHpUl\nfHnPCUy/aYh3bBDLvaXCmOJ2SGSOr9E+Zf4GBmyflnjfeeQJWVVL0OdoXq88P/G26+eP+JMcA+U1\nXpRPbevUESOZ4ORhnflb08TE+8bPn+UXL3r3wQk9jBwWX+HM8SCNdus5dnBHzjjmiMT71rXeSGsH\nM2Qk6ARvoAt/MwNovCJaY8zlCsVIu9lo/wRisXBljo9jVjT+dRU8bEyYC0OjI5nyLsTcdY/L1U4O\nqaihZJdxjYZkaZbRvdqyKeoV9tccVEBJtRGG1y7zyVeaYd43816kc5me42iGxleXZa8QbUa7fnDp\nS97c9trN/KDPPC4f35uzzdMVVCSAibF0EMu9yhKbl3qvw3D/JGd5hmAb7UrB6Eua7Y4qh+Od9/lo\nie6cWbZJR35UKKOjKwyNdmBnW++ZGVtlrCc+/XfkoTs8P471p9vwCVk2c8krJDr6osTbMdtfZ1ej\nsab8vBdR7pzi6c4AmorbobLRUWfMa49+8hS/HLGMxy8ew9+uOpQ3b53AiWWLYJMbFVBQBkPOSO/v\nH3QqXPgM9D/eex/ni2e5adkN/K7g59yc9/+4Oe//8XTBjxkX+ZI5a6oT1+NqIz9Ap+Im2OSOGqqo\nNyqeTvIKoeNgfyfvxgV6aUmDaEQxtGtr8raZAwNJdYzkgQTHgV074YWbjWPGNneI5LU8P9pcGWHW\nM/Sp9BrkSzfu4N0FXoj9nScPIi8aSUoaHGB5bSRHPK/rJoZ3a81jXx9Nq8I8Clcaz/QsJErbK5KS\nyc5atS3RmfTbN75gXMRrHJcNPTnrenHmD/0W7zcZ98KMP3ivmxroXON57qrI3uBFlzbFFPQ8mNkx\nfV/kNdVx9K63AehWUczYdkZUV7k02oWQE23nVdYjxmhRB0I20g6+3l3e/jH89mh/SHcYGu1tenrL\npNRvg82LwhceD7sfHQhDoyMZpYgYzj84uMkfZRGC8HgApRTlHXsn3l9U+2d/gqUgRtp7H+WFGG9e\nzOB83XjrZzTaa9tkfvmV3dJ+ABx2Q+Jt4cw/cvcpgzm2i5FsMMg12uNUTfBeL54CsSZo3AXV8XOp\n/KPxQdFzHETcivT6L/U0oiDD4wFGnK9zOiRxQfRN/vXpKuoamlizrY4qtZrRESO0O52ZuQ+Awh5e\n46ds8xf6RVMDTTP+mNj/R06hf8cML6O2G8pGncUudKTPMLWYRXNneh/O8UbeX246OPOh8XF6eQMD\nbFpA5NlLOGbjnzm0qq1e0WDGk97nw8/JfOLBHuO8EeTaLXTf8UWzQ+7M+wuKGDNXbGVXYyyRawFg\ncMNsiCdUa9c/syHU5V10dnrQc/7NJHFxGnf5O+SSR9orenv3Xd1WuL+X7qSPr3hR2BrO+5M/AgGg\n+6EtR18NnAT57uj6hrn8atW5XBLVUy/mrt2eWIouomBclXuuzZwaQdbVjHntF/bYzPM3jGdwF3e6\nwIJXveP6HZ9lsRboOQ7HXQt9qFrKzu1bWbmllk+Wb6Fw5fuJJHQNlf0DHRTq07E1v2wyOtw++7vu\nHAKY9ivaN+hrdKdTSFOHISn+h8zxozOG8VzUm3d/fvQtivOjPHjOCPJ80brSaA8NSqluSqknlFKr\nlVL1SqmlSqmHlFIBrmUWAiq8BoaZpMafPT4EI+2gGx8m5tyrwnIoKCFwlPKPtq/8OPwj7cmEpJLc\nDMN5UGyhb35mmJxLe41KvM6f/19YZ1QKsz2nHSC/yFcBGbpNz4Ezw+Mb2gbcaAcYcYHX0Fw5HdZ+\nYTSGCUejvf1Ar8JZtxXWfKY7Dh13RLO8iz7fQVPQCrof4r1fMiX4RntJpY6omPRzuPETYm7G4ZGR\nhSz/4j2+XK3zqFwR/a/3M/1OCM2Uok5GMrpeuxbopermvUR0p54TvNapYEOXY4hGAlxppbiCOaWe\nZ+P0J/WL5R/qTiaXV5oOpjxbjfZ2/WD41/z73nkQtq/VkXKzn/P2j7408z7RPJh4l2++eEzlsbjC\n61wYHlnCSZGPmLliK7NWbqWuIUZftZKxFTW0nfoD7//qYdxjmUAp3xKOvHUv1CctqbXoTW9qW5se\nzetByXWSuq3+eezH/6+eWnj6o3DSA3DUHXDCfXpaQUsUlsLg0xNvi5pquDvvT3RX63h7/oZEvoLu\nlSUU5Ud1zhxzGdwgR7GNkXaWToVGd6S1oRaWvON9ls5pGgdCcQXKndeep2KMicznk+VbeHzqEo6O\neJ1y+QNPDMoQgH4dSvkgNogl8TxZ9dvgy3/pqJS3f5I47qHGMykpa5NVt/4dyzj7kluoQ5c5gyPL\neOGCzhxa1dY/8CeN9nCglOoDzAAuBT4C/g9YDHwTmKaUCmCB4pBQaTTatyxhUlWU1tQwLOI14EMz\n0t6qHXQ7OPVnYRhlj2Ous/rpn2GTkbQoLCPtbXp6lWEV0euYFpTqueHDs5j5eF8wktEx9eeww50a\nUda5eYdOkEy4A4afl/ozs5Msmxghod3Wvg44vpF2p92gFD+UZUrb+0NX/3WNv2MmDI12pfyj7fNf\ngY9+570PU5RK1QTv9TsP6vDaOEGdyw4D4eDLoW0f1ODTEruv5xkeeWsR7dnKWVFjHfTDbwpAMjXl\n3QazM5GMbgsLXngQXvx24vO/N01geM/gqxKbqrzRrkHLnqbhzftpemoyxPSI3IxYP1bTLnsj7QBn\n/gZu/MRLqtWwE6Y8AO/8FJrc+dhdRvkynWeUMZfBrfNh8qMw/hYi106l6pv/8S2fd1veM8xatoEP\nFm/i7rw/8Xrh7TxTexVsnKcPyG8FR9yWeVdjZJiPn4A/np5YtgqAz5/xXhsNaR8tLCPHoFNh1Df0\n66LWcMjVcPSdMO76Pdf7TrwPDrsxUfeKKofLoy+xYMVaSt1VSvq2d6MmFr4G291Ow1YdoM/EVP9j\ndugyEvLc6IjNi717eMm70OhOg2jX3183Dppe/hD5Fz5bw5QvlnJsdIZ3TMCRAT0qS8iP5vFXY/74\nspcf4oPHrk2c1y9ivXi86WTKi7L47HHp17Mr0X7HJN733fSW7vCK188jeeEZVMsQ1jTagUeBDsBN\njuOc7jjOHY7jTEQ33gcA9wZqFySFZV6iklgj93WdxgNlz1AZn1NY1sWXvTJwTv81jLlcF/AmscbU\nxwfBAGNe0bKpOkM76MylIQnjRik4948w9mq45L9w+iPwnWXwrS/9PfthoqUK3ciLsrvm654oLIMz\nfwsXPuvfX1mls8EGQb/jEuv1lmyZy115f6Zc6WRf25wSCipCstTJmMu81+s+9+6dyqqWO+yyTdUE\n7/WUn8CHv/beB7HWfUsMO8tbV37jPK+xAaEYUVBH3pYI+5wYnUm7BX/nDwUPUOiGe9JllG8+Z+BE\noqwt9eZsDp55b2KKTsxRPNM4gcP6BP9873bYOXwc05EzeTSS/859RJv0vb4zv5I7G64AyG6jHXS5\ncuwPvPcfPw4f/dZ7P+GO7Pq0agsjL4Rj/8fLVXH4zcQK9TzuqshaBq97gejMP3F53kvNf/7Y/9FL\nuGaaiXf7O6VXfQyz/qZf12/3j2C31OE+4jydLwAFB1+hO1BunQ/n/Xn/E5AWV8DxP4IzHkvsuiTv\nVWYXXca0whs5IfIRfTq4jfaPjfnNIy/UyfaCoqgcjrnbe//JU/Duz2G+0UEcltD4OMZz8KLoa+ya\n9yr3RX9Ll3ii1qI2ennXAMmLRqhq34p/Nh2ZSMbbs34+hzZ4STvvbLiCmIrSobwwEMf8IZO9N7Of\ng9fv8d6P/Hr217jPMlY02pVSVcDxwFLgkaSP/wfYAVyklApHtpsgGHd94mXr6Q9xQsPr3meTHgz2\nAZtMu35wys/hyjf9+8PUQ9ZpqG9d4gQT726+L0i6jISTH9BL7IBu+OYF8zDdK9oNgI4pwvpHXdR8\nXxjod5y+TuM5Dloafc8GhWW+tVSvMCqhs2O9KA2g5zslvY/Q94kyslpX9IaL/h2e51DVhNT7ux2s\nM+GHhcoqOPXh5vs7Dg1H1EKHQdQP8RoZD+T/jqGRpd7nE+4MbkWDFmh39s9ZRvMOrimx4XTrPYAj\n+wXfaO/XsZxftLqRXY4/M/xKpx2Td97FfEc3NLPeaAcdcpyqI6bHuHA0lEoqiYz3ErT9KO/3XFud\n4h6qmqAbv9mgtAN84zlfFADPXQ9v3w8Pj4BGd6WF9oNaHmCprIJbZsPti2HSz3QHSroiKKuObjbd\nrkzV8mj+w5xc/Q89t3nha96H8ZH9IDn0On8yvTfu0Z1IccJwLZr0OZqYG9FQpmr5Y8H9nBY1kuYd\n98NQlI9njerGJlrz+6bmCfGeazqMz50qzh3dnQ5lAU0h63+CNy1m9Sewxp1ekFcER30nGKcsYkWj\nHYjHarzqOE7M/MBxnO3Ae0AJEGw3VZAMOi112PbgyTrhSBhRCi57xVuDdVjIQrqT5szRc3x45kjZ\nSiQCF/zNn8yt73GhmfOakq6j4ebP9fI9QRcKJ/64WXj+WqeCBxrPo6woRJEKR94GV0/RWaSHnQuX\nvRyuTrnyLrqCHKewtZ6nfdmrwcwV3x0jvqZDWOOMvgQu+U9oGsNFx91FA/7KZpOjmD3mPi/bd4go\n7zWSRWe9wk8bzuWzWBVzYt15u2kEP2y6lHsmD8lONvY9oJSi/5AxPNjolYl/b5zAKfX3sqDJy09z\nzKAAMngrBZMfaZ6Q89gfhOaa5JBr2JbXfJrDwkgvHYl25Vs6iiqS4eXyTJSCI2/3OoAB3r7Pn5hu\n+Dm7P4dF5TqvRCbcDms+jSWqHA6a+zO9Ok286t37qHDkn1FKd2gay6EmaD9IdyKFicIyIhf9m40p\nUnA5oy+F0RcHINWcy8f3ZtKwzvyi8UwWx7xnTYMT5eeNZzNpeGfuPSPAyN2SSn9izDiHXBO+sjsD\nhKiWt1sGuP/Ob+HzBeiR+P7AG7v7j5RSM1r4KEQxkftBNA/G3QAvefPz6DxCV0TDTI9D4ZqpULcN\nuqdYsiRI2vbRD4Jpv9IdC8fdE55Kic207qaTWf33Wzoj9kn3B220ZwpK9PI9QdOmB1w3Dab+H8un\n/pVX64fwUONZ1FBCaWHIHuedhsE5TwZt0TIn/hheu1vnWTjm+zqRU1g5/kcwaLKusIRt6kubHsw/\n+Ie0+vAhSlQ965w2PNx4FneMDcFoXAtMHNaDxVvv5PzX5rPTXYv6qiOrGNipPGAzj+OHdOLcqafy\nQWww1ZSw1PFPh7jzpIEc1T+gdagre8O178F7D8MX/4ShZwce2uujoITZ/a9l3OwfJXYtjHXh2f4P\ncEfrbsFFqRSWwlG3w0u3N/+soKx5sr9sMvQsFkx/hYZl0/l70wTOjb7NkMgy/zGlneDknwbjl4qC\nEvj6s/DMxbDgFb1vwCQdXRrUNLbd0XEwT/T7NSPnPMCIyGIcoHTQcbQ66YGgzRJEIoqfnTuCK2ob\nuHPRlfyl8D6iNPFFt69xxdBjOH9sD738X5AMOhUWv+W973FYuCLkMohyktaMDCNKqd8CVwJXOo7z\n+xSf3wt8F/iu4zg/3sP/1WKjfdSoUSUzZrT0sQU01ME/Loa1n+vG5qHXhWuesI3EYjDvRT3vrfOI\noG0EIcFVf/yYV2d76+Yu+fHJoRglFHKPhqYY4378BhtrdEIypWDOD0/UWadDTF1DEx8u2UxjU4yJ\nAzuE6v5pijkcfO/rbN6hz+nATmVMGNCBaYs3ce1RVZw4NPicBmFm7qpNLHzsAg6OzOXpxmN5Qk3m\nqSvHM7pnBkaq94XGXfDva2DZNOh+sM5HFC3Q4fodgh07mr9uO8f/n86+XkwdVxZP4ZZu82DDXD3X\nfuJd4Zwz3NSoG+0lbcPVeZSCGcu2cM5jegnZB88ZwZmjQjDNKQWxmMOGmno61MxFbVuhO0MiIQnO\n3rUT/na+TkJ42E06R9Y+uo0ePZpPPvnkE8dxWsjwGE6+Ki26eEm7xx6Ilr4gtzE/KtVn1pBfBBf8\nPWiLrxaRCAw6Zc/HCUKWKUuawx6mBoeQW+RHI5wxsiu/e1evG92ldXHoG+wARfnR4Ear90A0ojhh\nSEf++pFezuiSw3rxtbEhnkYUMgZ2bcubxzzCs0s2c1iftrw2ogudW2dwPfa9Ja9g90uxBUgiUzxQ\nSxHP5E/mlsuP2c1PhIRoXningSYxumcFr99yFI0xh/4dy4LWaZFIRNGxvAjKD/KvfhAGCkp0jogc\nxJZG+zb335a6+MqTjhMEQfhKE6o57ELOc97BPXjy/aU0NDmM7tl83qaw79x6/AB2NTp0bl3EuWOy\nkOX8K8Z1E/py3YSgLewhEvF3/EYj0hGcCaqMzhFB2BdsqfXF17np38Ln8YwoLc15FwRB+EohjXYh\nTPTtUMofLhnLzBVbZEQ4TbQrLeRn58q0LCF73HPaEP7n+S8BuO2ElqrcgiAEgS21vnjGgeOVUhEz\ng7xSqgw4HKgFPghCThAEIdsc0a89v3xzIQDdKkIQ9inkPOP7tWN8CJZMEwRh/7jgkB7s3NVEXkRx\n2oiuQesIgmBgRaPdcZxFSqlX0Rnirwd+aXx8D9AK+I3jODuC8BMEQcg2Y3tXcuPEvny4ZDPfPXnQ\nnn9AEARBEHZDfjTCtRNCtkKFIAiAJY12l+uA94FfKKWOAeYAhwBHo8PivxegmyAIQta59fgBez5I\nEARBEARBsJqQ5O/fM47jLALGAE+iG+u3An2AXwDjHMfZFJydIAiCIAiCIAiCIKQfm0bacRxnBXBp\n0B6CIAiCIAiCIAiCkA2sGWkXBEEQBEEQBEEQhFxDGu2CIAiCIAiCIAiCEFKk0S4IgiAIgiAIgiAI\nIUUa7YIgCIIgCIIgCIIQUqTRLgiCIAiCIAiCIAghRRrtgiAIgiAIgiAIghBSpNEuCIIgCIIgCIIg\nCCFFGu2CIAiCIAiCIAiCEFKk0S4IgiAIgiAIgiAIIUUa7YIgCIIgCIIgCIIQUqTRLgiCIAiCIAiC\nIAghRRrtgiAIgiAIgiAIghBSpNEuCIIgCIIgCIIgCCFFGu2CIAiCIAiCIAiCEFKU4zhBO4QCpdSm\n4uLiykGDBgWtIgiCIAiCIAiCIKSZOXPmUFtbu9lxnLZBu+wL0mh3UUotAcqBpQGrBMVA99+5gVrs\nHhscwQ5PGxzBDk8bHMEOTxscwQ5PcUwfNnja4Ah2eNrgCHZ4imP6sMHTBkeAEUCT4ziFQYvsC3lB\nC4QFx3F6B+0QJEqpGQCO44wO2qUlbHAEOzxtcAQ7PG1wBDs8bXAEOzzFMX3Y4GmDI9jhaYMj2OEp\njunDBk8bHMHztA2Z0y4IgiAIgiAIgiAIIUUa7YIgCIIgCIIgCIIQUqTRLgiCIAiCIAiCIAghRRrt\ngiAIgiAIgiAIghBSpNEuCIIgCIIgCIIgCCFFlnwTBEEQBEEQBEEQhJAiI+2CIAiCIAiCIAiCEFKk\n0S4IgiAIgiAIgiAIIUUa7YIgCIIgCIIgCIIQUqTRLgiCIAiCIAiCIAghRRrtgiAIgiAIgiAIghBS\npNEuCIIgCIIgCIIgCCFFGu2CIAiCIAiCIAiCEFKk0S4IgiAIgiAIgiAIIUUa7YIg5ARKKRW0w+4I\nu18cpVTHoB0EQRBsIOzP9bD7xZFyRxCk0S4IVhDGglUpVR60w96glDoXwHEcJ2iXllBKnQ6cqJRq\nFbTL7lBKPQ+8rJRqE7TL7lBKFSqlou7r0JdzYby/U2HDuRTSR1ivSxvKHil30oeUO5kjrPe4iS3n\nMhvkBS0gfLVQSqmwFlJKqf5AD6AN8A6wxXGchmCtmqOUGg+MBKqAt4B3HcfZEqZzq5T6F7BIKXW/\n4zgbgvZpCaXUS8BwpdQSx3GmB+2TCqXUE8CZwLvADGBHsEapcStOpwArgF7AzDBdkwBKqUuAw4AB\nwOdKqZ86jrMsTJ5KqWFAV6AU+BDY7DjODqVUxHGcWLB2Hkqpk9Hfc3tgOjA9rPd6mL7fVNhQ9thQ\n7oAdZY+UO+lDyp30YUPZY1O5AwGUPY7jyCbbAW3AfcClxnsVtFMKx58DS4GYu30KXAO0CtotyfMR\nYJ3hucU9v6HxBH5k+N0LtAvaqQXPF4E64FtAWdA+LTj+G9juXp993H3K/TcStJ/h+TKwC3jf/d4f\nCdopheOfgK3ATve+iQGvAJVBuxmOjwGrjPtnJfAPoCpotyTPPwPbDM8YMAc4FigM2s91DH2543qF\nvuyxodxxPUNf9ki5k1ZPKXfS5xn6sseGcsf1DKzsCfyPl83uzb3pY8AHwNnG/tBUoIDn3UJ0GvAD\n4E33IbsAGBu0n+H5nPvQ/ztwPHA5MBdYDHQP2s91jAC/AZrQPfShrDwBLwG1bsWptbE/TNflXW7F\n6c7dFfBBOxvn8lpgLLAJWAOMDPocGo5/cc/lz4ARQE/gDaAeGBa0n+v4L7di90/gQuAe4CP3HloH\nHBu0o+v5V6DGreSd6Lo+73puB24DOgXsGPpyx/UJfdljQ7njeoa+7JFyJyPnUsqdA/cMfdljQ7nj\negZa9gR+Mclm7wbc6l68c92b7XPgHOPzwAsq4BduheROoL27rxNwv+v+aNCOrtNj7oPpO4ZnFPiJ\n63lE0vGB9YYDZ6N7bK8DPnP9fhSWyhPwAjrU71agIumzfsBBQGugJEDHUnSY7EdAR3dfEdDbLVB/\nCTwMjAr4u37RrTjdEj+XrlcMuCLo79r1ucatkNxjVkLdgn8NcIj7Ps/9N+vPJeB295zdk3R/9wXe\nxhvdnOx+Fsh3Dkxy752fpbh37gLWutfD3fG/IwDH0Jc7rkfoyx6byh3394e27JFyJ62eUu6kzzP0\nZY8N5Y7rEnjZE8gfLpv9G3AksBBYDRwK3OzedLPCUoECTnZv9ifjhToQdf+tcm+6dwEVsOcV6FCl\nXwJtkz57xC0ARgFfdx9uXd3PgqrYH4MOWevjvv4Ub9Sjs3tMOdA3ALe34i7GvlJgAjoksM546D5J\nQCNJwDC3ELrHOF9XAPPxh4btQFeoOwd0LuOjRuXG/rNct8VAryDOX5Lnk8CGFPfO99zr9BbgceB3\nBDS6iR4xWG08hyLxf/EqfzF0aOAY85gse8YrJUcafnnG51cBy93r8tpse2JBueP+/tCXPVhW7ri/\nO5RlD1LupPtcSrmTPs/Qlz2EvNxxf18oyp5ALiLZ7N/cB30MOMV93wX4bhAXcQt+EXSvXQMwwPRA\nJ2DMA75A99qX41aoAvSsTi6I0KGKa0YtrJgAABinSURBVNG9oIuMAnUh0D/Ac9sRWA9c4r4/HfjE\ndbsTPaKwCD3vp02W3f7teryBG0qFDq9bgw5LfReYik5qEwPeI4AKFDDUvVf+131/KrAZPXfvbOBw\n4CF33w7gm/HrJUt+p6N7kW/HrTiZvxt41i3sT3LfZ/3+QTd42rvX2iqM0TbgaPf+rgW+dP+NuffZ\nhdk6l+793RnY6N63JcZn8Ubcwe5z6g3XcSbBjWLf5TocFz/HKb77693zuBU3VDVbzyFCXu4Y33mo\nyx4sLHfc3xvKsgcpd9LlJ+VO+jytKXsIebnj/q5QlD1Z/WJk+2pt6NGEMuN9x91cxHlZditwC/Hv\nuu+bPSiB14FlITiPbWheuTsaPf+xHvgmuse+FzpRR/zh2iEg33xgNvAHY99kdDbSeKhVLVkMY0t6\nuD/peryKnp+5Gl1J6uMWZPluYfWOe9xDZDnJCXq0bSPwMbri/rx7zxQmHXe9ey63kMXRI9dpJFCa\ndF3Ge+mvcs/di0Fcg0muf3ddfo7O3nu5ey3uAs5Dh37m44X8bsFtfGTR8R10RTgeMhk/j1H0KOFc\noAL4r+sYD1XMauMIuNL9/c/SfATJvMcecI97iSwn2yLE5Y77O60oe7Cs3HH9QlX2IOVOuv2k3Em/\nZ+jLHiwod9zfH3jZk9U/WLavxsZuejdTXcTm8ehKQVZCrtwCoFeK/fGC4GV0T2k0yXEASfNqsuQb\n91Lo8MQYcEyK46ag5yFmPSGL8cD/G/C2eT0Al6IrezG3AMhq5S7pO3wKb4ToA6DIPMfu68PdwuxD\nAsiS7Bb6u9BhgCuA++J/R9Lf8rj7d3wjm9fhHo5pDcxDh30et7c/l6Fr8Qi80TZzO9M8zn39J/ez\nW7N1LtEVtwfd3/sOerQr3/38QnRSsrfc732Se9yvsn09uj5l7v2yEfgaUNDCOVfo0OTFZGlkBgvK\nHeMZbk3ZgwXlTtK1F6qyB38Yr5Q7B/j97uEYKXf23jP0ZY/x7AltueP+3hYb4GS57JEF64V9xnGc\npt18tg79sL8X3cN8NzoEC6XURcAfgAeVUnlZ8Kx2HGdpio+i8UPQPeAl8b9JKXUi8CjwHaVUNMXP\nZgzHvcPdf78NHOw4zhtKqYjrVuIe+iXQCr3ub1ZxvLU8PwVGKKV6Oo7TpJTqBPwQXXFaCZwEXK2U\n6pxFt6b4d+Y4zsXozK67gOsdx6lz1yJ1jB9ZgH7QDiKL5zL+faIzIW9Gf9ed0NlxQV8CTUqpQvf9\n6+6/rbPhl3SOmqGUijqOsw2dxKoAPRq3x59LN8a1OA0d2nkXuvC8HngNeCm+/qxSqsg99lX33+Is\nOTqOXo/7/9AhqOPRCateV0pNAZ5AJ4K62H0GLUTPd26TDT8T996pRY+qlqDP5zjzOeieywL3u/4M\nPQo7OBt+7j2Rss4SlnLHeIaHvuxRSqkk51CWO3GPsJY9juM0xp/VIS938t2XYS13Yq5nyntcyp19\n9gx92eM4juM+k0Nb7ri/v7GlZ3LWy55M9k7I9tXa2IceTXTv0/fQD4FZ6GQ3a9BzUoYE6Yh/tGOl\nsf94dM9pHTA4qHOJv5dOJR+PLgAWkeHlL/bgeD66YlIJtEWPGm0CLnMfWNPQvbZ3keE5XMmeSefv\n6ySNuuDvtV2CLtAKsuno7isDfoy3PvKXeHNG843jfuqe6yMz6bin7zzFsYeiK8q1uAlssrHtyRE9\nsrACb06keT08jJ7Dd3K2HI3rrRt6JG6O+33PRy8f09U4tgKdEOjxDPv1B05wn3kDkz6rxBtlm4le\nI7c4xXX5V3RyoK7ZdNzd84QAyp198SSgsmdvHAlBubOXnoGWPbtxLDReB1ru7OH+Dk25s5/3eFbL\nnd04Jtc7Ai139vCdh6LsAQ5Dd258Fzgv6bNQlDu789zDdZmVsiejF7ts9m/Ar3ATaLjv96VS3wa4\nAx1SF0P37g4NiyO6J3mO+zpeadoGDA/TucRfYbkInYjlKdx5X0E4otcjXYXuXVzmfrfXGZ+fjV5O\nJCOdH3vypIVQ2qRzeZ17Xd5vFgrZcMSrFLdHjxysBRrRvc29jONOR1eUPyZDIZ8HeI/H55hdkXx+\ng3JEV562A6fgFvru/tPcwn467nJHWfy+4xX2VkA7dGhlB5KWf0KHrNYCF+zrd7EPng+iR/vi4Zwz\ngRuTjumIHjGMocNRb8AI8UOHUq5Czy1sHYTjbn42K+XOgXiSxbLnAByzVu7sjafxzAys7NkLx5Rh\ntGS33Nmb+zsM5c6B3OPZKnf29H1Hko7NermzD995oGUPunxcZTj6Vltwjwm03Nlbz938bMbLnrT/\nwbJ9dTbgGbxeudOM/Xsa6TIfZDe5hcImMlOI7rMj3rqZb6LnxpyJzlhaTeYa7Pt7Ls3e2rjnCqAq\nSEf3ob/GPX4JeumQ5IZzRubrpelcnoHudV4I9AzCEa8h1x4d3hkvdBejk9v8E53MaEMm7p0DPJfx\nCnR8HtwiMjQXd28dDacL0aMaM9HLFo0F/gddCdgMDAro+07VaWM+K09FZx7+jAzNvwaeQ4+efYwe\n/XkFPbq7FpjkHhN/PnZEjxasRz/DP0WPPDzuft8bSRrRyZZjCz+XtXJnfz3JctlzAOcya+XOvnoS\nUNmTpnOZ6XJnb+7veB6AIMud/T2X2Sx39sqRAMudffjOIyl8s1b2AP9CN2T/gu7EOAc9RWQjXpRH\nvD4USLmzt54t/Fz22jyZ+MNls38DbsPr7YqhewonG5/vTRj6JegQrM1kIDRxfx3xCq2p7g35qXuj\nZqrBfkDnEh1Odzu6QbAOGBako/HQPxOdTOc2syDYm2sj4OvyZnTG1PVkJvJjf85lKXrO2V/Q4VUx\n97v+D26G5zCeS/e4T9CjcJko7PfZER0y+2e85Xbi2xdheg4Zn+ejK3lz3GsyU2Hcv0Qvl3MH3pq9\nHdAhfb7RBLwKVGv0qNvzxnmsRo9kZqLzY68dd/N/XEIGy50D8SSLZc+BnkuyUO7sx3UZSNmTpusy\n0+XO/pzHIMqdAz6X7s9kstzZZ0eyXO6k41yShbIH+C06iuhOoNLYf6fr2CyxJXrEOmvlzv54knr6\n0yVkuuzJxH8qm92b+xBf6t7EVcCt7kW7jL2sjKKXu3jZfaBkopGZDsf42qqbyFyD/YA80ck24g+u\n98nMyNZ+OaKTKlWRouc2pOdyKDoRSwyYkYmH//44pjivPYHh6DlemQpFTcf9Ex81PAnoFwZH41ps\nD1wNPI2eA3czGZgDl6bz+E33Z6Zm4pp0f8ck9OjZH2i+pM4h6MrvF+gET5FUzu79Mw6dPCsTIfH7\n7Jji/8houZNGz4yWPQfqSBbKnf3wNEess1b2pOFcZqPcScf9nY1yJx33TqbLnf0+l2Sp3Enjucxo\n2YNuxK5Edy5UJn32GPr5NwjdCTeZFNMayXC5k0bPjJc9jiONdtmSNnRv9dXo3qLJxr672ffK6FlA\nn7A5okcQCtChRHPIXAjYAZ9LdI/jje7/k/YEQOn6vlsqFMLkia6M3INOUNQtbI60UJkKm2eK/y8T\n86732zHT12ImziNwHJmbOxoFfu069Ul1vtDL7SwhxRzbbJzPA3VM+r8yUu6k61yS4bInHeeSDJc7\n6fzOM3l9pulcZrrcScv9vbvnUxg8U/x/mSh39tsxG8/JTJxLMlT2uM+5J9HlYK+kz45HT7fZig55\nj4+mv43biUmGkwOn0dPsTMxY2ZP4Hdm6yGSzZ0Mnq5iMXg7CfBC0VBlNfnhl/GY7UEd3X1sylBgk\nzZ6+9VPD5phJtwycy4JMXp+5ci6z4ZnO5xAZqpCmwbEoC+cxim58JdZjTvo8H10JWdHS+SLznXLp\ncEx7Uq9MeLr7Mlb2pNEx0+VOTlyX7r6MlTs2nEdbPNP9HEp1LYTIszATbkm/oxswwvz9wOHAu+j5\n/zcARwJDgL+hy8yXMu2Vbs9s3D+J35XtkyNbuDf20OtKC5VR97OjxNEuTxscbfG0wdEWT3FMu2sF\nKUZMjQrKC+jERUUYGbDJ0LxWWx1t8bTB0RZPccwtTxscbfAk9RKSJcCj6CX7jk86vhN6+kgMGJfF\n82iFZ+L3Z/sXymb/hlcZXQ6c5O77hrvviaD9bHG0xdMGR1s8bXC0xVMc0+r5PHqZmxJj3/HoTMg/\nCdrPFkdbPG1wtMVTHHPL0wbHMHsCI4DR7ut4x3eR++/9btk4IQTnL5SegV9Ystm5Ad/HG0V6CG+9\n1GaZIMXRfk8bHG3xtMHRFk9xTItfFL1M0HJjX0bXDv8qOtriaYOjLZ7imFueNjiG2ZPUSWPNfS+h\n55C3zaaXTZ6BX1yy2bfh9TrFl5WIAVvIwDImX2VHWzxtcLTF0wZHWzzFMW2OCngNmOe+PxG9FFmY\nKqGhd7TF0wZHWzzFMbc8bXC0zNNc3/xS9LKDT2FEB4RhC5NnBEHYB5RSEcdxYu7blXiV0MMdx/ki\nODMPGxzBDk8bHMEOTxscwQ5PcUwPSinlvmwCCpRSZ6JD//oARziOMyswORcbHMEOTxscwQ5PcUwf\nNnja4AhWeSbKR6XU6cAt6KXV7nEcZ2egcgah8wy6B0M2OzfgKvQakZuBIUH72Opoi6cNjrZ42uBo\ni6c4psUvD3jL9ZsBVBOi0RhbHG3xtMHRFk9xzC1PGxwt8ywEbgUWAOsJUQRaWD3zEHKOpBGg/fn5\nbsBpQEf0Mglfpk3O+x2hd3R/T+g9bXB0f0/oPW1wdH9P6D3FMX0cqCfQiF6buwcw3snAaIwNjmCH\npw2OYIenOKYPGzxtcAQ7PPfX0Y0G6IEOMT8SmAac6jjO3DQrxn+fFZ57g4TH5xhJoR4HK6VOUkp1\n3cf/Zh3wK6Cfk4EwTxscwQ5PGxzBDk8bHMEOT3FMH2nwjAFT0Bnuj8p05S6sjrZ42uBoi6c45pan\nDY62eB6Io6OHr7cDfwS+BZydjQZ7mD33mqCG+GXL/oY/mcK30FmMl6CTVESC8rLN0RZPGxxt8bTB\n0RZPcQyfJ9AFaJerjrZ42uBoi6c45panDY62eKbRMYKxTnqueu7T3xS0gGwBfOl67eAm4B/ApKB9\nbHW0xdMGR1s8bXC0xVMcc8vTBkdbPG1wtMVTHHPL0wZHWzxtcLTJc6/+lqAFZMvyFw5nAjuB3wN9\ng/ax1dEWTxscbfG0wdEWT3HMLU8bHG3xtMHRFk9xzC1PGxxt8bTB0SbPvd0kEV2O4CZUiACT0D1O\nv3YcZ2GwVn5scAQ7PG1wBDs8bXAEOzzFMX3Y4GmDI9jhaYMj2OEpjunDBk8bHMEOTxscwR7PfUW5\nPRFCDqCUKgemAzWO44xu4ZiI4zgxpVSB4zi7smtoh6PrEHpPGxxdh9B72uDoOoTeUxzThw2eNji6\nDqH3tMHRdQi9pzimDxs8bXB0HULvaYOj62CF574g2eNzC+VurZRSxcol8aF38UaBK5VSHcTRak8b\nHG3xtMHRFk9xzC1PGxxt8bTB0RZPccwtTxscbfG0wdEmz71GGu05glIqAtQDXwL9gZMdF/c6Ntcx\nfAD4JtBOHO30tMHRFk8bHG3xFMfc8rTB0RZPGxxt8RTH3PK0wdEWTxscbfLcV6TR/hXDvVCb4ThO\nzHGcOuAFd9cjSqmJ8R+LX7xKqVOAE4AFwOpcdbTF0wZHWzxtcLTFUxxzy9MGR1s8bXC0xVMcc8vT\nBkdbPG1wtMkzbTghyIYnW3o2/GsSDgFOAi4ADgMKjM9+BsSAauAbQB+gALgemAWsBQbkqqMtnjY4\n2uJpg6MtnuKYW542ONriaYOjLZ7imFueNjja4mmDo02eaf2bgxaQLU1fpP/i/Tawyr1I49s/gVOM\nY+41Pqt1L+YYMB8YmquOtnja4GiLpw2OtniKY2552uBoi6cNjrZ4imNuedrgaIunDY42eab97w5a\nQLY0f6Fwp3shvgCcAUwA7kGvU7gYOMs49nTgp8AbwNPATUA3cbTH0wZHWzxtcLTFUxxzy9MGR1s8\nbXC0xVMcc8vTBkdbPG1wtMkzbX9v0AKypfHLhGOAjcAzwGBj/2RgG7AS6JTi56LiaJ+nDY62eNrg\naIunOOaWpw2Otnja4GiLpzjmlqcNjrZ42uBok2da/+agBWRL45cJd6DDPo513yt0z9I8YA3Qy92f\nB7QyjlHx1+Joj6cNjrZ42uBoi6c45panDY62eNrgaIunOOaWpw2Otnja4GiTZ1r/5qAFZEvDl0hi\nLcJXgBXG/jOAucC6+MXr7u8H3AAUiqN9njY42uJpg6MtnuKYW542ONriaYOjLZ7imFueNjja4mmD\no02eGfnbgxaQbR+/MKNnKP4aNyED8CSwHRgLHJfq4nWP+wc6W2KXXHW0xdMGR1s8bXC0xVMcc8vT\nBkdbPG1wtMVTHHPL0wZHWzxtcLTJM1tb4AKy7eMXBh3drRwoSfrsenRChhfRaw6uTXHxXgasAH4J\nFOWqoy2eNjja4mmDoy2e4phbnjY42uJpg6MtnuKYW542ONriaYOjTZ7Z2gIXkG0vvyiYCPzEvSi3\nAUuAfwPHGce0AV52L+IdwKFJ/8cZ6DUJv0y+sHPF0RZPGxxt8bTB0RZPccwtTxscbfG0wdEWT3HM\nLU8bHG3xtMHRJs9sb4ELyLYXXxLcD6wGmtC9SbOADXhrDn4LKHOPnQy8h07O8H/uRXsQ8CC6t2kD\nMCQXHW3xtMHRFk8bHG3xFMfc8rTB0RZPGxxt8RTH3PK0wdEWTxscbfIMYgtcQLY9fEHwLLAZ3cM0\nHDe8AxjlXpTxi/j76MQMUeAU4D/GZzF0T9XrwMBcdLTF0wZHWzxtcLTFUxxzy9MGR1s8bXC0xVMc\nc8vTBkdbPG1wtMkzqC1wAdl28+XoeRo1wPeAju6+gqRjbjEu0qvdfQooBM5Gz/m4ExgHtM1FR1s8\nbXC0xdMGR1s8xTG3PG1wtMXTBkdbPMUxtzxtcLTF0wZHmzyD3AIXkK2FLwZecC/eW4E27j4zi2LU\neH2HewHXA4eIo32eNjja4mmDoy2e4phbnjY42uJpg6MtnuKYW542ONriaYOjTZ5Bb4ELyJbiS4E3\n3QvyZ8a+SIrjIsbrJ92fua2l43PN0RZPGxxt8bTB0RZPccwtTxscbfG0wdEWT3HMLU8bHG3xtMHR\nJs8wbBGEMLLT/fdqpdRQ97VKPshxnJhSKqKUUsBUd/ex8c/EEbDD0wZHsMPTBkeww1Mc04cNnjY4\ngh2eNjiCHZ7imD5s8LTBEezwtMER7PEMHGm0hwj3QsRxnFOAPwAlwEdKqTGO4zQppZp9X47jxBzd\nzfQx+sLfmuuOtnja4GiLpw2OtniKY2552uBoi6cNjrZ4imNuedrgaIunDY42eYYJabSHCMdxnPhF\n6jjO5ejwjyLgHfcijiVfxMb7SvQFvyLXHW3xtMHRFk8bHG3xFMfc8rTB0RZPGxxt8RTH3PK0wdEW\nTxscbfIMFU4IYvRl82/45208gZ63sRMYY36OP0nDX4CNwIjkz3LV0RZPGxxt8bTB0RZPccwtTxsc\nbfG0wdEWT3HMLU8bHG3xtMHRJs8wbIELyNbCF7Pnizjf+PxiYDXwe6BUHO3ztMHRFk8bHG3xFMfc\n8rTB0RZPGxxt8RTH3PK0wdEWTxscbfIMegtcQLbdfDktX8Rjjf0nATOBOUAvcbTX0wZHWzxtcLTF\nUxxzy9MGR1s8bXC0xVMcc8vTBkdbPG1wtMkzyC1wAdn28AWlvoh3AKOAMcCnwCZgiDja72mDoy2e\nNjja4imOueVpg6MtnjY42uIpjrnlaYOjLZ42ONrkGdj5CVpAtr34klJfxNXAAvffYeL41fG0wdEW\nTxscbfEUx9zytMHRFk8bHG3xFMfc8rTB0RZPGxxt8gzk3AQtINteflH+i/j37kW8ERgatJtNjrZ4\n2uBoi6cNjrZ4imNuedrgaIunDY62eIpjbnna4GiLpw2ONnlme1PuCREsQCkVcRwn5r7+DfCI4ziz\nAtbyYYMj2OFpgyPY4WmDI9jhKY7pwwZPGxzBDk8bHMEOT3FMHzZ42uAIdnja4Aj2eGYTabRbhnkR\nhxUbHMEOTxscwQ5PGxzBDk9xTB82eNrgCHZ42uAIdniKY/qwwdMGR7DD0wZHsMczW0ijXRAEQRAE\nQRAEQRBCSiRoAUEQBEEQBEEQBEEQUiONdkEQBEEQBEEQBEEIKdJoFwRBEARBEARBEISQIo12QRAE\nQRAEQRAEQQgp0mgXBEEQBEEQBEEQhJAijXZBEARBEARBEARBCCnSaBcEQRAEQRAEQRCEkCKNdkEQ\nBEEQBEEQBEEIKdJoFwRBEARBEARBEISQIo12QRAEQRAEQRAEQQgp0mgXBEEQBEEQBEEQhJAijXZB\nEARBEARBEARBCCnSaBcEQRAEQRAEQRCEkCKNdkEQBEEQBEEQBEEIKdJoFwRBEARBEARBEISQIo12\nQRAEQRAEQRAEQQgp/x8MtrW4tcE1CQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x79ea0f0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 272,
       "width": 502
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "mean, std = scaled_features['cnt']\n",
    "predictions = network.run(test_features).T*std + mean\n",
    "ax.plot(predictions[0], label='Prediction')\n",
    "ax.plot((test_targets['cnt']*std + mean).values, label='Data')\n",
    "ax.set_xlim(right=len(predictions))\n",
    "ax.legend()\n",
    "\n",
    "dates = pd.to_datetime(rides.ix[test_data.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可选：思考下你的结果（我们不会评估这道题的答案）\n",
    "\n",
    " \n",
    "请针对你的结果回答以下问题。模型对数据的预测效果如何？哪里出现问题了？为何出现问题呢？\n",
    "\n",
    "> **注意**：你可以通过双击该单元编辑文本。如果想要预览文本，请按 Control + Enter\n",
    "\n",
    "#### 请将你的答案填写在下方\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
